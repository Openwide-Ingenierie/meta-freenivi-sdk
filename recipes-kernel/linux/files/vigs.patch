diff --git a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
index 8e7fa4d..8cfe6e6 100644
--- a/drivers/gpu/drm/Kconfig
+++ b/drivers/gpu/drm/Kconfig
@@ -174,6 +174,8 @@ source "drivers/gpu/drm/gma500/Kconfig"

 source "drivers/gpu/drm/udl/Kconfig"

+source "drivers/gpu/drm/vigs/Kconfig"
+
 source "drivers/gpu/drm/ast/Kconfig"

 source "drivers/gpu/drm/mgag200/Kconfig"
diff --git a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
index 292a79d..246b57d 100644
--- a/drivers/gpu/drm/Makefile
+++ b/drivers/gpu/drm/Makefile
@@ -51,6 +51,7 @@ obj-$(CONFIG_DRM_NOUVEAU) +=nouveau/
 obj-$(CONFIG_DRM_EXYNOS) +=exynos/
 obj-$(CONFIG_DRM_GMA500) += gma500/
 obj-$(CONFIG_DRM_UDL) += udl/
+obj-$(CONFIG_DRM_VIGS) += vigs/
 obj-$(CONFIG_DRM_AST) += ast/
 obj-$(CONFIG_DRM_ARMADA) += armada/
 obj-$(CONFIG_DRM_RCAR_DU) += rcar-du/
diff --git a/drivers/gpu/drm/vigs/Kconfig b/drivers/gpu/drm/vigs/Kconfig
new file mode 100644
index 0000000..6818d9f
--- /dev/null
+++ b/drivers/gpu/drm/vigs/Kconfig
@@ -0,0 +1,23 @@
+#
+# VIGS configuration
+#
+
+config DRM_VIGS
+	tristate "DRM Support for VIGS"
+	depends on DRM && PCI
+	select DRM_KMS_HELPER
+	select DRM_TTM
+	select FB_CFB_FILLRECT
+	select FB_CFB_COPYAREA
+	select FB_CFB_IMAGEBLIT
+	select VT_HW_CONSOLE_BINDING if FRAMEBUFFER_CONSOLE
+	help
+	  This module enables VIGS passthrough from emulated system
+	  to hypervisor (for example, QEMU).
+
+config DRM_VIGS_DEBUG
+	bool "VIGS debug messages"
+	depends on DRM_VIGS
+	default no
+	help
+	  Enable VIGS debug messages.
diff --git a/drivers/gpu/drm/vigs/Makefile b/drivers/gpu/drm/vigs/Makefile
new file mode 100644
index 0000000..d893230
--- /dev/null
+++ b/drivers/gpu/drm/vigs/Makefile
@@ -0,0 +1,26 @@
+#
+# Makefile for the drm device driver.  This driver provides support for the
+# Direct Rendering Infrastructure (DRI) in XFree86 4.1.0 and higher.
+
+ccflags-y := -Iinclude/drm -Idrivers/gpu/drm/vigs
+vigs_drm-y := main.o \
+              vigs_driver.o \
+              vigs_gem.o \
+              vigs_surface.o \
+              vigs_execbuffer.o \
+              vigs_device.o \
+              vigs_mman.o \
+              vigs_crtc.o \
+              vigs_output.o \
+              vigs_framebuffer.o \
+              vigs_comm.o \
+              vigs_fbdev.o \
+              vigs_irq.o \
+              vigs_fence.o \
+              vigs_fenceman.o \
+              vigs_file.o \
+              vigs_plane.o \
+              vigs_dp.o \
+	      vigs_prime.o
+
+obj-$(CONFIG_DRM_VIGS) += vigs_drm.o
diff --git a/drivers/gpu/drm/vigs/main.c b/drivers/gpu/drm/vigs/main.c
new file mode 100644
index 0000000..2ff1911
--- /dev/null
+++ b/drivers/gpu/drm/vigs/main.c
@@ -0,0 +1,25 @@
+#include "vigs_driver.h"
+#include <linux/module.h>
+#include <linux/init.h>
+
+MODULE_AUTHOR("Stanislav Vorobiov");
+MODULE_LICENSE("Dual BSD/GPL");
+
+int vigs_init(void)
+{
+    int ret = vigs_driver_register();
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    return 0;
+}
+
+void vigs_cleanup(void)
+{
+    vigs_driver_unregister();
+}
+
+module_init(vigs_init);
+module_exit(vigs_cleanup);
diff --git a/drivers/gpu/drm/vigs/vigs_comm.c b/drivers/gpu/drm/vigs/vigs_comm.c
new file mode 100644
index 0000000..1171cfe
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_comm.c
@@ -0,0 +1,523 @@
+#include "vigs_comm.h"
+#include "vigs_device.h"
+#include "vigs_execbuffer.h"
+#include "vigs_regs.h"
+#include "vigs_fence.h"
+#include <drm/vigs_drm.h>
+
+static int vigs_comm_alloc(struct vigs_comm *comm,
+                           unsigned long size,
+                           void **ptr)
+{
+    int ret;
+
+    if (!comm->execbuffer ||
+        (vigs_gem_size(&comm->execbuffer->gem) < size)) {
+        if (comm->execbuffer) {
+            drm_gem_object_unreference_unlocked(&comm->execbuffer->gem.base);
+            comm->execbuffer = NULL;
+        }
+
+        ret = vigs_execbuffer_create(comm->vigs_dev,
+                                     size,
+                                     true,
+                                     &comm->execbuffer);
+
+        if (ret != 0) {
+            DRM_ERROR("unable to create execbuffer\n");
+            return ret;
+        }
+
+        vigs_gem_reserve(&comm->execbuffer->gem);
+
+        ret = vigs_gem_kmap(&comm->execbuffer->gem);
+
+        vigs_gem_unreserve(&comm->execbuffer->gem);
+
+        if (ret != 0) {
+            DRM_ERROR("unable to kmap execbuffer\n");
+
+            drm_gem_object_unreference_unlocked(&comm->execbuffer->gem.base);
+            comm->execbuffer = NULL;
+
+            return ret;
+        }
+    }
+
+    *ptr = comm->execbuffer->gem.kptr;
+
+    return 0;
+}
+
+static int vigs_comm_prepare(struct vigs_comm *comm,
+                             vigsp_cmd cmd,
+                             unsigned long request_size,
+                             void **request)
+{
+    int ret;
+    void *ptr;
+    struct vigsp_cmd_batch_header *batch_header;
+    struct vigsp_cmd_request_header *request_header;
+
+    ret = vigs_comm_alloc(comm,
+                          sizeof(*batch_header) +
+                          sizeof(*request_header) +
+                          request_size,
+                          &ptr);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    batch_header = ptr;
+    request_header = (struct vigsp_cmd_request_header*)(batch_header + 1);
+
+    batch_header->fence_seq = 0;
+    batch_header->size = sizeof(*request_header) + request_size;
+
+    request_header->cmd = cmd;
+    request_header->size = request_size;
+
+    if (request) {
+        *request = (request_header + 1);
+    }
+
+    return 0;
+}
+
+static void vigs_comm_exec_internal(struct vigs_comm *comm,
+                                    struct vigs_execbuffer *execbuffer)
+{
+    writel(vigs_gem_offset(&execbuffer->gem), comm->io_ptr + VIGS_REG_EXEC);
+}
+
+static int vigs_comm_init(struct vigs_comm *comm)
+{
+    int ret;
+    struct vigsp_cmd_init_request *request;
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_init,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    request->client_version = VIGS_PROTOCOL_VERSION;
+    request->server_version = 0;
+
+    vigs_comm_exec_internal(comm, comm->execbuffer);
+
+    if (request->server_version != VIGS_PROTOCOL_VERSION) {
+        DRM_ERROR("protocol version mismatch, expected %u, actual %u\n",
+                  VIGS_PROTOCOL_VERSION,
+                  request->server_version);
+        return -ENODEV;
+    }
+
+    return 0;
+}
+
+static void vigs_comm_exit(struct vigs_comm *comm)
+{
+    int ret;
+
+    ret = vigs_comm_prepare(comm, vigsp_cmd_exit, 0, NULL);
+
+    if (ret != 0) {
+        return;
+    }
+
+    vigs_comm_exec_internal(comm, comm->execbuffer);
+}
+
+int vigs_comm_create(struct vigs_device *vigs_dev,
+                     struct vigs_comm **comm)
+{
+    int ret = 0;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    *comm = kzalloc(sizeof(**comm), GFP_KERNEL);
+
+    if (!*comm) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    (*comm)->vigs_dev = vigs_dev;
+    (*comm)->io_ptr = vigs_dev->io_map->handle;
+
+    ret = vigs_comm_init(*comm);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    mutex_init(&(*comm)->mutex);
+
+    return 0;
+
+fail2:
+    if ((*comm)->execbuffer) {
+        drm_gem_object_unreference_unlocked(&(*comm)->execbuffer->gem.base);
+    }
+    kfree(*comm);
+fail1:
+    *comm = NULL;
+
+    return ret;
+}
+
+void vigs_comm_destroy(struct vigs_comm *comm)
+{
+    DRM_DEBUG_DRIVER("enter\n");
+
+    mutex_destroy(&comm->mutex);
+    vigs_comm_exit(comm);
+    if (comm->execbuffer) {
+        drm_gem_object_unreference_unlocked(&comm->execbuffer->gem.base);
+    }
+    kfree(comm);
+}
+
+void vigs_comm_exec(struct vigs_comm *comm,
+                    struct vigs_execbuffer *execbuffer)
+{
+    vigs_comm_exec_internal(comm, execbuffer);
+}
+
+int vigs_comm_reset(struct vigs_comm *comm)
+{
+    int ret;
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm, vigsp_cmd_reset, 0, NULL);
+
+    if (ret == 0) {
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    return ret;
+}
+
+int vigs_comm_create_surface(struct vigs_comm *comm,
+                             u32 width,
+                             u32 height,
+                             u32 stride,
+                             vigsp_surface_format format,
+                             vigsp_surface_id id)
+{
+    int ret;
+    struct vigsp_cmd_create_surface_request *request;
+
+    DRM_DEBUG_DRIVER("width = %u, height = %u, stride = %u, fmt = %d, id = %u\n",
+                     width,
+                     height,
+                     stride,
+                     format,
+                     id);
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_create_surface,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->width = width;
+        request->height = height;
+        request->stride = stride;
+        request->format = format;
+        request->id = id;
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    return ret;
+}
+
+int vigs_comm_destroy_surface(struct vigs_comm *comm, vigsp_surface_id id)
+{
+    int ret;
+    struct vigsp_cmd_destroy_surface_request *request;
+
+    DRM_DEBUG_DRIVER("id = %u\n", id);
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_destroy_surface,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->id = id;
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    return ret;
+}
+
+int vigs_comm_set_root_surface(struct vigs_comm *comm,
+                               vigsp_surface_id id,
+                               bool scanout,
+                               vigsp_offset offset)
+{
+    int ret;
+    struct vigs_fence *fence = NULL;
+    struct vigsp_cmd_set_root_surface_request *request;
+
+    DRM_DEBUG_DRIVER("id = %u, scanout = %d, offset = %u\n",
+                     id, scanout, offset);
+
+    if (scanout) {
+        /*
+         * We only need to fence this if surface is
+         * scanout, this is in order not to display garbage
+         * on page flip.
+         */
+
+        ret = vigs_fence_create(comm->vigs_dev->fenceman, &fence);
+
+        if (ret != 0) {
+            return ret;
+        }
+    }
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_set_root_surface,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->id = id;
+        request->scanout = scanout;
+        request->offset = offset;
+
+        if (fence) {
+            vigs_execbuffer_fence(comm->execbuffer, fence);
+        }
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    if ((ret == 0) && fence) {
+        vigs_fence_wait(fence, false);
+    }
+
+    vigs_fence_unref(fence);
+
+    return ret;
+}
+
+int vigs_comm_update_vram(struct vigs_comm *comm,
+                          vigsp_surface_id id,
+                          vigsp_offset offset)
+{
+    int ret;
+    struct vigs_fence *fence;
+    struct vigsp_cmd_update_vram_request *request;
+
+    DRM_DEBUG_DRIVER("id = %u, offset = %u\n", id, offset);
+
+    ret = vigs_fence_create(comm->vigs_dev->fenceman, &fence);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_update_vram,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->sfc_id = id;
+        request->offset = offset;
+
+        vigs_execbuffer_fence(comm->execbuffer, fence);
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    if (ret == 0) {
+        vigs_fence_wait(fence, false);
+    }
+
+    vigs_fence_unref(fence);
+
+    return ret;
+}
+
+int vigs_comm_update_gpu(struct vigs_comm *comm,
+                         vigsp_surface_id id,
+                         u32 width,
+                         u32 height,
+                         vigsp_offset offset)
+{
+    int ret;
+    struct vigs_fence *fence;
+    struct vigsp_cmd_update_gpu_request *request;
+
+    DRM_DEBUG_DRIVER("id = %u, offset = %u\n", id, offset);
+
+    ret = vigs_fence_create(comm->vigs_dev->fenceman, &fence);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_update_gpu,
+                            sizeof(*request) + sizeof(struct vigsp_rect),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->sfc_id = id;
+        request->offset = offset;
+        request->num_entries = 1;
+        request->entries[0].pos.x = 0;
+        request->entries[0].pos.y = 0;
+        request->entries[0].size.w = width;
+        request->entries[0].size.h = height;
+
+        vigs_execbuffer_fence(comm->execbuffer, fence);
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    if (ret == 0) {
+        vigs_fence_wait(fence, false);
+    }
+
+    vigs_fence_unref(fence);
+
+    return ret;
+}
+
+int vigs_comm_set_plane(struct vigs_comm *comm,
+                        u32 plane,
+                        u32 width,
+                        u32 height,
+                        vigsp_plane_format format,
+                        vigsp_surface_id surfaces[4],
+                        unsigned int src_x,
+                        unsigned int src_y,
+                        unsigned int src_w,
+                        unsigned int src_h,
+                        int dst_x,
+                        int dst_y,
+                        unsigned int dst_w,
+                        unsigned int dst_h,
+                        int z_pos,
+                        int hflip,
+                        int vflip,
+                        int rotation)
+{
+    int ret;
+    struct vigsp_cmd_set_plane_request *request;
+
+    DRM_DEBUG_DRIVER("plane = %u, src_x = %u, src_y = %u, src_w = %u, src_h = %u, dst_x = %d, dst_y = %d, dst_w = %u, dst_h = %u, z_pos = %d, hflip = %d, vflip = %d, rotation = %d\n",
+                     plane, src_x, src_y, src_w, src_h,
+                     dst_x, dst_y, dst_w, dst_h, z_pos, hflip, vflip,
+                     rotation);
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_prepare(comm,
+                            vigsp_cmd_set_plane,
+                            sizeof(*request),
+                            (void**)&request);
+
+    if (ret == 0) {
+        request->plane = plane;
+        request->width = width;
+        request->height = height;
+        request->format = format;
+        memcpy(request->surfaces, surfaces, sizeof(request->surfaces));
+        request->src_rect.pos.x = src_x;
+        request->src_rect.pos.y = src_y;
+        request->src_rect.size.w = src_w;
+        request->src_rect.size.h = src_h;
+        request->dst_x = dst_x;
+        request->dst_y = dst_y;
+        request->dst_size.w = dst_w;
+        request->dst_size.h = dst_h;
+        request->z_pos = z_pos;
+        request->hflip = hflip;
+        request->vflip = vflip;
+        request->rotation = rotation;
+
+        vigs_comm_exec_internal(comm, comm->execbuffer);
+    }
+
+    mutex_unlock(&comm->mutex);
+
+    return ret;
+}
+
+int vigs_comm_fence(struct vigs_comm *comm, struct vigs_fence *fence)
+{
+    struct vigsp_cmd_batch_header *batch_header;
+    int ret;
+
+    DRM_DEBUG_DRIVER("seq = %u\n", fence->seq);
+
+    mutex_lock(&comm->mutex);
+
+    ret = vigs_comm_alloc(comm,
+                          sizeof(*batch_header),
+                          (void**)&batch_header);
+
+    if (ret != 0) {
+        mutex_unlock(&comm->mutex);
+
+        return ret;
+    }
+
+    batch_header->fence_seq = 0;
+    batch_header->size = 0;
+
+    vigs_execbuffer_fence(comm->execbuffer, fence);
+
+    vigs_comm_exec_internal(comm, comm->execbuffer);
+
+    mutex_unlock(&comm->mutex);
+
+    return 0;
+}
+
+int vigs_comm_get_protocol_version_ioctl(struct drm_device *drm_dev,
+                                         void *data,
+                                         struct drm_file *file_priv)
+{
+    struct drm_vigs_get_protocol_version *args = data;
+
+    args->version = VIGS_PROTOCOL_VERSION;
+
+    return 0;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_comm.h b/drivers/gpu/drm/vigs/vigs_comm.h
new file mode 100644
index 0000000..9b87e94
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_comm.h
@@ -0,0 +1,102 @@
+#ifndef _VIGS_COMM_H_
+#define _VIGS_COMM_H_
+
+#include <linux/types.h>
+#include <linux/mutex.h>
+#include "vigs_protocol.h"
+
+struct drm_device;
+struct drm_file;
+struct vigs_device;
+struct vigs_execbuffer;
+struct vigs_fence;
+
+struct vigs_comm
+{
+    struct vigs_device *vigs_dev;
+
+    /*
+     * From vigs_device::io_map::handle for speed.
+     */
+    void __iomem *io_ptr;
+
+    /*
+     * For synchronizing all calls.
+     */
+    struct mutex mutex;
+
+    /*
+     * For internal use.
+     */
+    struct vigs_execbuffer *execbuffer;
+};
+
+int vigs_comm_create(struct vigs_device *vigs_dev,
+                     struct vigs_comm **comm);
+
+void vigs_comm_destroy(struct vigs_comm *comm);
+
+void vigs_comm_exec(struct vigs_comm *comm,
+                    struct vigs_execbuffer *execbuffer);
+
+int vigs_comm_reset(struct vigs_comm *comm);
+
+int vigs_comm_create_surface(struct vigs_comm *comm,
+                             u32 width,
+                             u32 height,
+                             u32 stride,
+                             vigsp_surface_format format,
+                             vigsp_surface_id id);
+
+int vigs_comm_destroy_surface(struct vigs_comm *comm, vigsp_surface_id id);
+
+int vigs_comm_set_root_surface(struct vigs_comm *comm,
+                               vigsp_surface_id id,
+                               bool scanout,
+                               vigsp_offset offset);
+
+int vigs_comm_update_vram(struct vigs_comm *comm,
+                          vigsp_surface_id id,
+                          vigsp_offset offset);
+
+int vigs_comm_update_gpu(struct vigs_comm *comm,
+                         vigsp_surface_id id,
+                         u32 width,
+                         u32 height,
+                         vigsp_offset offset);
+
+int vigs_comm_set_plane(struct vigs_comm *comm,
+                        u32 plane,
+                        u32 width,
+                        u32 height,
+                        vigsp_plane_format format,
+                        vigsp_surface_id surfaces[4],
+                        unsigned int src_x,
+                        unsigned int src_y,
+                        unsigned int src_w,
+                        unsigned int src_h,
+                        int dst_x,
+                        int dst_y,
+                        unsigned int dst_w,
+                        unsigned int dst_h,
+                        int z_pos,
+                        int hflip,
+                        int vflip,
+                        int rotation);
+
+int vigs_comm_fence(struct vigs_comm *comm, struct vigs_fence *fence);
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_comm_get_protocol_version_ioctl(struct drm_device *drm_dev,
+                                         void *data,
+                                         struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_crtc.c b/drivers/gpu/drm/vigs/vigs_crtc.c
new file mode 100644
index 0000000..25d10e7
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_crtc.c
@@ -0,0 +1,368 @@
+#include "vigs_crtc.h"
+#include "vigs_device.h"
+#include "vigs_framebuffer.h"
+#include "vigs_surface.h"
+#include "vigs_comm.h"
+#include "vigs_fbdev.h"
+#include "drm_crtc_helper.h"
+#include <linux/console.h>
+
+static int vigs_crtc_update(struct drm_crtc *crtc,
+                            struct drm_framebuffer *old_fb)
+{
+    struct vigs_device *vigs_dev = crtc->dev->dev_private;
+    struct vigs_framebuffer *vigs_old_fb = NULL;
+    struct vigs_framebuffer *vigs_fb;
+    int ret;
+
+    /*
+     * New framebuffer has been attached, notify the host that
+     * root surface has been updated.
+     */
+
+    if (!crtc->fb) {
+        DRM_ERROR("crtc->fb is NULL\n");
+        return -EINVAL;
+    }
+
+    if (old_fb) {
+        vigs_old_fb = fb_to_vigs_fb(old_fb);
+    }
+
+    vigs_fb = fb_to_vigs_fb(crtc->fb);
+
+    if (vigs_fb->surfaces[0]->scanout) {
+retry:
+        ret = vigs_framebuffer_pin(vigs_fb);
+
+        if (ret != 0) {
+            /*
+             * In condition of very intense GEM operations
+             * and with small amount of VRAM memory it's possible that
+             * GEM pin will be failing for some time, thus, framebuffer pin
+             * will be failing. This is unavoidable with current TTM design,
+             * thus, if someone is intensively
+             * reserves/unreserves GEMs then ttm_bo_validate can fail even if there
+             * is free space in a placement. Even worse, ttm_bo_validate fails with
+             * ENOMEM so it's not possible to tell if it's a temporary failure due
+             * to reserve/unreserve pressure or constant one due to memory shortage.
+             * We assume here that it's temporary and retry framebuffer pin. This
+             * is relatively safe since we only pin GEMs on pageflip and user
+             * should have started the VM with VRAM size equal to at least 3 frames,
+             * thus, 2 frame will always be free and we can always pin 1 frame.
+             */
+            cpu_relax();
+            goto retry;
+        }
+
+        vigs_gem_reserve(&vigs_fb->surfaces[0]->gem);
+
+        ret = vigs_comm_set_root_surface(vigs_dev->comm,
+                                         vigs_fb->surfaces[0]->id,
+                                         1,
+                                         vigs_gem_offset(&vigs_fb->surfaces[0]->gem));
+
+        vigs_gem_unreserve(&vigs_fb->surfaces[0]->gem);
+
+        if (ret != 0) {
+            vigs_framebuffer_unpin(vigs_fb);
+
+            return ret;
+        }
+    } else {
+        ret = vigs_comm_set_root_surface(vigs_dev->comm,
+                                         vigs_fb->surfaces[0]->id,
+                                         0,
+                                         0);
+
+        if (ret != 0) {
+            return ret;
+        }
+    }
+
+    if (vigs_old_fb && vigs_old_fb->surfaces[0]->scanout) {
+        vigs_framebuffer_unpin(vigs_old_fb);
+    }
+
+    return 0;
+}
+
+static void vigs_crtc_destroy(struct drm_crtc *crtc)
+{
+    struct vigs_crtc *vigs_crtc = crtc_to_vigs_crtc(crtc);
+
+    DRM_DEBUG_KMS("enter");
+
+    drm_crtc_cleanup(crtc);
+
+    kfree(vigs_crtc);
+}
+
+static void vigs_crtc_dpms(struct drm_crtc *crtc, int mode)
+{
+    struct vigs_device *vigs_dev = crtc->dev->dev_private;
+    int blank, i;
+    struct fb_event event;
+
+    DRM_DEBUG_KMS("enter: in_dpms = %d, mode = %d\n",
+                  vigs_dev->in_dpms,
+                  mode);
+
+    if (vigs_dev->in_dpms) {
+        return;
+    }
+
+    switch (mode) {
+    case DRM_MODE_DPMS_ON:
+        blank = FB_BLANK_UNBLANK;
+        break;
+    case DRM_MODE_DPMS_STANDBY:
+        blank = FB_BLANK_NORMAL;
+        break;
+    case DRM_MODE_DPMS_SUSPEND:
+        blank = FB_BLANK_VSYNC_SUSPEND;
+        break;
+    case DRM_MODE_DPMS_OFF:
+        blank = FB_BLANK_POWERDOWN;
+        break;
+    default:
+        DRM_ERROR("unspecified mode %d\n", mode);
+        return;
+    }
+
+    event.info = vigs_dev->fbdev->base.fbdev;
+    event.data = &blank;
+
+    /*
+     * We can't just 'console_lock' here, since
+     * this may result in deadlock:
+     * fb func:
+     * console_lock();
+     * mutex_lock(&dev->mode_config.mutex);
+     * DRM func:
+     * mutex_lock(&dev->mode_config.mutex);
+     * console_lock();
+     *
+     * So we just try to acquire it for 5 times with a delay
+     * and then just skip.
+     *
+     * This code is here only because pm is currently done via
+     * backlight which is bad, we need to make proper pm via
+     * kernel support.
+     */
+    for (i = 0; i < 5; ++i) {
+        if (console_trylock()) {
+            /*
+             * We must set in_dpms to true while walking
+             * fb call chain because a callback inside the
+             * call chain might do FB_BLANK on its own, i.e.
+             * 'vigs_fbdev_dpms' might get called from here. To avoid
+             * this we set in_dpms to true and 'vigs_fbdev_dpms'
+             * checks this and returns.
+             */
+            vigs_dev->in_dpms = true;
+
+            fb_notifier_call_chain(FB_EVENT_BLANK, &event);
+
+            vigs_dev->in_dpms = false;
+
+            console_unlock();
+            return;
+        }
+        msleep(100);
+        DRM_ERROR("unable to lock console, trying again\n");
+    }
+
+    DRM_ERROR("unable to lock console, skipping fb call chain\n");
+}
+
+static bool vigs_crtc_mode_fixup(struct drm_crtc *crtc,
+                                 const struct drm_display_mode *mode,
+                                 struct drm_display_mode *adjusted_mode)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    return true;
+}
+
+static int vigs_crtc_mode_set_base(struct drm_crtc *crtc, int x, int y,
+                                   struct drm_framebuffer *old_fb)
+{
+    DRM_DEBUG_KMS("enter: x = %d, y = %d\n", x, y);
+
+    return vigs_crtc_update(crtc, old_fb);
+}
+
+static int vigs_crtc_mode_set(struct drm_crtc *crtc,
+                              struct drm_display_mode *mode,
+                              struct drm_display_mode *adjusted_mode,
+                              int x, int y,
+                              struct drm_framebuffer *old_fb)
+{
+    DRM_DEBUG_KMS("enter: x = %d, y = %d\n", x, y);
+
+    return vigs_crtc_mode_set_base(crtc, x, y, old_fb);
+}
+
+static void vigs_crtc_prepare(struct drm_crtc *crtc)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static void vigs_crtc_commit(struct drm_crtc *crtc)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static void vigs_crtc_load_lut(struct drm_crtc *crtc)
+{
+}
+
+static int vigs_crtc_cursor_set(struct drm_crtc *crtc,
+                                struct drm_file *file_priv,
+                                uint32_t handle,
+                                uint32_t width,
+                                uint32_t height)
+{
+    /*
+     * Not supported.
+     */
+
+    return 0;
+}
+
+static int vigs_crtc_cursor_move(struct drm_crtc *crtc, int x, int y)
+{
+    /*
+     * Not supported.
+     */
+
+    return 0;
+}
+
+static int vigs_crtc_page_flip(struct drm_crtc *crtc,
+                               struct drm_framebuffer *fb,
+                               struct drm_pending_vblank_event *event,
+                               uint32_t page_flip_flags)
+{
+    unsigned long flags;
+    struct vigs_device *vigs_dev = crtc->dev->dev_private;
+    struct drm_framebuffer *old_fb = crtc->fb;
+    int ret = -EINVAL;
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    if (event) {
+        event->pipe = 0;
+
+        ret = drm_vblank_get(vigs_dev->drm_dev, 0);
+
+        if (ret != 0) {
+            DRM_ERROR("failed to acquire vblank counter\n");
+            goto out;
+        }
+
+        spin_lock_irqsave(&vigs_dev->drm_dev->event_lock, flags);
+        list_add_tail(&event->base.link,
+                      &vigs_dev->pageflip_event_list);
+        spin_unlock_irqrestore(&vigs_dev->drm_dev->event_lock, flags);
+
+        crtc->fb = fb;
+        ret = vigs_crtc_update(crtc, old_fb);
+        if (ret != 0) {
+            crtc->fb = old_fb;
+            spin_lock_irqsave(&vigs_dev->drm_dev->event_lock, flags);
+            if (atomic_read(&vigs_dev->drm_dev->vblank->refcount) > 0) {
+                /*
+                 * Only do this if event wasn't already processed.
+                 */
+                drm_vblank_put(vigs_dev->drm_dev, 0);
+                list_del(&event->base.link);
+            }
+            spin_unlock_irqrestore(&vigs_dev->drm_dev->event_lock, flags);
+            goto out;
+        }
+    }
+
+out:
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    return ret;
+}
+
+static void vigs_crtc_disable(struct drm_crtc *crtc)
+{
+    struct vigs_device *vigs_dev = crtc->dev->dev_private;
+    struct vigs_framebuffer *vigs_fb;
+
+    /*
+     * Framebuffer has been detached, notify the host that
+     * root surface is gone.
+     */
+
+    DRM_DEBUG_KMS("enter\n");
+
+    if (!crtc->fb) {
+        /*
+         * No current framebuffer, no need to notify the host.
+         */
+
+        return;
+    }
+
+    vigs_fb = fb_to_vigs_fb(crtc->fb);
+
+    vigs_comm_set_root_surface(vigs_dev->comm, 0, 0, 0);
+
+    if (vigs_fb->surfaces[0]->scanout) {
+        vigs_framebuffer_unpin(vigs_fb);
+    }
+}
+
+static const struct drm_crtc_funcs vigs_crtc_funcs =
+{
+    .cursor_set = vigs_crtc_cursor_set,
+    .cursor_move = vigs_crtc_cursor_move,
+    .set_config = drm_crtc_helper_set_config,
+    .page_flip = vigs_crtc_page_flip,
+    .destroy = vigs_crtc_destroy,
+};
+
+static const struct drm_crtc_helper_funcs vigs_crtc_helper_funcs =
+{
+    .dpms = vigs_crtc_dpms,
+    .mode_fixup = vigs_crtc_mode_fixup,
+    .mode_set = vigs_crtc_mode_set,
+    .mode_set_base = vigs_crtc_mode_set_base,
+    .prepare = vigs_crtc_prepare,
+    .commit = vigs_crtc_commit,
+    .load_lut = vigs_crtc_load_lut,
+    .disable = vigs_crtc_disable,
+};
+
+int vigs_crtc_init(struct vigs_device *vigs_dev)
+{
+    struct vigs_crtc *vigs_crtc;
+    int ret;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    vigs_crtc = kzalloc(sizeof(*vigs_crtc), GFP_KERNEL);
+
+    if (!vigs_crtc) {
+        return -ENOMEM;
+    }
+
+    ret = drm_crtc_init(vigs_dev->drm_dev,
+                        &vigs_crtc->base,
+                        &vigs_crtc_funcs);
+
+    if (ret != 0) {
+        kfree(vigs_crtc);
+        return ret;
+    }
+
+    drm_crtc_helper_add(&vigs_crtc->base, &vigs_crtc_helper_funcs);
+
+    return 0;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_crtc.h b/drivers/gpu/drm/vigs/vigs_crtc.h
new file mode 100644
index 0000000..b6e6feb
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_crtc.h
@@ -0,0 +1,20 @@
+#ifndef _VIGS_CRTC_H_
+#define _VIGS_CRTC_H_
+
+#include "drmP.h"
+
+struct vigs_device;
+
+struct vigs_crtc
+{
+    struct drm_crtc base;
+};
+
+static inline struct vigs_crtc *crtc_to_vigs_crtc(struct drm_crtc *crtc)
+{
+    return container_of(crtc, struct vigs_crtc, base);
+}
+
+int vigs_crtc_init(struct vigs_device *vigs_dev);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_device.c b/drivers/gpu/drm/vigs/vigs_device.c
new file mode 100644
index 0000000..68a1180
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_device.c
@@ -0,0 +1,370 @@
+#include "vigs_device.h"
+#include "vigs_mman.h"
+#include "vigs_fenceman.h"
+#include "vigs_crtc.h"
+#include "vigs_output.h"
+#include "vigs_plane.h"
+#include "vigs_framebuffer.h"
+#include "vigs_comm.h"
+#include "vigs_fbdev.h"
+#include "vigs_execbuffer.h"
+#include "vigs_surface.h"
+#include "vigs_dp.h"
+#include <drm/vigs_drm.h>
+
+extern const struct dma_buf_ops vigs_drm_prime_dmabuf_ops;
+
+static void vigs_device_mman_vram_to_gpu(void *user_data,
+                                         struct ttm_buffer_object *bo)
+{
+    struct vigs_device *vigs_dev = user_data;
+    struct vigs_gem_object *vigs_gem = bo_to_vigs_gem(bo);
+    struct vigs_surface *vigs_sfc = vigs_gem_to_vigs_surface(vigs_gem);
+    bool need_gpu_update = vigs_surface_need_gpu_update(vigs_sfc);
+
+    if (!vigs_sfc->is_gpu_dirty && need_gpu_update) {
+        DRM_INFO("vram_to_gpu: 0x%llX\n",
+                 drm_vma_node_offset_addr(&bo->vma_node));
+        vigs_comm_update_gpu(vigs_dev->comm,
+                             vigs_sfc->id,
+                             vigs_sfc->width,
+                             vigs_sfc->height,
+                             vigs_gem_offset(vigs_gem));
+    } else {
+        DRM_INFO("vram_to_gpu: 0x%llX (no-op)\n",
+                 drm_vma_node_offset_addr(&bo->vma_node));
+    }
+
+    vigs_sfc->is_gpu_dirty = false;
+}
+
+static void vigs_device_mman_gpu_to_vram(void *user_data,
+                                         struct ttm_buffer_object *bo,
+                                         unsigned long new_offset)
+{
+    struct vigs_device *vigs_dev = user_data;
+    struct vigs_gem_object *vigs_gem = bo_to_vigs_gem(bo);
+    struct vigs_surface *vigs_sfc = vigs_gem_to_vigs_surface(vigs_gem);
+
+    if (vigs_surface_need_vram_update(vigs_sfc)) {
+        DRM_DEBUG_DRIVER("0x%llX\n",
+                         drm_vma_node_offset_addr(&bo->vma_node));
+        vigs_comm_update_vram(vigs_dev->comm,
+                              vigs_sfc->id,
+                              new_offset);
+    } else {
+        DRM_DEBUG_DRIVER("0x%llX (no-op)\n",
+                         drm_vma_node_offset_addr(&bo->vma_node));
+    }
+}
+
+static void vigs_device_mman_init_vma(void *user_data,
+                                      void *vma_data_opaque,
+                                      struct ttm_buffer_object *bo,
+                                      bool track_access)
+{
+    struct vigs_vma_data *vma_data = vma_data_opaque;
+    struct vigs_gem_object *vigs_gem = bo_to_vigs_gem(bo);
+
+    if (vigs_gem->type != VIGS_GEM_TYPE_SURFACE) {
+        vma_data->sfc = NULL;
+        return;
+    }
+
+    vigs_vma_data_init(vma_data,
+                       vigs_gem_to_vigs_surface(vigs_gem),
+                       track_access);
+}
+
+static void vigs_device_mman_cleanup_vma(void *user_data,
+                                         void *vma_data_opaque)
+{
+    struct vigs_vma_data *vma_data = vma_data_opaque;
+
+    if (!vma_data->sfc) {
+        return;
+    }
+
+    vigs_vma_data_cleanup(vma_data);
+}
+
+static struct vigs_mman_ops mman_ops =
+{
+    .vram_to_gpu = &vigs_device_mman_vram_to_gpu,
+    .gpu_to_vram = &vigs_device_mman_gpu_to_vram,
+    .init_vma = &vigs_device_mman_init_vma,
+    .cleanup_vma = &vigs_device_mman_cleanup_vma
+};
+
+int vigs_device_init(struct vigs_device *vigs_dev,
+                     struct drm_device *drm_dev,
+                     struct pci_dev *pci_dev,
+                     unsigned long flags)
+{
+    int ret;
+    u32 i;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    vigs_dev->dev = &pci_dev->dev;
+    vigs_dev->drm_dev = drm_dev;
+    vigs_dev->pci_dev = pci_dev;
+
+    INIT_LIST_HEAD(&vigs_dev->pageflip_event_list);
+
+    vigs_dev->vram_base = pci_resource_start(pci_dev, 0);
+    vigs_dev->vram_size = pci_resource_len(pci_dev, 0);
+
+    vigs_dev->ram_base = pci_resource_start(pci_dev, 1);
+    vigs_dev->ram_size = pci_resource_len(pci_dev, 1);
+
+    vigs_dev->io_base = pci_resource_start(pci_dev, 2);
+    vigs_dev->io_size = pci_resource_len(pci_dev, 2);
+
+    idr_init(&vigs_dev->surface_idr);
+    mutex_init(&vigs_dev->surface_idr_mutex);
+
+    if (!vigs_dev->vram_base || !vigs_dev->ram_base || !vigs_dev->io_base) {
+        DRM_ERROR("VRAM, RAM or IO bar not found on device\n");
+        ret = -ENODEV;
+        goto fail1;
+    }
+
+    if ((vigs_dev->io_size < sizeof(void*)) ||
+        ((vigs_dev->io_size % sizeof(void*)) != 0)) {
+        DRM_ERROR("IO bar has bad size: %u bytes\n", vigs_dev->io_size);
+        ret = -ENODEV;
+        goto fail1;
+    }
+
+    ret = drm_addmap(vigs_dev->drm_dev,
+                     vigs_dev->io_base,
+                     vigs_dev->io_size,
+                     _DRM_REGISTERS,
+                     0,
+                     &vigs_dev->io_map);
+    if (ret != 0) {
+        goto fail1;
+    }
+
+    ret = vigs_mman_create(vigs_dev->vram_base, vigs_dev->vram_size,
+                           vigs_dev->ram_base, vigs_dev->ram_size,
+                           sizeof(struct vigs_vma_data),
+                           &mman_ops,
+                           vigs_dev,
+                           &vigs_dev->mman);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    vigs_dev->obj_dev = ttm_object_device_init(vigs_dev->mman->mem_global_ref.object,
+                                               12,
+                                               &vigs_drm_prime_dmabuf_ops);
+
+    if (!vigs_dev->obj_dev) {
+        DRM_ERROR("Unable to initialize obj_dev\n");
+        ret = -ENOMEM;
+        goto fail3;
+    }
+
+    ret = vigs_fenceman_create(&vigs_dev->fenceman);
+
+    if (ret != 0) {
+        goto fail4;
+    }
+
+    ret = vigs_dp_create(vigs_dev, &vigs_dev->dp);
+
+    if (ret != 0) {
+        goto fail5;
+    }
+
+    ret = vigs_comm_create(vigs_dev, &vigs_dev->comm);
+
+    if (ret != 0) {
+        goto fail6;
+    }
+
+    spin_lock_init(&vigs_dev->irq_lock);
+
+    drm_mode_config_init(vigs_dev->drm_dev);
+
+    vigs_framebuffer_config_init(vigs_dev);
+
+    ret = vigs_crtc_init(vigs_dev);
+
+    if (ret != 0) {
+        goto fail7;
+    }
+
+    ret = vigs_output_init(vigs_dev);
+
+    if (ret != 0) {
+        goto fail7;
+    }
+
+    for (i = 0; i < VIGS_MAX_PLANES; ++i) {
+        ret = vigs_plane_init(vigs_dev, i);
+
+        if (ret != 0) {
+            goto fail7;
+        }
+    }
+
+    ret = drm_vblank_init(drm_dev, 1);
+
+    if (ret != 0) {
+        goto fail7;
+    }
+
+    /*
+     * We allow VBLANK interrupt disabling right from the start. There's
+     * no point in "waiting until first modeset".
+     */
+    drm_dev->vblank_disable_allowed = 1;
+
+    ret = drm_irq_install(drm_dev);
+
+    if (ret != 0) {
+        goto fail8;
+    }
+
+    ret = vigs_fbdev_create(vigs_dev, &vigs_dev->fbdev);
+
+    if (ret != 0) {
+        goto fail9;
+    }
+
+    return 0;
+
+fail9:
+    drm_irq_uninstall(drm_dev);
+fail8:
+    drm_vblank_cleanup(drm_dev);
+fail7:
+    drm_mode_config_cleanup(vigs_dev->drm_dev);
+    vigs_comm_destroy(vigs_dev->comm);
+fail6:
+    vigs_dp_destroy(vigs_dev->dp);
+fail5:
+    vigs_fenceman_destroy(vigs_dev->fenceman);
+fail4:
+    ttm_object_device_release(&vigs_dev->obj_dev);
+fail3:
+    vigs_mman_destroy(vigs_dev->mman);
+fail2:
+    drm_rmmap(vigs_dev->drm_dev, vigs_dev->io_map);
+fail1:
+    idr_destroy(&vigs_dev->surface_idr);
+    mutex_destroy(&vigs_dev->surface_idr_mutex);
+
+    return ret;
+}
+
+void vigs_device_cleanup(struct vigs_device *vigs_dev)
+{
+    DRM_DEBUG_DRIVER("enter\n");
+
+    vigs_fbdev_destroy(vigs_dev->fbdev);
+    drm_irq_uninstall(vigs_dev->drm_dev);
+    drm_vblank_cleanup(vigs_dev->drm_dev);
+    drm_mode_config_cleanup(vigs_dev->drm_dev);
+    vigs_comm_destroy(vigs_dev->comm);
+    vigs_dp_destroy(vigs_dev->dp);
+    vigs_fenceman_destroy(vigs_dev->fenceman);
+    ttm_object_device_release(&vigs_dev->obj_dev);
+    vigs_mman_destroy(vigs_dev->mman);
+    drm_rmmap(vigs_dev->drm_dev, vigs_dev->io_map);
+    idr_destroy(&vigs_dev->surface_idr);
+    mutex_destroy(&vigs_dev->surface_idr_mutex);
+}
+
+int vigs_device_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+    struct drm_file *file_priv = filp->private_data;
+    struct vigs_device *vigs_dev = file_priv->minor->dev->dev_private;
+
+    if (vigs_dev == NULL) {
+        DRM_ERROR("no device\n");
+        return -EINVAL;
+    }
+
+    return vigs_mman_mmap(vigs_dev->mman,
+                          filp,
+                          vma,
+                          vigs_dev->track_gem_access);
+}
+
+int vigs_device_add_surface(struct vigs_device *vigs_dev,
+                            struct vigs_surface *sfc,
+                            vigsp_surface_id* id)
+{
+    int ret;
+
+    mutex_lock(&vigs_dev->surface_idr_mutex);
+
+    ret = idr_alloc(&vigs_dev->surface_idr, sfc, 1, 0, GFP_KERNEL);
+
+    mutex_unlock(&vigs_dev->surface_idr_mutex);
+
+    if (ret < 0) {
+        return ret;
+    }
+
+    *id = ret;
+
+    return 0;
+}
+
+void vigs_device_remove_surface(struct vigs_device *vigs_dev,
+                                vigsp_surface_id sfc_id)
+{
+    mutex_lock(&vigs_dev->surface_idr_mutex);
+    idr_remove(&vigs_dev->surface_idr, sfc_id);
+    mutex_unlock(&vigs_dev->surface_idr_mutex);
+}
+
+struct vigs_surface
+    *vigs_device_reference_surface(struct vigs_device *vigs_dev,
+                                   vigsp_surface_id sfc_id)
+{
+    struct vigs_surface *sfc;
+
+    mutex_lock(&vigs_dev->surface_idr_mutex);
+
+    sfc = idr_find(&vigs_dev->surface_idr, sfc_id);
+
+    if (sfc) {
+        if (vigs_gem_freed(&sfc->gem)) {
+            sfc = NULL;
+        } else {
+            drm_gem_object_reference(&sfc->gem.base);
+        }
+    }
+
+    mutex_unlock(&vigs_dev->surface_idr_mutex);
+
+    return sfc;
+}
+
+int vigs_device_add_surface_unlocked(struct vigs_device *vigs_dev,
+                                     struct vigs_surface *sfc,
+                                     vigsp_surface_id* id)
+{
+    int ret;
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+    ret = vigs_device_add_surface(vigs_dev, sfc, id);
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    return ret;
+}
+
+void vigs_device_remove_surface_unlocked(struct vigs_device *vigs_dev,
+                                         vigsp_surface_id sfc_id)
+{
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+    vigs_device_remove_surface(vigs_dev, sfc_id);
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_device.h b/drivers/gpu/drm/vigs/vigs_device.h
new file mode 100644
index 0000000..d75f61c
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_device.h
@@ -0,0 +1,107 @@
+#ifndef _VIGS_DEVICE_H_
+#define _VIGS_DEVICE_H_
+
+#include "drmP.h"
+#include "vigs_protocol.h"
+
+struct vigs_mman;
+struct vigs_fenceman;
+struct vigs_dp;
+struct vigs_comm;
+struct vigs_fbdev;
+struct vigs_surface;
+
+struct vigs_device
+{
+    struct device *dev;
+    struct drm_device *drm_dev;
+    struct pci_dev *pci_dev;
+
+    struct list_head pageflip_event_list;
+
+    resource_size_t vram_base;
+    resource_size_t vram_size;
+
+    resource_size_t ram_base;
+    resource_size_t ram_size;
+
+    resource_size_t io_base;
+    resource_size_t io_size;
+
+    struct idr surface_idr;
+    struct mutex surface_idr_mutex;
+
+    /* Map of IO BAR. */
+    drm_local_map_t *io_map;
+
+    struct vigs_mman *mman;
+
+    struct ttm_object_device *obj_dev;
+
+    struct vigs_fenceman *fenceman;
+
+    struct vigs_dp *dp;
+
+    struct vigs_comm *comm;
+
+    struct vigs_fbdev *fbdev;
+
+    /*
+     * We need this because it's essential to read 'lower' and 'upper'
+     * fence acks atomically in IRQ handler and on SMP systems IRQ handler
+     * can be run on several CPUs concurrently.
+     */
+    spinlock_t irq_lock;
+
+    /*
+     * A hack we're forced to have in order to tell if we
+     * need to track GEM access or not in 'vigs_device_mmap'.
+     * current's 'mmap_sem' is write-locked while this is true,
+     * so no race will occur.
+     */
+    bool track_gem_access;
+
+    /*
+     * A hack to tell if DPMS callback is called from inside
+     * 'fb_blank' or vice-versa.
+     */
+    bool in_dpms;
+};
+
+int vigs_device_init(struct vigs_device *vigs_dev,
+                     struct drm_device *drm_dev,
+                     struct pci_dev *pci_dev,
+                     unsigned long flags);
+
+void vigs_device_cleanup(struct vigs_device *vigs_dev);
+
+int vigs_device_mmap(struct file *filp, struct vm_area_struct *vma);
+
+int vigs_device_add_surface(struct vigs_device *vigs_dev,
+                            struct vigs_surface *sfc,
+                            vigsp_surface_id* id);
+
+void vigs_device_remove_surface(struct vigs_device *vigs_dev,
+                                vigsp_surface_id sfc_id);
+
+struct vigs_surface
+    *vigs_device_reference_surface(struct vigs_device *vigs_dev,
+                                   vigsp_surface_id sfc_id);
+
+/*
+ * Locks drm_device::struct_mutex.
+ * @{
+ */
+
+int vigs_device_add_surface_unlocked(struct vigs_device *vigs_dev,
+                                     struct vigs_surface *sfc,
+                                     vigsp_surface_id* id);
+
+void vigs_device_remove_surface_unlocked(struct vigs_device *vigs_dev,
+                                         vigsp_surface_id sfc_id);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_dp.c b/drivers/gpu/drm/vigs/vigs_dp.c
new file mode 100644
index 0000000..7680230
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_dp.c
@@ -0,0 +1,228 @@
+#include "vigs_dp.h"
+#include "vigs_surface.h"
+#include "vigs_device.h"
+
+int vigs_dp_create(struct vigs_device *vigs_dev,
+                   struct vigs_dp **dp)
+{
+    int ret = 0;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    *dp = kzalloc(sizeof(**dp), GFP_KERNEL);
+
+    if (!*dp) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    return 0;
+
+fail1:
+    *dp = NULL;
+
+    return ret;
+}
+
+void vigs_dp_destroy(struct vigs_dp *dp)
+{
+    DRM_DEBUG_DRIVER("enter\n");
+
+    kfree(dp);
+}
+
+void vigs_dp_remove_surface(struct vigs_dp *dp, struct vigs_surface *sfc)
+{
+    int i, j;
+
+    for (i = 0; i < VIGS_MAX_PLANES; ++i) {
+        for (j = 0; j < DRM_VIGS_NUM_DP_FB_BUF; ++j) {
+            if (dp->planes[i].fb_bufs[j].y == sfc) {
+                dp->planes[i].fb_bufs[j].y = NULL;
+            }
+            if (dp->planes[i].fb_bufs[j].c == sfc) {
+                dp->planes[i].fb_bufs[j].c = NULL;
+            }
+        }
+    }
+}
+
+int vigs_dp_surface_create_ioctl(struct drm_device *drm_dev,
+                                 void *data,
+                                 struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct vigs_dp *dp = vigs_dev->dp;
+    struct drm_vigs_dp_create_surface *args = data;
+    struct vigs_surface *sfc = NULL;
+    bool busy;
+    uint32_t handle;
+    int ret;
+
+    if (args->dp_plane >= VIGS_MAX_PLANES) {
+        DRM_ERROR("bad DP plane = %u\n", args->dp_plane);
+        return -ENOMEM;
+    }
+
+    if (args->dp_fb_buf >= DRM_VIGS_NUM_DP_FB_BUF) {
+        DRM_ERROR("bad DP fb buf = %u\n", args->dp_fb_buf);
+        return -ENOMEM;
+    }
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    switch (args->dp_mem_flag) {
+    case DRM_VIGS_DP_FB_Y:
+        busy = dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].y != NULL;
+        break;
+    case DRM_VIGS_DP_FB_C:
+        busy = dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].c != NULL;
+        break;
+    default:
+        mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+        DRM_ERROR("bad DP mem flag = %u\n", args->dp_mem_flag);
+        return -ENOMEM;
+    }
+
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    if (busy) {
+        DRM_INFO("DP mem %u:%u:%u is busy\n", args->dp_plane,
+                                              args->dp_fb_buf,
+                                              args->dp_mem_flag);
+        return -ENOMEM;
+    }
+
+    ret = vigs_surface_create(vigs_dev,
+                              args->width,
+                              args->height,
+                              args->stride,
+                              args->format,
+                              false,
+                              &sfc);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    /*
+     * Check busy again since DP mem might
+     * gotten busy while we were creating our surface.
+     * If it's not busy then occupy it.
+     */
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    switch (args->dp_mem_flag) {
+    case DRM_VIGS_DP_FB_Y:
+        if (dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].y) {
+            busy = true;
+        } else {
+            dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].y = sfc;
+        }
+        break;
+    case DRM_VIGS_DP_FB_C:
+        if (dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].c) {
+            busy = true;
+        } else {
+            dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].c = sfc;
+        }
+        break;
+    default:
+        drm_gem_object_unreference(&sfc->gem.base);
+        mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+        BUG();
+        return -ENOMEM;
+    }
+
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    if (busy) {
+        drm_gem_object_unreference_unlocked(&sfc->gem.base);
+
+        DRM_INFO("DP mem %u:%u:%u is busy\n", args->dp_plane,
+                                              args->dp_fb_buf,
+                                              args->dp_mem_flag);
+        return -ENOMEM;
+    }
+
+    ret = drm_gem_handle_create(file_priv,
+                                &sfc->gem.base,
+                                &handle);
+
+    if (ret == 0) {
+        args->handle = handle;
+        args->size = vigs_gem_size(&sfc->gem);
+        args->id = sfc->id;
+    } else {
+        /*
+         * Don't bother setting DP mem slot to NULL here, DRM
+         * will do this for us once the GEM is freed.
+         */
+    }
+
+    drm_gem_object_unreference_unlocked(&sfc->gem.base);
+
+    return ret;
+}
+
+int vigs_dp_surface_open_ioctl(struct drm_device *drm_dev,
+                               void *data,
+                               struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct vigs_dp *dp = vigs_dev->dp;
+    struct drm_vigs_dp_open_surface *args = data;
+    struct vigs_surface *sfc = NULL;
+    uint32_t handle;
+    int ret;
+
+    if (args->dp_plane >= VIGS_MAX_PLANES) {
+        DRM_ERROR("bad DP plane = %u\n", args->dp_plane);
+        return -ENOMEM;
+    }
+
+    if (args->dp_fb_buf >= DRM_VIGS_NUM_DP_FB_BUF) {
+        DRM_ERROR("bad DP fb buf = %u\n", args->dp_fb_buf);
+        return -ENOMEM;
+    }
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    switch (args->dp_mem_flag) {
+    case DRM_VIGS_DP_FB_Y:
+        sfc = dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].y;
+        break;
+    case DRM_VIGS_DP_FB_C:
+        sfc = dp->planes[args->dp_plane].fb_bufs[args->dp_fb_buf].c;
+        break;
+    default:
+        mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+        DRM_ERROR("bad DP mem flag = %u\n", args->dp_mem_flag);
+        return -ENOMEM;
+    }
+
+    if (sfc) {
+        drm_gem_object_reference(&sfc->gem.base);
+    } else {
+        mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+        DRM_INFO("DP mem %u:%u:%u is empty\n", args->dp_plane,
+                                               args->dp_fb_buf,
+                                               args->dp_mem_flag);
+        return -ENOMEM;
+    }
+
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    ret = drm_gem_handle_create(file_priv,
+                                &sfc->gem.base,
+                                &handle);
+
+    if (ret == 0) {
+        args->handle = handle;
+    }
+
+    drm_gem_object_unreference_unlocked(&sfc->gem.base);
+
+    return ret;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_dp.h b/drivers/gpu/drm/vigs/vigs_dp.h
new file mode 100644
index 0000000..46093ec
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_dp.h
@@ -0,0 +1,72 @@
+#ifndef _VIGS_DP_H_
+#define _VIGS_DP_H_
+
+#include "drmP.h"
+#include "vigs_protocol.h"
+#include <drm/vigs_drm.h>
+
+struct vigs_device;
+struct vigs_surface;
+
+struct vigs_dp_fb_buf
+{
+    /*
+     * These are weak pointers, no reference is kept
+     * for them. When surface is destroyed they're
+     * automatically reset to NULL. Must be
+     * accessed only with drm_device::struct_mutex held.
+     * @{
+     */
+
+    struct vigs_surface *y;
+    struct vigs_surface *c;
+
+    /*
+     * @}
+     */
+};
+
+struct vigs_dp_plane
+{
+    struct vigs_dp_fb_buf fb_bufs[DRM_VIGS_NUM_DP_FB_BUF];
+};
+
+struct vigs_dp
+{
+    struct vigs_dp_plane planes[VIGS_MAX_PLANES];
+};
+
+int vigs_dp_create(struct vigs_device *vigs_dev,
+                   struct vigs_dp **dp);
+
+void vigs_dp_destroy(struct vigs_dp *dp);
+
+/*
+ * Must be called with drm_device::struct_mutex held.
+ * @{
+ */
+
+void vigs_dp_remove_surface(struct vigs_dp *dp, struct vigs_surface *sfc);
+
+/*
+ * @}
+ */
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_dp_surface_create_ioctl(struct drm_device *drm_dev,
+                                 void *data,
+                                 struct drm_file *file_priv);
+
+int vigs_dp_surface_open_ioctl(struct drm_device *drm_dev,
+                               void *data,
+                               struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_driver.c b/drivers/gpu/drm/vigs/vigs_driver.c
new file mode 100644
index 0000000..8a66f92
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_driver.c
@@ -0,0 +1,260 @@
+#include "vigs_driver.h"
+#include "vigs_gem.h"
+#include "vigs_device.h"
+#include "vigs_fbdev.h"
+#include "vigs_comm.h"
+#include "vigs_surface.h"
+#include "vigs_execbuffer.h"
+#include "vigs_irq.h"
+#include "vigs_fence.h"
+#include "vigs_file.h"
+#include "vigs_plane.h"
+#include "vigs_mman.h"
+#include "vigs_dp.h"
+#include <drm/drmP.h>
+#include <linux/module.h>
+#include <drm/vigs_drm.h>
+
+#define PCI_VENDOR_ID_VIGS 0x19B2
+#define PCI_DEVICE_ID_VIGS 0x1011
+
+#define DRIVER_NAME "vigs"
+#define DRIVER_DESC "VIGS DRM"
+#define DRIVER_DATE "20121102"
+#define DRIVER_MAJOR DRM_VIGS_DRIVER_VERSION
+#define DRIVER_MINOR 0
+
+static struct pci_device_id vigs_pci_table[] =
+{
+    {
+        .vendor     = PCI_VENDOR_ID_VIGS,
+        .device     = PCI_DEVICE_ID_VIGS,
+        .subvendor  = PCI_ANY_ID,
+        .subdevice  = PCI_ANY_ID,
+    },
+    { 0 }
+};
+MODULE_DEVICE_TABLE(pci, vigs_pci_table);
+
+static struct drm_ioctl_desc vigs_drm_ioctls[] =
+{
+    DRM_IOCTL_DEF_DRV(VIGS_GET_PROTOCOL_VERSION, vigs_comm_get_protocol_version_ioctl,
+                                                 DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_CREATE_SURFACE, vigs_surface_create_ioctl,
+                                           DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_CREATE_EXECBUFFER, vigs_execbuffer_create_ioctl,
+                                              DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_GEM_MAP, vigs_gem_map_ioctl,
+                                    DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_GEM_WAIT, vigs_gem_wait_ioctl,
+                                     DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_SURFACE_INFO, vigs_surface_info_ioctl,
+                                         DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_EXEC, vigs_execbuffer_exec_ioctl,
+                                 DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_SURFACE_SET_GPU_DIRTY, vigs_surface_set_gpu_dirty_ioctl,
+                                                  DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_SURFACE_START_ACCESS, vigs_surface_start_access_ioctl,
+                                                 DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_SURFACE_END_ACCESS, vigs_surface_end_access_ioctl,
+                                               DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_CREATE_FENCE, vigs_fence_create_ioctl,
+                                         DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_FENCE_WAIT, vigs_fence_wait_ioctl,
+                                       DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_FENCE_SIGNALED, vigs_fence_signaled_ioctl,
+                                           DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_FENCE_UNREF, vigs_fence_unref_ioctl,
+                                        DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_PLANE_SET_ZPOS, vigs_plane_set_zpos_ioctl,
+                                           DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_PLANE_SET_TRANSFORM, vigs_plane_set_transform_ioctl,
+                                                DRM_UNLOCKED | DRM_AUTH),
+
+    DRM_IOCTL_DEF_DRV(VIGS_DP_CREATE_SURFACE, vigs_dp_surface_create_ioctl,
+                                              DRM_UNLOCKED | DRM_AUTH),
+    DRM_IOCTL_DEF_DRV(VIGS_DP_OPEN_SURFACE, vigs_dp_surface_open_ioctl,
+                                            DRM_UNLOCKED | DRM_AUTH)
+};
+
+static const struct file_operations vigs_drm_driver_fops =
+{
+    .owner = THIS_MODULE,
+    .open = drm_open,
+    .release = drm_release,
+    .unlocked_ioctl = drm_ioctl,
+    .poll = drm_poll,
+    .mmap = vigs_device_mmap,
+    .read = drm_read
+};
+
+static int vigs_drm_load(struct drm_device *dev, unsigned long flags)
+{
+    int ret = 0;
+    struct vigs_device *vigs_dev = NULL;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    vigs_dev = kzalloc(sizeof(*vigs_dev), GFP_KERNEL);
+
+    if (vigs_dev == NULL) {
+        DRM_ERROR("failed to allocate VIGS device\n");
+        return -ENOMEM;
+    }
+
+    dev->dev_private = vigs_dev;
+
+    ret = vigs_device_init(vigs_dev, dev, dev->pdev, flags);
+
+    if (ret != 0) {
+        goto fail;
+    }
+
+    return 0;
+
+fail:
+    kfree(vigs_dev);
+
+    return ret;
+}
+
+static int vigs_drm_unload(struct drm_device *dev)
+{
+    struct vigs_device *vigs_dev = dev->dev_private;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    vigs_device_cleanup(vigs_dev);
+
+    kfree(dev->dev_private);
+    dev->dev_private = NULL;
+
+    return 0;
+}
+
+static int vigs_drm_open(struct drm_device *dev, struct drm_file *file_priv)
+{
+    int ret = 0;
+    struct vigs_device *vigs_dev = dev->dev_private;
+    struct vigs_file *vigs_file;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    ret = vigs_file_create(vigs_dev, &vigs_file);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    file_priv->driver_priv = vigs_file;
+
+    vigs_dev->mman->bo_dev.dev_mapping = dev->dev_mapping;
+
+    return 0;
+}
+
+static void vigs_drm_preclose(struct drm_device *dev,
+                              struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = dev->dev_private;
+    struct drm_pending_vblank_event *event, *tmp;
+    unsigned long flags;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    spin_lock_irqsave(&dev->event_lock, flags);
+
+    list_for_each_entry_safe(event, tmp,
+                             &vigs_dev->pageflip_event_list,
+                             base.link) {
+        if (event->base.file_priv == file_priv) {
+            list_del(&event->base.link);
+            event->base.destroy(&event->base);
+        }
+    }
+
+    spin_unlock_irqrestore(&dev->event_lock, flags);
+}
+
+static void vigs_drm_postclose(struct drm_device *dev,
+                               struct drm_file *file_priv)
+{
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    vigs_file_destroy(vigs_file);
+
+    file_priv->driver_priv = NULL;
+}
+
+static void vigs_drm_lastclose(struct drm_device *dev)
+{
+    struct vigs_device *vigs_dev = dev->dev_private;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    if (vigs_dev->fbdev) {
+        vigs_fbdev_restore_mode(vigs_dev->fbdev);
+    }
+
+    vigs_comm_reset(vigs_dev->comm);
+}
+
+static struct drm_driver vigs_drm_driver =
+{
+    .driver_features = DRIVER_GEM | DRIVER_MODESET |
+                       DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED,
+    .load = vigs_drm_load,
+    .unload = vigs_drm_unload,
+    .open = vigs_drm_open,
+    .preclose = vigs_drm_preclose,
+    .postclose = vigs_drm_postclose,
+    .lastclose = vigs_drm_lastclose,
+    .get_vblank_counter = drm_vblank_count,
+    .enable_vblank = vigs_enable_vblank,
+    .disable_vblank = vigs_disable_vblank,
+    .irq_handler = vigs_irq_handler,
+    .gem_free_object = vigs_gem_free_object,
+    .dumb_create = vigs_gem_dumb_create,
+    .dumb_map_offset = vigs_gem_dumb_map_offset,
+    .dumb_destroy = drm_gem_dumb_destroy,
+    .ioctls = vigs_drm_ioctls,
+    .num_ioctls = DRM_ARRAY_SIZE(vigs_drm_ioctls),
+    .fops = &vigs_drm_driver_fops,
+    .name = DRIVER_NAME,
+    .desc = DRIVER_DESC,
+    .date = DRIVER_DATE,
+    .major = DRIVER_MAJOR,
+    .minor = DRIVER_MINOR,
+};
+
+static int vigs_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+    return drm_get_pci_dev(pdev, ent, &vigs_drm_driver);
+}
+
+static void vigs_pci_remove(struct pci_dev *pdev)
+{
+    struct drm_device *dev = pci_get_drvdata(pdev);
+
+    drm_put_dev(dev);
+}
+
+static struct pci_driver vigs_pci_driver =
+{
+     .name = DRIVER_NAME,
+     .id_table = vigs_pci_table,
+     .probe = vigs_pci_probe,
+     .remove = vigs_pci_remove,
+};
+
+int vigs_driver_register(void)
+{
+    return drm_pci_init(&vigs_drm_driver, &vigs_pci_driver);
+}
+
+void vigs_driver_unregister(void)
+{
+    drm_pci_exit(&vigs_drm_driver, &vigs_pci_driver);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_driver.h b/drivers/gpu/drm/vigs/vigs_driver.h
new file mode 100644
index 0000000..4cd8374
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_driver.h
@@ -0,0 +1,10 @@
+#ifndef _VIGS_DRIVER_H_
+#define _VIGS_DRIVER_H_
+
+#include <linux/types.h>
+
+int vigs_driver_register(void);
+
+void vigs_driver_unregister(void);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_execbuffer.c b/drivers/gpu/drm/vigs/vigs_execbuffer.c
new file mode 100644
index 0000000..d609448
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_execbuffer.c
@@ -0,0 +1,548 @@
+#include "vigs_execbuffer.h"
+#include "vigs_device.h"
+#include "vigs_surface.h"
+#include "vigs_comm.h"
+#include "vigs_fence.h"
+#include <drm/vigs_drm.h>
+
+union vigs_request
+{
+    struct vigsp_cmd_update_vram_request *update_vram;
+    struct vigsp_cmd_update_gpu_request *update_gpu;
+    struct vigsp_cmd_copy_request *copy;
+    struct vigsp_cmd_solid_fill_request *solid_fill;
+    struct vigsp_cmd_ga_copy_request *ga_copy;
+    void *data;
+};
+
+static int vigs_execbuffer_validate_buffer(struct vigs_device *vigs_dev,
+                                           struct vigs_validate_buffer *buffer,
+                                           struct list_head* list,
+                                           vigsp_surface_id sfc_id,
+                                           vigsp_cmd cmd,
+                                           int which,
+                                           void *data)
+{
+    struct vigs_surface *sfc = vigs_device_reference_surface(vigs_dev, sfc_id);
+    struct vigs_validate_buffer *tmp;
+
+    if (!sfc) {
+        DRM_ERROR("Surface %u not found\n", sfc_id);
+        return -EINVAL;
+    }
+
+    buffer->base.bo = &sfc->gem.bo;
+    buffer->cmd = cmd;
+    buffer->which = which;
+    buffer->data = data;
+
+    list_for_each_entry(tmp, list, base.head) {
+        if (tmp->base.bo == buffer->base.bo) {
+            /*
+             * Already on the list, we're done.
+             */
+            return 0;
+        }
+    }
+
+    list_add_tail(&buffer->base.head, list);
+
+    return 0;
+}
+
+static void vigs_execbuffer_clear_validation(struct vigs_validate_buffer *buffer)
+{
+    struct vigs_gem_object *gem = bo_to_vigs_gem(buffer->base.bo);
+
+    drm_gem_object_unreference(&gem->base);
+}
+
+static void vigs_execbuffer_destroy(struct vigs_gem_object *gem)
+{
+}
+
+int vigs_execbuffer_create(struct vigs_device *vigs_dev,
+                           unsigned long size,
+                           bool kernel,
+                           struct vigs_execbuffer **execbuffer)
+{
+    int ret = 0;
+
+    *execbuffer = kzalloc(sizeof(**execbuffer), GFP_KERNEL);
+
+    if (!*execbuffer) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    ret = vigs_gem_init(&(*execbuffer)->gem,
+                        vigs_dev,
+                        VIGS_GEM_TYPE_EXECBUFFER,
+                        size,
+                        kernel,
+                        &vigs_execbuffer_destroy);
+
+    if (ret != 0) {
+        goto fail1;
+    }
+
+    return 0;
+
+fail1:
+    *execbuffer = NULL;
+
+    return ret;
+}
+
+int vigs_execbuffer_validate_buffers(struct vigs_execbuffer *execbuffer,
+                                     struct list_head* list,
+                                     struct vigs_validate_buffer **buffers,
+                                     int *num_buffers,
+                                     bool *sync)
+{
+    struct vigs_device *vigs_dev = execbuffer->gem.base.dev->dev_private;
+    void *data = execbuffer->gem.kptr;
+    u32 data_size = vigs_gem_size(&execbuffer->gem);
+    struct vigsp_cmd_batch_header *batch_header = data;
+    struct vigsp_cmd_request_header *request_header =
+        (struct vigsp_cmd_request_header*)(batch_header + 1);
+    union vigs_request request;
+    int num_commands = 0, ret = 0;
+
+    *num_buffers = 0;
+    *sync = false;
+
+    /*
+     * GEM is always at least PAGE_SIZE long, so don't check
+     * if batch header is out of bounds.
+     */
+
+    while ((void*)request_header <
+           ((void*)(batch_header + 1) + batch_header->size)) {
+        if (((void*)(request_header) + sizeof(*request_header)) >
+            (data + data_size)) {
+            DRM_ERROR("request header outside of GEM\n");
+            ret = -EINVAL;
+            goto fail1;
+        }
+
+        if (((void*)(request_header + 1) + request_header->size) >
+            (data + data_size)) {
+            DRM_ERROR("request data outside of GEM\n");
+            ret = -EINVAL;
+            goto fail1;
+        }
+
+        request.data = (request_header + 1);
+
+        switch (request_header->cmd) {
+        case vigsp_cmd_update_vram:
+        case vigsp_cmd_update_gpu:
+            *sync = true;
+            *num_buffers += 1;
+            break;
+        case vigsp_cmd_copy:
+            *num_buffers += 2;
+            break;
+        case vigsp_cmd_solid_fill:
+            *num_buffers += 1;
+            break;
+        case vigsp_cmd_ga_copy:
+            *num_buffers += 2;
+            break;
+        default:
+            break;
+        }
+
+        request_header =
+            (struct vigsp_cmd_request_header*)(request.data +
+                                               request_header->size);
+
+        ++num_commands;
+    }
+
+    *buffers = kmalloc(*num_buffers * sizeof(**buffers), GFP_KERNEL);
+
+    if (!*buffers) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    request_header = (struct vigsp_cmd_request_header*)(batch_header + 1);
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    *num_buffers = 0;
+
+    while (--num_commands >= 0) {
+        request.data = (request_header + 1);
+
+        switch (request_header->cmd) {
+        case vigsp_cmd_update_vram:
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.update_vram->sfc_id,
+                                                  request_header->cmd,
+                                                  0,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            break;
+        case vigsp_cmd_update_gpu:
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.update_gpu->sfc_id,
+                                                  request_header->cmd,
+                                                  0,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            break;
+        case vigsp_cmd_copy:
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.copy->src_id,
+                                                  request_header->cmd,
+                                                  0,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.copy->dst_id,
+                                                  request_header->cmd,
+                                                  1,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            break;
+        case vigsp_cmd_solid_fill:
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.solid_fill->sfc_id,
+                                                  request_header->cmd,
+                                                  0,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            break;
+        case vigsp_cmd_ga_copy:
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.ga_copy->src_id,
+                                                  request_header->cmd,
+                                                  0,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            ret = vigs_execbuffer_validate_buffer(vigs_dev,
+                                                  &(*buffers)[*num_buffers],
+                                                  list,
+                                                  request.ga_copy->dst_id,
+                                                  request_header->cmd,
+                                                  1,
+                                                  request.data);
+
+            if (ret != 0) {
+                goto fail2;
+            }
+
+            ++*num_buffers;
+
+            break;
+        default:
+            break;
+        }
+
+        request_header =
+            (struct vigsp_cmd_request_header*)(request.data +
+                                               request_header->size);
+    }
+
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    return 0;
+
+fail2:
+    while (--*num_buffers >= 0) {
+        vigs_execbuffer_clear_validation(&(*buffers)[*num_buffers]);
+    }
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+    kfree(*buffers);
+fail1:
+    *buffers = NULL;
+
+    return ret;
+}
+
+void vigs_execbuffer_process_buffers(struct vigs_execbuffer *execbuffer,
+                                     struct vigs_validate_buffer *buffers,
+                                     int num_buffers,
+                                     bool *sync)
+{
+    union vigs_request request;
+    struct vigs_gem_object *gem;
+    struct vigs_surface *sfc;
+    int i;
+
+    for (i = 0; i < num_buffers; ++i) {
+        request.data = buffers[i].data;
+        gem = bo_to_vigs_gem(buffers[i].base.bo);
+        sfc = vigs_gem_to_vigs_surface(gem);
+
+        switch (buffers[i].cmd) {
+        case vigsp_cmd_update_vram:
+            if (vigs_gem_in_vram(&sfc->gem)) {
+                if (vigs_surface_need_vram_update(sfc)) {
+                    request.update_vram->offset = vigs_gem_offset(&sfc->gem);
+                    sfc->is_gpu_dirty = false;
+                } else {
+                    DRM_DEBUG_DRIVER("Surface %u doesn't need to be updated, ignoring update_vram\n",
+                                     request.update_vram->sfc_id);
+                    request.update_vram->sfc_id = 0;
+                }
+            } else {
+                DRM_DEBUG_DRIVER("Surface %u not in VRAM, ignoring update_vram\n",
+                                 request.update_vram->sfc_id);
+                request.update_vram->sfc_id = 0;
+            }
+            break;
+        case vigsp_cmd_update_gpu:
+            if (vigs_gem_in_vram(&sfc->gem)) {
+                if (vigs_surface_need_gpu_update(sfc)) {
+                    request.update_gpu->offset = vigs_gem_offset(&sfc->gem);
+                    sfc->is_gpu_dirty = false;
+                } else {
+                    DRM_DEBUG_DRIVER("Surface %u doesn't need to be updated, ignoring update_gpu\n",
+                                     request.update_gpu->sfc_id);
+                    request.update_gpu->sfc_id = 0;
+                }
+            } else {
+                DRM_DEBUG_DRIVER("Surface %u not in VRAM, ignoring update_gpu\n",
+                                 request.update_gpu->sfc_id);
+                request.update_gpu->sfc_id = 0;
+            }
+            break;
+        case vigsp_cmd_copy:
+            if (buffers[i].which && vigs_gem_in_vram(&sfc->gem)) {
+                sfc->is_gpu_dirty = true;
+            }
+            break;
+        case vigsp_cmd_solid_fill:
+            if (vigs_gem_in_vram(&sfc->gem)) {
+                sfc->is_gpu_dirty = true;
+            }
+            break;
+        case vigsp_cmd_ga_copy:
+            if (buffers[i].which && vigs_gem_in_vram(&sfc->gem)) {
+                sfc->is_gpu_dirty = true;
+            } else if (buffers[i].which == 0) {
+                if (vigs_gem_in_vram(&sfc->gem)) {
+                    request.ga_copy->src_scanout = true;
+                    request.ga_copy->src_offset = vigs_gem_offset(&sfc->gem);
+                    *sync = true;
+                } else {
+                    request.ga_copy->src_scanout = false;
+                    request.ga_copy->src_offset = 0;
+                }
+            }
+            break;
+        default:
+            break;
+        }
+    }
+}
+
+void vigs_execbuffer_fence(struct vigs_execbuffer *execbuffer,
+                           struct vigs_fence *fence)
+{
+    struct vigsp_cmd_batch_header *batch_header = execbuffer->gem.kptr;
+
+    batch_header->fence_seq = fence->seq;
+}
+
+void vigs_execbuffer_clear_validations(struct vigs_execbuffer *execbuffer,
+                                       struct vigs_validate_buffer *buffers,
+                                       int num_buffers)
+{
+    struct vigs_device *vigs_dev = execbuffer->gem.base.dev->dev_private;
+    int i;
+
+    mutex_lock(&vigs_dev->drm_dev->struct_mutex);
+
+    for (i = 0; i < num_buffers; ++i) {
+        vigs_execbuffer_clear_validation(&buffers[i]);
+    }
+
+    mutex_unlock(&vigs_dev->drm_dev->struct_mutex);
+
+    kfree(buffers);
+}
+
+int vigs_execbuffer_create_ioctl(struct drm_device *drm_dev,
+                                 void *data,
+                                 struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_create_execbuffer *args = data;
+    struct vigs_execbuffer *execbuffer = NULL;
+    uint32_t handle;
+    int ret;
+
+    ret = vigs_execbuffer_create(vigs_dev,
+                                 args->size,
+                                 false,
+                                 &execbuffer);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    ret = drm_gem_handle_create(file_priv,
+                                &execbuffer->gem.base,
+                                &handle);
+
+    drm_gem_object_unreference_unlocked(&execbuffer->gem.base);
+
+    if (ret == 0) {
+        args->size = vigs_gem_size(&execbuffer->gem);
+        args->handle = handle;
+    }
+
+    return ret;
+}
+
+int vigs_execbuffer_exec_ioctl(struct drm_device *drm_dev,
+                               void *data,
+                               struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_exec *args = data;
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+    struct vigs_execbuffer *execbuffer;
+    struct ww_acquire_ctx ticket;
+    struct list_head list;
+    struct vigs_validate_buffer *buffers;
+    int num_buffers = 0;
+    struct vigs_fence *fence = NULL;
+    bool sync = false;
+    int ret = 0;
+
+    INIT_LIST_HEAD(&list);
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, args->handle);
+
+    if (gem == NULL) {
+        ret = -ENOENT;
+        goto out1;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    if (vigs_gem->type != VIGS_GEM_TYPE_EXECBUFFER) {
+        ret = -ENOENT;
+        goto out2;
+    }
+
+    execbuffer = vigs_gem_to_vigs_execbuffer(vigs_gem);
+
+    vigs_gem_reserve(vigs_gem);
+
+    /*
+     * Never unmap for optimization, but we got to be careful,
+     * worst case scenario is when whole RAM BAR is mapped into kernel.
+     */
+    ret = vigs_gem_kmap(vigs_gem);
+
+    if (ret != 0) {
+        vigs_gem_unreserve(vigs_gem);
+        goto out2;
+    }
+
+    vigs_gem_unreserve(vigs_gem);
+
+    ret = vigs_execbuffer_validate_buffers(execbuffer,
+                                           &list,
+                                           &buffers,
+                                           &num_buffers,
+                                           &sync);
+
+    if (ret != 0) {
+        goto out2;
+    }
+
+    if (list_empty(&list)) {
+        vigs_comm_exec(vigs_dev->comm, execbuffer);
+    } else {
+        ret = ttm_eu_reserve_buffers(&ticket, &list);
+
+        if (ret != 0) {
+            goto out3;
+        }
+
+        ret = vigs_fence_create(vigs_dev->fenceman, &fence);
+
+        if (ret != 0) {
+            ttm_eu_backoff_reservation(&ticket, &list);
+            goto out3;
+        }
+
+        vigs_execbuffer_process_buffers(execbuffer, buffers, num_buffers, &sync);
+
+        vigs_execbuffer_fence(execbuffer, fence);
+
+        vigs_comm_exec(vigs_dev->comm, execbuffer);
+
+        ttm_eu_fence_buffer_objects(&ticket, &list, fence);
+
+        if (sync) {
+            vigs_fence_wait(fence, false);
+        }
+
+        vigs_fence_unref(fence);
+    }
+
+out3:
+    vigs_execbuffer_clear_validations(execbuffer, buffers, num_buffers);
+out2:
+    drm_gem_object_unreference_unlocked(gem);
+out1:
+    return ret;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_execbuffer.h b/drivers/gpu/drm/vigs/vigs_execbuffer.h
new file mode 100644
index 0000000..0564954
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_execbuffer.h
@@ -0,0 +1,75 @@
+#ifndef _VIGS_EXECBUFFER_H_
+#define _VIGS_EXECBUFFER_H_
+
+#include "drmP.h"
+#include "vigs_gem.h"
+#include "vigs_protocol.h"
+#include <ttm/ttm_execbuf_util.h>
+
+struct vigs_fence;
+
+struct vigs_validate_buffer
+{
+    struct ttm_validate_buffer base;
+
+    vigsp_cmd cmd;
+
+    int which;
+
+    void *data;
+};
+
+struct vigs_execbuffer
+{
+    /*
+     * Must be first member!
+     */
+    struct vigs_gem_object gem;
+};
+
+static inline struct vigs_execbuffer *vigs_gem_to_vigs_execbuffer(struct vigs_gem_object *vigs_gem)
+{
+    return container_of(vigs_gem, struct vigs_execbuffer, gem);
+}
+
+int vigs_execbuffer_create(struct vigs_device *vigs_dev,
+                           unsigned long size,
+                           bool kernel,
+                           struct vigs_execbuffer **execbuffer);
+
+int vigs_execbuffer_validate_buffers(struct vigs_execbuffer *execbuffer,
+                                     struct list_head* list,
+                                     struct vigs_validate_buffer **buffers,
+                                     int *num_buffers,
+                                     bool *sync);
+
+void vigs_execbuffer_process_buffers(struct vigs_execbuffer *execbuffer,
+                                     struct vigs_validate_buffer *buffers,
+                                     int num_buffers,
+                                     bool *sync);
+
+void vigs_execbuffer_fence(struct vigs_execbuffer *execbuffer,
+                           struct vigs_fence *fence);
+
+void vigs_execbuffer_clear_validations(struct vigs_execbuffer *execbuffer,
+                                       struct vigs_validate_buffer *buffers,
+                                       int num_buffers);
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_execbuffer_create_ioctl(struct drm_device *drm_dev,
+                                 void *data,
+                                 struct drm_file *file_priv);
+
+int vigs_execbuffer_exec_ioctl(struct drm_device *drm_dev,
+                               void *data,
+                               struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_fbdev.c b/drivers/gpu/drm/vigs/vigs_fbdev.c
new file mode 100644
index 0000000..6447667
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fbdev.c
@@ -0,0 +1,566 @@
+#include "vigs_fbdev.h"
+#include "vigs_device.h"
+#include "vigs_surface.h"
+#include "vigs_framebuffer.h"
+#include "vigs_output.h"
+#include "vigs_crtc.h"
+#include "drm_crtc_helper.h"
+#include <drm/vigs_drm.h>
+
+/*
+ * From drm_fb_helper.c, modified to work with 'regno' > 16.
+ * @{
+ */
+
+static bool vigs_fbdev_helper_is_bound(struct drm_fb_helper *fb_helper)
+{
+    struct drm_device *dev = fb_helper->dev;
+    struct drm_crtc *crtc;
+    int bound = 0, crtcs_bound = 0;
+
+    list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+        if (crtc->fb) {
+            crtcs_bound++;
+        }
+
+        if (crtc->fb == fb_helper->fb) {
+            bound++;
+        }
+    }
+
+    if (bound < crtcs_bound) {
+        return false;
+    }
+
+    return true;
+}
+
+static int vigs_fbdev_setcolreg(struct drm_crtc *crtc, u16 red, u16 green,
+                                u16 blue, u16 regno, struct fb_info *fbi)
+{
+    struct drm_fb_helper *fb_helper = fbi->par;
+    struct drm_framebuffer *fb = fb_helper->fb;
+    int pindex;
+
+    if (fbi->fix.visual == FB_VISUAL_TRUECOLOR) {
+        u32 *palette;
+        u32 value;
+        /* place color in psuedopalette */
+        if (regno <= 16) {
+            palette = (u32*)fbi->pseudo_palette;
+            red >>= (16 - fbi->var.red.length);
+            green >>= (16 - fbi->var.green.length);
+            blue >>= (16 - fbi->var.blue.length);
+            value = (red << fbi->var.red.offset) |
+                    (green << fbi->var.green.offset) |
+                    (blue << fbi->var.blue.offset);
+            if (fbi->var.transp.length > 0) {
+                u32 mask = (1 << fbi->var.transp.length) - 1;
+                mask <<= fbi->var.transp.offset;
+                value |= mask;
+            }
+            palette[regno] = value;
+        }
+        return 0;
+    }
+
+    /*
+     * The driver really shouldn't advertise pseudo/directcolor
+     * visuals if it can't deal with the palette.
+     */
+    if (WARN_ON(!fb_helper->funcs->gamma_set ||
+                !fb_helper->funcs->gamma_get)) {
+        return -EINVAL;
+    }
+
+    pindex = regno;
+
+    if (fb->bits_per_pixel == 16) {
+        pindex = regno << 3;
+
+        if ((fb->depth == 16) && (regno > 63)) {
+            return -EINVAL;
+        }
+
+        if ((fb->depth == 15) && (regno > 31)) {
+            return -EINVAL;
+        }
+
+        if (fb->depth == 16) {
+            u16 r, g, b;
+            int i;
+
+            if (regno < 32) {
+                for (i = 0; i < 8; i++) {
+                    fb_helper->funcs->gamma_set(crtc, red,
+                        green, blue, pindex + i);
+                }
+            }
+
+            fb_helper->funcs->gamma_get(crtc, &r,
+                &g, &b,
+                (pindex >> 1));
+
+            for (i = 0; i < 4; i++) {
+                fb_helper->funcs->gamma_set(crtc, r,
+                    green, b,
+                    (pindex >> 1) + i);
+            }
+        }
+    }
+
+    if (fb->depth != 16) {
+        fb_helper->funcs->gamma_set(crtc, red, green, blue, pindex);
+    }
+
+    return 0;
+}
+
+static int vigs_fbdev_setcmap(struct fb_cmap *cmap, struct fb_info *fbi)
+{
+    struct drm_fb_helper *fb_helper = fbi->par;
+    struct drm_device *dev = fb_helper->dev;
+    struct drm_crtc_helper_funcs *crtc_funcs;
+    u16 *red, *green, *blue, *transp;
+    struct drm_crtc *crtc;
+    int i, j, ret = 0;
+    int start;
+
+    drm_modeset_lock_all(dev);
+    if (!vigs_fbdev_helper_is_bound(fb_helper)) {
+        drm_modeset_unlock_all(dev);
+        return -EBUSY;
+    }
+
+    for (i = 0; i < fb_helper->crtc_count; i++) {
+        crtc = fb_helper->crtc_info[i].mode_set.crtc;
+        crtc_funcs = crtc->helper_private;
+
+        red = cmap->red;
+        green = cmap->green;
+        blue = cmap->blue;
+        transp = cmap->transp;
+        start = cmap->start;
+
+        for (j = 0; j < cmap->len; j++) {
+            u16 hred, hgreen, hblue, htransp = 0xffff;
+
+            hred = *red++;
+            hgreen = *green++;
+            hblue = *blue++;
+
+            if (transp) {
+                htransp = *transp++;
+            }
+
+            ret = vigs_fbdev_setcolreg(crtc, hred, hgreen, hblue, start++, fbi);
+
+            if (ret != 0) {
+                goto out;
+            }
+        }
+
+        if (crtc_funcs->load_lut) {
+            crtc_funcs->load_lut(crtc);
+        }
+    }
+
+ out:
+    drm_modeset_unlock_all(dev);
+    return ret;
+}
+
+/*
+ * @}
+ */
+
+static int vigs_fbdev_set_par(struct fb_info *fbi)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    return drm_fb_helper_set_par(fbi);
+}
+
+/*
+ * This is 'drm_fb_helper_dpms' modified to set 'in_dpms'
+ * flag inside drm_modeset_lock_all.
+ */
+static void vigs_fbdev_dpms(struct fb_info *fbi, int dpms_mode)
+{
+    struct drm_fb_helper *fb_helper = fbi->par;
+    struct drm_device *dev = fb_helper->dev;
+    struct vigs_device *vigs_dev = dev->dev_private;
+    struct drm_crtc *crtc;
+    struct vigs_crtc *vigs_crtc;
+    struct drm_connector *connector;
+    int i, j;
+
+    /*
+     * fbdev->blank can be called from irq context in case of a panic.
+     * Since we already have our own special panic handler which will
+     * restore the fbdev console mode completely, just bail out early.
+     */
+    if (oops_in_progress) {
+        return;
+    }
+
+    if (vigs_dev->in_dpms) {
+        /*
+         * If this is called from 'vigs_crtc_dpms' then we just
+         * return in order to not deadlock. Note that it's
+         * correct to check this flag here without any locks
+         * being held since 'fb_blank' callback is already called with
+         * console lock being held and 'vigs_crtc_dpms' only sets in_dpms
+         * inside the console lock.
+         */
+        return;
+    }
+
+    /*
+     * For each CRTC in this fb, turn the connectors on/off.
+     */
+    drm_modeset_lock_all(dev);
+    if (!vigs_fbdev_helper_is_bound(fb_helper)) {
+        drm_modeset_unlock_all(dev);
+        return;
+    }
+
+    for (i = 0; i < fb_helper->crtc_count; i++) {
+        crtc = fb_helper->crtc_info[i].mode_set.crtc;
+        vigs_crtc = crtc_to_vigs_crtc(crtc);
+
+        if (!crtc->enabled) {
+            continue;
+        }
+
+        vigs_dev->in_dpms = true;
+
+        /* Walk the connectors & encoders on this fb turning them on/off */
+        for (j = 0; j < fb_helper->connector_count; j++) {
+            connector = fb_helper->connector_info[j]->connector;
+            connector->funcs->dpms(connector, dpms_mode);
+            drm_object_property_set_value(&connector->base,
+                dev->mode_config.dpms_property, dpms_mode);
+        }
+
+        vigs_dev->in_dpms = false;
+    }
+
+    drm_modeset_unlock_all(dev);
+}
+
+/*
+ * This is 'drm_fb_helper_blank' modified to use
+ * 'vigs_fbdev_dpms'.
+ */
+static int vigs_fbdev_blank(int blank, struct fb_info *fbi)
+{
+    switch (blank) {
+    /* Display: On; HSync: On, VSync: On */
+    case FB_BLANK_UNBLANK:
+        vigs_fbdev_dpms(fbi, DRM_MODE_DPMS_ON);
+        break;
+    /* Display: Off; HSync: On, VSync: On */
+    case FB_BLANK_NORMAL:
+        vigs_fbdev_dpms(fbi, DRM_MODE_DPMS_STANDBY);
+        break;
+    /* Display: Off; HSync: Off, VSync: On */
+    case FB_BLANK_HSYNC_SUSPEND:
+        vigs_fbdev_dpms(fbi, DRM_MODE_DPMS_STANDBY);
+        break;
+    /* Display: Off; HSync: On, VSync: Off */
+    case FB_BLANK_VSYNC_SUSPEND:
+        vigs_fbdev_dpms(fbi, DRM_MODE_DPMS_SUSPEND);
+        break;
+    /* Display: Off; HSync: Off, VSync: Off */
+    case FB_BLANK_POWERDOWN:
+        vigs_fbdev_dpms(fbi, DRM_MODE_DPMS_OFF);
+        break;
+    }
+
+    return 0;
+}
+
+static struct fb_ops vigs_fbdev_ops =
+{
+    .owner = THIS_MODULE,
+    .fb_fillrect = cfb_fillrect,
+    .fb_copyarea = cfb_copyarea,
+    .fb_imageblit = cfb_imageblit,
+    .fb_check_var = drm_fb_helper_check_var,
+    .fb_set_par = vigs_fbdev_set_par,
+    .fb_blank = vigs_fbdev_blank,
+    .fb_pan_display = drm_fb_helper_pan_display,
+    .fb_setcmap = vigs_fbdev_setcmap,
+    .fb_debug_enter = drm_fb_helper_debug_enter,
+    .fb_debug_leave = drm_fb_helper_debug_leave,
+};
+
+static int vigs_fbdev_probe_once(struct drm_fb_helper *helper,
+                                 struct drm_fb_helper_surface_size *sizes)
+{
+    struct vigs_fbdev *vigs_fbdev = fbdev_to_vigs_fbdev(helper);
+    struct vigs_device *vigs_dev = helper->dev->dev_private;
+    struct vigs_surface *fb_sfc;
+    struct vigs_framebuffer *vigs_fb;
+    struct fb_info *fbi;
+    struct drm_mode_fb_cmd2 mode_cmd = { 0 };
+    vigsp_surface_format format;
+    unsigned long offset;
+    int dpi;
+    int ret;
+    struct drm_connector *connector;
+
+    DRM_DEBUG_KMS("%dx%dx%d\n",
+                  sizes->surface_width,
+                  sizes->surface_height,
+                  sizes->surface_bpp);
+
+    mode_cmd.width = sizes->surface_width;
+    mode_cmd.height = sizes->surface_height;
+    mode_cmd.pitches[0] = sizes->surface_width * (sizes->surface_bpp >> 3);
+    mode_cmd.pixel_format = drm_mode_legacy_fb_format(sizes->surface_bpp,
+                                                      sizes->surface_depth);
+
+    switch (mode_cmd.pixel_format) {
+    case DRM_FORMAT_XRGB8888:
+        format = vigsp_surface_bgrx8888;
+        break;
+    case DRM_FORMAT_ARGB8888:
+        format = vigsp_surface_bgra8888;
+        break;
+    default:
+        DRM_DEBUG_KMS("unsupported pixel format: %u\n", mode_cmd.pixel_format);
+        ret = -EINVAL;
+        goto fail1;
+    }
+
+    fbi = framebuffer_alloc(0, &vigs_dev->pci_dev->dev);
+
+    if (!fbi) {
+        DRM_ERROR("failed to allocate fb info\n");
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    ret = vigs_surface_create(vigs_dev,
+                              mode_cmd.width,
+                              mode_cmd.height,
+                              mode_cmd.pitches[0],
+                              format,
+                              true,
+                              &fb_sfc);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    ret = vigs_framebuffer_create(vigs_dev,
+                                  &mode_cmd,
+                                  fb_sfc,
+                                  &vigs_fb);
+
+    drm_gem_object_unreference_unlocked(&fb_sfc->gem.base);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    helper->fb = &vigs_fb->base;
+    helper->fbdev = fbi;
+
+    fbi->par = helper;
+    fbi->flags = FBINFO_DEFAULT | FBINFO_CAN_FORCE_OUTPUT;
+    fbi->fbops = &vigs_fbdev_ops;
+
+    ret = fb_alloc_cmap(&fbi->cmap, 256, 0);
+
+    if (ret != 0) {
+        DRM_ERROR("failed to allocate cmap\n");
+        goto fail3;
+    }
+
+    /*
+     * This is a hack to make fbdev work without calling
+     * 'vigs_framebuffer_pin'. VRAM is precious resource and we
+     * don't want to give it away to fbdev just to show
+     * that "kernel loading" thing. Here we assume that
+     * GEM zero is always located at offset 0 in VRAM and just map
+     * it and give it to fbdev. If later, when X starts for example,
+     * one will attempt to write to /dev/fb0 then he'll probably
+     * write to some GEM's memory, but we don't care.
+     */
+    vigs_fbdev->kptr = ioremap(vigs_dev->vram_base,
+                               vigs_gem_size(&fb_sfc->gem));
+
+    if (!vigs_fbdev->kptr) {
+        goto fail4;
+    }
+
+    strcpy(fbi->fix.id, "VIGS");
+
+    drm_fb_helper_fill_fix(fbi, vigs_fb->base.pitches[0], vigs_fb->base.depth);
+    drm_fb_helper_fill_var(fbi, helper, vigs_fb->base.width, vigs_fb->base.height);
+
+    /*
+     * Setup DPI.
+     * @{
+     */
+
+    dpi = vigs_output_get_dpi();
+    fbi->var.height = vigs_output_get_phys_height(dpi, fbi->var.yres);
+    fbi->var.width = vigs_output_get_phys_width(dpi, fbi->var.xres);
+
+    /*
+     * Walk all connectors and set display_info.
+     */
+
+    list_for_each_entry(connector, &vigs_dev->drm_dev->mode_config.connector_list, head) {
+        connector->display_info.width_mm = fbi->var.width;
+        connector->display_info.height_mm = fbi->var.height;
+    }
+
+    /*
+     * @}
+     */
+
+    /*
+     * TODO: Play around with xoffset/yoffset, make sure this code works.
+     */
+
+    offset = fbi->var.xoffset * (vigs_fb->base.bits_per_pixel >> 3);
+    offset += fbi->var.yoffset * vigs_fb->base.pitches[0];
+
+    /*
+     * TODO: "vram_base + ..." - not nice, make a function for this.
+     */
+    fbi->fix.smem_start = vigs_dev->vram_base +
+                          0 +
+                          offset;
+    fbi->screen_base = vigs_fbdev->kptr + offset;
+    fbi->screen_size = fbi->fix.smem_len = vigs_fb->base.width *
+                                           vigs_fb->base.height *
+                                           (vigs_fb->base.bits_per_pixel >> 3);
+
+    return 0;
+
+fail4:
+    fb_dealloc_cmap(&fbi->cmap);
+fail3:
+    helper->fb = NULL;
+    helper->fbdev = NULL;
+fail2:
+    framebuffer_release(fbi);
+fail1:
+
+    return ret;
+}
+
+static int vigs_fbdev_probe(struct drm_fb_helper *helper,
+                            struct drm_fb_helper_surface_size *sizes)
+{
+    int ret = 0;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    /*
+     * With !helper->fb, it means that this function is called first time
+     * and after that, the helper->fb would be used as clone mode.
+     */
+
+    if (!helper->fb) {
+        ret = vigs_fbdev_probe_once(helper, sizes);
+
+        if (ret >= 0) {
+            ret = 1;
+        }
+    }
+
+    return ret;
+}
+
+static struct drm_fb_helper_funcs vigs_fbdev_funcs =
+{
+    .fb_probe = vigs_fbdev_probe,
+};
+
+int vigs_fbdev_create(struct vigs_device *vigs_dev,
+                      struct vigs_fbdev **vigs_fbdev)
+{
+    int ret = 0;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    *vigs_fbdev = kzalloc(sizeof(**vigs_fbdev), GFP_KERNEL);
+
+    if (!*vigs_fbdev) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    (*vigs_fbdev)->base.funcs = &vigs_fbdev_funcs;
+
+    ret = drm_fb_helper_init(vigs_dev->drm_dev,
+                             &(*vigs_fbdev)->base,
+                             1, 1);
+
+    if (ret != 0) {
+        DRM_ERROR("unable to init fb_helper: %d\n", ret);
+        goto fail2;
+    }
+
+    drm_fb_helper_single_add_all_connectors(&(*vigs_fbdev)->base);
+    drm_fb_helper_initial_config(&(*vigs_fbdev)->base, 32);
+
+    return 0;
+
+fail2:
+    kfree(*vigs_fbdev);
+fail1:
+    *vigs_fbdev = NULL;
+
+    return ret;
+}
+
+void vigs_fbdev_destroy(struct vigs_fbdev *vigs_fbdev)
+{
+    struct fb_info *fbi = vigs_fbdev->base.fbdev;
+    struct drm_framebuffer *fb;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    if (fbi) {
+        unregister_framebuffer(fbi);
+        fb_dealloc_cmap(&fbi->cmap);
+        framebuffer_release(fbi);
+    }
+
+    fb = vigs_fbdev->base.fb;
+
+    drm_fb_helper_fini(&vigs_fbdev->base);
+
+    if (vigs_fbdev->kptr) {
+        iounmap(vigs_fbdev->kptr);
+    }
+
+    drm_framebuffer_unregister_private(fb);
+    drm_framebuffer_remove(fb);
+
+    kfree(vigs_fbdev);
+}
+
+void vigs_fbdev_output_poll_changed(struct vigs_fbdev *vigs_fbdev)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    drm_fb_helper_hotplug_event(&vigs_fbdev->base);
+}
+
+void vigs_fbdev_restore_mode(struct vigs_fbdev *vigs_fbdev)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    drm_modeset_lock_all(vigs_fbdev->base.dev);
+    drm_fb_helper_restore_fbdev_mode(&vigs_fbdev->base);
+    drm_modeset_unlock_all(vigs_fbdev->base.dev);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_fbdev.h b/drivers/gpu/drm/vigs/vigs_fbdev.h
new file mode 100644
index 0000000..81a997e
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fbdev.h
@@ -0,0 +1,30 @@
+#ifndef _VIGS_FBDEV_H_
+#define _VIGS_FBDEV_H_
+
+#include "drmP.h"
+#include "drm_fb_helper.h"
+
+struct vigs_device;
+
+struct vigs_fbdev
+{
+    struct drm_fb_helper base;
+
+    void __iomem *kptr;
+};
+
+static inline struct vigs_fbdev *fbdev_to_vigs_fbdev(struct drm_fb_helper *fbdev)
+{
+    return container_of(fbdev, struct vigs_fbdev, base);
+}
+
+int vigs_fbdev_create(struct vigs_device *vigs_dev,
+                      struct vigs_fbdev **vigs_fbdev);
+
+void vigs_fbdev_destroy(struct vigs_fbdev *vigs_fbdev);
+
+void vigs_fbdev_output_poll_changed(struct vigs_fbdev *vigs_fbdev);
+
+void vigs_fbdev_restore_mode(struct vigs_fbdev *vigs_fbdev);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_fence.c b/drivers/gpu/drm/vigs/vigs_fence.c
new file mode 100644
index 0000000..8a9b349
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fence.c
@@ -0,0 +1,305 @@
+#include "vigs_fence.h"
+#include "vigs_fenceman.h"
+#include "vigs_file.h"
+#include "vigs_device.h"
+#include "vigs_comm.h"
+#include <drm/vigs_drm.h>
+
+static void vigs_fence_cleanup(struct vigs_fence *fence)
+{
+}
+
+static void vigs_fence_destroy(struct vigs_fence *fence)
+{
+    vigs_fence_cleanup(fence);
+    kfree(fence);
+}
+
+static void vigs_user_fence_destroy(struct vigs_fence *fence)
+{
+    struct vigs_user_fence *user_fence = vigs_fence_to_vigs_user_fence(fence);
+
+    vigs_fence_cleanup(&user_fence->fence);
+    ttm_base_object_kfree(user_fence, base);
+}
+
+static void vigs_fence_release_locked(struct kref *kref)
+{
+    struct vigs_fence *fence = kref_to_vigs_fence(kref);
+
+    DRM_DEBUG_DRIVER("Fence destroyed (seq = %u, signaled = %u)\n",
+                     fence->seq,
+                     fence->signaled);
+
+    list_del_init(&fence->list);
+    fence->destroy(fence);
+}
+
+static void vigs_user_fence_refcount_release(struct ttm_base_object **base)
+{
+    struct ttm_base_object *tmp = *base;
+    struct vigs_user_fence *user_fence = base_to_vigs_user_fence(tmp);
+
+    vigs_fence_unref(&user_fence->fence);
+    *base = NULL;
+}
+
+static void vigs_fence_init(struct vigs_fence *fence,
+                            struct vigs_fenceman *fenceman,
+                            void (*destroy)(struct vigs_fence*))
+{
+    unsigned long flags;
+
+    kref_init(&fence->kref);
+    INIT_LIST_HEAD(&fence->list);
+    fence->fenceman = fenceman;
+    fence->signaled = false;
+    init_waitqueue_head(&fence->wait);
+    fence->destroy = destroy;
+
+    spin_lock_irqsave(&fenceman->lock, flags);
+
+    fence->seq = vigs_fence_seq_next(fenceman->seq);
+    fenceman->seq = fence->seq;
+
+    list_add_tail(&fence->list, &fenceman->fence_list);
+
+    spin_unlock_irqrestore(&fenceman->lock, flags);
+
+    DRM_DEBUG_DRIVER("Fence created (seq = %u)\n", fence->seq);
+}
+
+int vigs_fence_create(struct vigs_fenceman *fenceman,
+                      struct vigs_fence **fence)
+{
+    int ret = 0;
+
+    *fence = kzalloc(sizeof(**fence), GFP_KERNEL);
+
+    if (!*fence) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    vigs_fence_init(*fence, fenceman, &vigs_fence_destroy);
+
+    return 0;
+
+fail1:
+    *fence = NULL;
+
+    return ret;
+}
+
+int vigs_user_fence_create(struct vigs_fenceman *fenceman,
+                           struct drm_file *file_priv,
+                           struct vigs_user_fence **user_fence,
+                           uint32_t *handle)
+{
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+    int ret = 0;
+
+    *user_fence = kzalloc(sizeof(**user_fence), GFP_KERNEL);
+
+    if (!*user_fence) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    vigs_fence_init(&(*user_fence)->fence, fenceman, &vigs_user_fence_destroy);
+
+    ret = ttm_base_object_init(vigs_file->obj_file,
+                               &(*user_fence)->base, false,
+                               VIGS_FENCE_TYPE,
+                               &vigs_user_fence_refcount_release,
+                               NULL);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    /*
+     * For ttm_base_object.
+     */
+    vigs_fence_ref(&(*user_fence)->fence);
+
+    *handle = (*user_fence)->base.hash.key;
+
+    return 0;
+
+fail2:
+    vigs_fence_cleanup(&(*user_fence)->fence);
+    kfree(*user_fence);
+fail1:
+    *user_fence = NULL;
+
+    return ret;
+}
+
+int vigs_fence_wait(struct vigs_fence *fence, bool interruptible)
+{
+    long ret = 0;
+
+    if (vigs_fence_signaled(fence)) {
+        DRM_DEBUG_DRIVER("Fence wait (seq = %u, signaled = %u)\n",
+                         fence->seq,
+                         fence->signaled);
+        return 0;
+    }
+
+    DRM_DEBUG_DRIVER("Fence wait (seq = %u)\n", fence->seq);
+
+    if (interruptible) {
+        ret = wait_event_interruptible(fence->wait, vigs_fence_signaled(fence));
+    } else {
+        wait_event(fence->wait, vigs_fence_signaled(fence));
+    }
+
+    if (ret != 0) {
+        DRM_INFO("Fence wait interrupted (seq = %u) = %ld\n", fence->seq, ret);
+    } else {
+        DRM_DEBUG_DRIVER("Fence wait done (seq = %u)\n", fence->seq);
+    }
+
+    return ret;
+}
+
+bool vigs_fence_signaled(struct vigs_fence *fence)
+{
+    unsigned long flags;
+    bool signaled;
+
+    spin_lock_irqsave(&fence->fenceman->lock, flags);
+
+    signaled = fence->signaled;
+
+    spin_unlock_irqrestore(&fence->fenceman->lock, flags);
+
+    return signaled;
+}
+
+void vigs_fence_ref(struct vigs_fence *fence)
+{
+    if (unlikely(!fence)) {
+        return;
+    }
+
+    kref_get(&fence->kref);
+}
+
+void vigs_fence_unref(struct vigs_fence *fence)
+{
+    struct vigs_fenceman *fenceman;
+
+    if (unlikely(!fence)) {
+        return;
+    }
+
+    fenceman = fence->fenceman;
+
+    spin_lock_irq(&fenceman->lock);
+    BUG_ON(atomic_read(&fence->kref.refcount) == 0);
+    kref_put(&fence->kref, vigs_fence_release_locked);
+    spin_unlock_irq(&fenceman->lock);
+}
+
+int vigs_fence_create_ioctl(struct drm_device *drm_dev,
+                            void *data,
+                            struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+    struct drm_vigs_create_fence *args = data;
+    struct vigs_user_fence *user_fence;
+    uint32_t handle;
+    int ret;
+
+    ret = vigs_user_fence_create(vigs_dev->fenceman,
+                                 file_priv,
+                                 &user_fence,
+                                 &handle);
+
+    if (ret != 0) {
+        goto out;
+    }
+
+    if (args->send) {
+        ret = vigs_comm_fence(vigs_dev->comm, &user_fence->fence);
+
+        if (ret != 0) {
+            ttm_ref_object_base_unref(vigs_file->obj_file,
+                                      handle,
+                                      TTM_REF_USAGE);
+            goto out;
+        }
+    }
+
+    args->handle = handle;
+    args->seq = user_fence->fence.seq;
+
+out:
+    vigs_fence_unref(&user_fence->fence);
+
+    return ret;
+}
+
+int vigs_fence_wait_ioctl(struct drm_device *drm_dev,
+                          void *data,
+                          struct drm_file *file_priv)
+{
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+    struct drm_vigs_fence_wait *args = data;
+    struct ttm_base_object *base;
+    struct vigs_user_fence *user_fence;
+    int ret;
+
+    base = ttm_base_object_lookup(vigs_file->obj_file, args->handle);
+
+    if (!base) {
+        return -ENOENT;
+    }
+
+    user_fence = base_to_vigs_user_fence(base);
+
+    ret = vigs_fence_wait(&user_fence->fence, true);
+
+    ttm_base_object_unref(&base);
+
+    return ret;
+}
+
+int vigs_fence_signaled_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv)
+{
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+    struct drm_vigs_fence_signaled *args = data;
+    struct ttm_base_object *base;
+    struct vigs_user_fence *user_fence;
+
+    base = ttm_base_object_lookup(vigs_file->obj_file, args->handle);
+
+    if (!base) {
+        return -ENOENT;
+    }
+
+    user_fence = base_to_vigs_user_fence(base);
+
+    args->signaled = vigs_fence_signaled(&user_fence->fence);
+
+    ttm_base_object_unref(&base);
+
+    return 0;
+}
+
+int vigs_fence_unref_ioctl(struct drm_device *drm_dev,
+                           void *data,
+                           struct drm_file *file_priv)
+{
+    struct vigs_file *vigs_file = file_priv->driver_priv;
+    struct drm_vigs_fence_unref *args = data;
+
+    return ttm_ref_object_base_unref(vigs_file->obj_file,
+                                     args->handle,
+                                     TTM_REF_USAGE);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_fence.h b/drivers/gpu/drm/vigs/vigs_fence.h
new file mode 100644
index 0000000..c0c41be
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fence.h
@@ -0,0 +1,123 @@
+#ifndef _VIGS_FENCE_H_
+#define _VIGS_FENCE_H_
+
+#include "drmP.h"
+#include <ttm/ttm_object.h>
+
+#define VIGS_FENCE_TYPE ttm_driver_type2
+
+struct vigs_fenceman;
+
+struct vigs_fence
+{
+    struct kref kref;
+
+    struct list_head list;
+
+    struct vigs_fenceman *fenceman;
+
+    uint32_t seq;
+
+    bool signaled;
+
+    wait_queue_head_t wait;
+
+    void (*destroy)(struct vigs_fence *fence);
+};
+
+/*
+ * Users can access fences via TTM base object mechanism,
+ * thus, we need to wrap vigs_fence into vigs_user_fence because
+ * not every fence object needs to be referenced from user space.
+ * So no point in always having struct ttm_base_object inside vigs_fence.
+ */
+
+struct vigs_user_fence
+{
+    struct ttm_base_object base;
+
+    struct vigs_fence fence;
+};
+
+static inline struct vigs_fence *kref_to_vigs_fence(struct kref *kref)
+{
+    return container_of(kref, struct vigs_fence, kref);
+}
+
+static inline struct vigs_user_fence *vigs_fence_to_vigs_user_fence(struct vigs_fence *fence)
+{
+    return container_of(fence, struct vigs_user_fence, fence);
+}
+
+static inline struct vigs_user_fence *base_to_vigs_user_fence(struct ttm_base_object *base)
+{
+    return container_of(base, struct vigs_user_fence, base);
+}
+
+static inline uint32_t vigs_fence_seq_next(uint32_t seq)
+{
+    if (++seq == 0) {
+        ++seq;
+    }
+    return seq;
+}
+
+#define vigs_fence_seq_num_after(a, b) \
+    (typecheck(u32, a) && typecheck(u32, b) && ((s32)(b) - (s32)(a) < 0))
+
+#define vigs_fence_seq_num_before(a, b) vigs_fence_seq_num_after(b, a)
+
+#define vigs_fence_seq_num_after_eq(a, b)  \
+    ( typecheck(u32, a) && typecheck(u32, b) && \
+      ((s32)(a) - (s32)(b) >= 0) )
+
+#define vigs_fence_seq_num_before_eq(a, b) vigs_fence_seq_num_after_eq(b, a)
+
+int vigs_fence_create(struct vigs_fenceman *fenceman,
+                      struct vigs_fence **fence);
+
+int vigs_user_fence_create(struct vigs_fenceman *fenceman,
+                           struct drm_file *file_priv,
+                           struct vigs_user_fence **user_fence,
+                           uint32_t *handle);
+
+int vigs_fence_wait(struct vigs_fence *fence, bool interruptible);
+
+bool vigs_fence_signaled(struct vigs_fence *fence);
+
+/*
+ * Passing NULL won't hurt, this is for convenience.
+ */
+void vigs_fence_ref(struct vigs_fence *fence);
+
+/*
+ * Passing NULL won't hurt, this is for convenience.
+ */
+void vigs_fence_unref(struct vigs_fence *fence);
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_fence_create_ioctl(struct drm_device *drm_dev,
+                            void *data,
+                            struct drm_file *file_priv);
+
+int vigs_fence_wait_ioctl(struct drm_device *drm_dev,
+                          void *data,
+                          struct drm_file *file_priv);
+
+int vigs_fence_signaled_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv);
+
+int vigs_fence_unref_ioctl(struct drm_device *drm_dev,
+                           void *data,
+                           struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_fenceman.c b/drivers/gpu/drm/vigs/vigs_fenceman.c
new file mode 100644
index 0000000..c551852c
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fenceman.c
@@ -0,0 +1,65 @@
+#include "vigs_fenceman.h"
+#include "vigs_fence.h"
+
+int vigs_fenceman_create(struct vigs_fenceman **fenceman)
+{
+    int ret = 0;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    *fenceman = kzalloc(sizeof(**fenceman), GFP_KERNEL);
+
+    if (!*fenceman) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    spin_lock_init(&(*fenceman)->lock);
+    INIT_LIST_HEAD(&(*fenceman)->fence_list);
+    (*fenceman)->seq = UINT_MAX;
+
+    return 0;
+
+fail1:
+    *fenceman = NULL;
+
+    return ret;
+}
+
+void vigs_fenceman_destroy(struct vigs_fenceman *fenceman)
+{
+    unsigned long flags;
+    bool fence_list_empty;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    spin_lock_irqsave(&fenceman->lock, flags);
+    fence_list_empty = list_empty(&fenceman->fence_list);
+    spin_unlock_irqrestore(&fenceman->lock, flags);
+
+    BUG_ON(!fence_list_empty);
+
+    kfree(fenceman);
+}
+
+void vigs_fenceman_ack(struct vigs_fenceman *fenceman,
+                       uint32_t lower, uint32_t upper)
+{
+    unsigned long flags;
+    struct vigs_fence *fence, *tmp;
+
+    spin_lock_irqsave(&fenceman->lock, flags);
+
+    list_for_each_entry_safe(fence, tmp, &fenceman->fence_list, list) {
+        if (vigs_fence_seq_num_after_eq(fence->seq, lower) &&
+            vigs_fence_seq_num_before_eq(fence->seq, upper)) {
+            DRM_DEBUG_DRIVER("Fence signaled (seq = %u)\n",
+                             fence->seq);
+            list_del_init(&fence->list);
+            fence->signaled = true;
+            wake_up_all(&fence->wait);
+        }
+    }
+
+    spin_unlock_irqrestore(&fenceman->lock, flags);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_fenceman.h b/drivers/gpu/drm/vigs/vigs_fenceman.h
new file mode 100644
index 0000000..e6e1028
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_fenceman.h
@@ -0,0 +1,47 @@
+#ifndef _VIGS_FENCEMAN_H_
+#define _VIGS_FENCEMAN_H_
+
+#include "drmP.h"
+
+/*
+ * This is fence manager for VIGS. It's responsible for the following:
+ * + Fence bookkeeping.
+ * + Fence sequence number management and IRQ processing.
+ */
+
+struct vigs_fenceman
+{
+    /*
+     * Lock that's used to guard all data inside
+     * fence manager and fence objects. Don't confuse it
+     * with struct ttm_bo_device::fence_lock, that lock
+     * is used to work with TTM sync objects, i.e. it's more
+     * "high level".
+     */
+    spinlock_t lock;
+
+    /*
+     * List of currently pending fences.
+     */
+    struct list_head fence_list;
+
+    /*
+     * Current sequence number, new fence should be
+     * assigned (seq + 1).
+     * Note! Sequence numbers are always non-0, 0 is
+     * a special value that tells GPU not to fence things.
+     */
+    uint32_t seq;
+};
+
+int vigs_fenceman_create(struct vigs_fenceman **fenceman);
+
+void vigs_fenceman_destroy(struct vigs_fenceman *fenceman);
+
+/*
+ * Can be called from IRQ handler.
+ */
+void vigs_fenceman_ack(struct vigs_fenceman *fenceman,
+                       uint32_t lower, uint32_t upper);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_file.c b/drivers/gpu/drm/vigs/vigs_file.c
new file mode 100644
index 0000000..eef78de
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_file.c
@@ -0,0 +1,37 @@
+#include "vigs_file.h"
+#include "vigs_device.h"
+
+int vigs_file_create(struct vigs_device *vigs_dev,
+                     struct vigs_file **vigs_file)
+{
+    int ret = 0;
+
+    *vigs_file = kzalloc(sizeof(**vigs_file), GFP_KERNEL);
+
+    if (!*vigs_file) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    (*vigs_file)->obj_file = ttm_object_file_init(vigs_dev->obj_dev, 10);
+
+    if (!(*vigs_file)->obj_file) {
+        ret = -ENOMEM;
+        goto fail2;
+    }
+
+    return 0;
+
+fail2:
+    kfree(*vigs_file);
+fail1:
+    *vigs_file = NULL;
+
+    return ret;
+}
+
+void vigs_file_destroy(struct vigs_file *vigs_file)
+{
+    ttm_object_file_release(&vigs_file->obj_file);
+    kfree(vigs_file);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_file.h b/drivers/gpu/drm/vigs/vigs_file.h
new file mode 100644
index 0000000..45f16f8
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_file.h
@@ -0,0 +1,19 @@
+#ifndef _VIGS_FILE_H_
+#define _VIGS_FILE_H_
+
+#include "drmP.h"
+#include <ttm/ttm_object.h>
+
+struct vigs_device;
+
+struct vigs_file
+{
+    struct ttm_object_file *obj_file;
+};
+
+int vigs_file_create(struct vigs_device *vigs_dev,
+                     struct vigs_file **vigs_file);
+
+void vigs_file_destroy(struct vigs_file *vigs_file);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_framebuffer.c b/drivers/gpu/drm/vigs/vigs_framebuffer.c
new file mode 100644
index 0000000..65c78f5
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_framebuffer.c
@@ -0,0 +1,237 @@
+#include "vigs_framebuffer.h"
+#include "vigs_device.h"
+#include "vigs_surface.h"
+#include "vigs_fbdev.h"
+#include "vigs_comm.h"
+#include "drm_crtc_helper.h"
+#include <drm/vigs_drm.h>
+
+static void vigs_framebuffer_destroy(struct drm_framebuffer *fb)
+{
+    struct vigs_framebuffer *vigs_fb = fb_to_vigs_fb(fb);
+    int i;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    /*
+     * First, we need to call 'drm_framebuffer_cleanup', this'll
+     * automatically call 'vigs_crtc_disable' if needed, thus, notifying
+     * the host that root surface is gone.
+     */
+
+    drm_framebuffer_cleanup(fb);
+
+    /*
+     * And we can finally free the GEMs.
+     */
+
+    for (i = 0; i < 4; ++i) {
+        if (vigs_fb->surfaces[i]) {
+            drm_gem_object_unreference_unlocked(&vigs_fb->surfaces[i]->gem.base);
+        }
+    }
+    kfree(vigs_fb);
+}
+
+static int vigs_framebuffer_create_handle(struct drm_framebuffer *fb,
+                                          struct drm_file *file_priv,
+                                          unsigned int *handle)
+{
+    struct vigs_framebuffer *vigs_fb = fb_to_vigs_fb(fb);
+
+    DRM_DEBUG_KMS("enter\n");
+
+    return drm_gem_handle_create(file_priv, &vigs_fb->surfaces[0]->gem.base, handle);
+}
+
+static int vigs_framebuffer_dirty(struct drm_framebuffer *fb,
+                                  struct drm_file *file_priv,
+                                  unsigned flags, unsigned color,
+                                  struct drm_clip_rect *clips,
+                                  unsigned num_clips)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    return 0;
+}
+
+static struct drm_framebuffer_funcs vigs_framebuffer_funcs =
+{
+    .destroy = vigs_framebuffer_destroy,
+    .create_handle = vigs_framebuffer_create_handle,
+    .dirty = vigs_framebuffer_dirty,
+};
+
+static struct drm_framebuffer *vigs_fb_create(struct drm_device *drm_dev,
+                                              struct drm_file *file_priv,
+                                              struct drm_mode_fb_cmd2 *mode_cmd)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct vigs_surface *surfaces[4];
+    int ret, i;
+    int num_planes = drm_format_num_planes(mode_cmd->pixel_format);
+    struct vigs_framebuffer *vigs_fb;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    for (i = 0; i < num_planes; ++i) {
+        struct drm_gem_object *gem;
+        struct vigs_gem_object *vigs_gem;
+
+        gem = drm_gem_object_lookup(drm_dev, file_priv, mode_cmd->handles[i]);
+
+        if (!gem) {
+            DRM_ERROR("GEM lookup failed, handle = %u\n", mode_cmd->handles[i]);
+            ret = -ENOENT;
+            goto fail;
+        }
+
+        vigs_gem = gem_to_vigs_gem(gem);
+
+        if (vigs_gem->type != VIGS_GEM_TYPE_SURFACE) {
+            DRM_ERROR("GEM is not a surface, handle = %u\n", mode_cmd->handles[i]);
+            drm_gem_object_unreference_unlocked(gem);
+            ret = -ENOENT;
+            goto fail;
+        }
+
+        surfaces[i] = vigs_gem_to_vigs_surface(vigs_gem);
+    }
+
+    vigs_fb = kzalloc(sizeof(*vigs_fb), GFP_KERNEL);
+
+    if (!vigs_fb) {
+        ret = -ENOMEM;
+        goto fail;
+    }
+
+    vigs_fb->comm = vigs_dev->comm;
+
+    for (i = 0; i < num_planes; ++i) {
+        vigs_fb->surfaces[i] = surfaces[i];
+    }
+
+    ret = drm_framebuffer_init(vigs_dev->drm_dev,
+                               &vigs_fb->base,
+                               &vigs_framebuffer_funcs);
+
+    if (ret != 0) {
+        DRM_ERROR("unable to create the framebuffer: %d\n", ret);
+        kfree(vigs_fb);
+        goto fail;
+    }
+
+    drm_helper_mode_fill_fb_struct(&vigs_fb->base, mode_cmd);
+
+    return &vigs_fb->base;
+
+fail:
+    for (i--; i >= 0; i--) {
+        drm_gem_object_unreference_unlocked(&surfaces[i]->gem.base);
+    }
+
+    return ERR_PTR(ret);
+}
+
+static void vigs_output_poll_changed(struct drm_device *drm_dev)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    if (vigs_dev->fbdev) {
+        vigs_fbdev_output_poll_changed(vigs_dev->fbdev);
+    }
+}
+
+static struct drm_mode_config_funcs vigs_mode_config_funcs =
+{
+    .fb_create = vigs_fb_create,
+    .output_poll_changed = vigs_output_poll_changed
+};
+
+void vigs_framebuffer_config_init(struct vigs_device *vigs_dev)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    vigs_dev->drm_dev->mode_config.min_width = 0;
+    vigs_dev->drm_dev->mode_config.min_height = 0;
+
+    vigs_dev->drm_dev->mode_config.max_width = 4096;
+    vigs_dev->drm_dev->mode_config.max_height = 4096;
+
+    vigs_dev->drm_dev->mode_config.funcs = &vigs_mode_config_funcs;
+}
+
+int vigs_framebuffer_create(struct vigs_device *vigs_dev,
+                            struct drm_mode_fb_cmd2 *mode_cmd,
+                            struct vigs_surface *fb_sfc,
+                            struct vigs_framebuffer **vigs_fb)
+{
+    int ret = 0;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    *vigs_fb = kzalloc(sizeof(**vigs_fb), GFP_KERNEL);
+
+    if (!*vigs_fb) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    if ((fb_sfc->width != mode_cmd->width) ||
+        (fb_sfc->height != mode_cmd->height) ||
+        (fb_sfc->stride != mode_cmd->pitches[0])) {
+        DRM_DEBUG_KMS("surface format mismatch, sfc - (%u,%u,%u), mode - (%u,%u,%u)\n",
+                      fb_sfc->width, fb_sfc->height, fb_sfc->stride,
+                      mode_cmd->width, mode_cmd->height, mode_cmd->pitches[0]);
+        ret = -EINVAL;
+        goto fail2;
+    }
+
+    (*vigs_fb)->comm = vigs_dev->comm;
+    (*vigs_fb)->surfaces[0] = fb_sfc;
+
+    ret = drm_framebuffer_init(vigs_dev->drm_dev,
+                               &(*vigs_fb)->base,
+                               &vigs_framebuffer_funcs);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    drm_helper_mode_fill_fb_struct(&(*vigs_fb)->base, mode_cmd);
+
+    drm_gem_object_reference(&fb_sfc->gem.base);
+
+    return 0;
+
+fail2:
+    kfree(*vigs_fb);
+fail1:
+    *vigs_fb = NULL;
+
+    return ret;
+}
+
+int vigs_framebuffer_pin(struct vigs_framebuffer *vigs_fb)
+{
+    int ret;
+
+    vigs_gem_reserve(&vigs_fb->surfaces[0]->gem);
+
+    ret = vigs_gem_pin(&vigs_fb->surfaces[0]->gem);
+
+    vigs_gem_unreserve(&vigs_fb->surfaces[0]->gem);
+
+    return ret;
+}
+
+void vigs_framebuffer_unpin(struct vigs_framebuffer *vigs_fb)
+{
+    vigs_gem_reserve(&vigs_fb->surfaces[0]->gem);
+
+    vigs_gem_unpin(&vigs_fb->surfaces[0]->gem);
+
+    vigs_gem_unreserve(&vigs_fb->surfaces[0]->gem);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_framebuffer.h b/drivers/gpu/drm/vigs/vigs_framebuffer.h
new file mode 100644
index 0000000..f95728ef
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_framebuffer.h
@@ -0,0 +1,43 @@
+#ifndef _VIGS_FRAMEBUFFER_H_
+#define _VIGS_FRAMEBUFFER_H_
+
+#include "drmP.h"
+#include "vigs_protocol.h"
+
+struct vigs_device;
+struct vigs_comm;
+struct vigs_surface;
+
+struct vigs_framebuffer
+{
+    struct drm_framebuffer base;
+
+    /*
+     * Cached from 'vigs_device' for speed.
+     */
+    struct vigs_comm *comm;
+
+    struct vigs_surface *surfaces[4];
+};
+
+static inline struct vigs_framebuffer *fb_to_vigs_fb(struct drm_framebuffer *fb)
+{
+    return container_of(fb, struct vigs_framebuffer, base);
+}
+
+void vigs_framebuffer_config_init(struct vigs_device *vigs_dev);
+
+/*
+ * Creates a framebuffer object.
+ * Note that it also gets a reference to 'fb_gem' (in case of success), so
+ * don't forget to unreference it in the calling code.
+ */
+int vigs_framebuffer_create(struct vigs_device *vigs_dev,
+                            struct drm_mode_fb_cmd2 *mode_cmd,
+                            struct vigs_surface *fb_sfc,
+                            struct vigs_framebuffer **vigs_fb);
+
+int vigs_framebuffer_pin(struct vigs_framebuffer *vigs_fb);
+void vigs_framebuffer_unpin(struct vigs_framebuffer *vigs_fb);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_gem.c b/drivers/gpu/drm/vigs/vigs_gem.c
new file mode 100644
index 0000000..8273351
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_gem.c
@@ -0,0 +1,445 @@
+#include "vigs_gem.h"
+#include "vigs_device.h"
+#include "vigs_mman.h"
+#include "vigs_surface.h"
+#include "vigs_dp.h"
+#include <drm/vigs_drm.h>
+#include <ttm/ttm_placement.h>
+
+static void vigs_gem_bo_destroy(struct ttm_buffer_object *bo)
+{
+    struct vigs_gem_object *vigs_gem = bo_to_vigs_gem(bo);
+
+    if (vigs_gem->destroy) {
+        vigs_gem->destroy(vigs_gem);
+    }
+
+    drm_gem_object_release(&vigs_gem->base);
+    kfree(vigs_gem);
+}
+
+int vigs_gem_init(struct vigs_gem_object *vigs_gem,
+                  struct vigs_device *vigs_dev,
+                  enum ttm_object_type type,
+                  unsigned long size,
+                  bool kernel,
+                  vigs_gem_destroy_func destroy)
+{
+    u32 placements[1];
+    struct ttm_placement placement;
+    enum ttm_bo_type bo_type;
+    int ret = 0;
+
+    size = roundup(size, PAGE_SIZE);
+
+    if (size == 0) {
+        kfree(vigs_gem);
+        return -EINVAL;
+    }
+
+    if (type == VIGS_GEM_TYPE_SURFACE) {
+        placements[0] =
+            TTM_PL_FLAG_CACHED | TTM_PL_FLAG_TT | TTM_PL_FLAG_NO_EVICT;
+    } else if (type == VIGS_GEM_TYPE_EXECBUFFER) {
+        placements[0] =
+            TTM_PL_FLAG_CACHED | TTM_PL_FLAG_PRIV0 | TTM_PL_FLAG_NO_EVICT;
+    } else {
+        kfree(vigs_gem);
+        return -EINVAL;
+    }
+
+    memset(&placement, 0, sizeof(placement));
+
+    placement.placement = placements;
+    placement.busy_placement = placements;
+    placement.num_placement = 1;
+    placement.num_busy_placement = 1;
+
+    if (kernel) {
+        bo_type = ttm_bo_type_kernel;
+    } else {
+        bo_type = ttm_bo_type_device;
+    }
+
+    if (unlikely(vigs_dev->mman->bo_dev.dev_mapping == NULL)) {
+        vigs_dev->mman->bo_dev.dev_mapping = vigs_dev->drm_dev->dev_mapping;
+    }
+
+    ret = drm_gem_object_init(vigs_dev->drm_dev, &vigs_gem->base, size);
+
+    if (ret != 0) {
+        kfree(vigs_gem);
+        return ret;
+    }
+
+    ret = ttm_bo_init(&vigs_dev->mman->bo_dev, &vigs_gem->bo, size, bo_type,
+                      &placement, 0,
+                      false, NULL, 0, NULL,
+                      &vigs_gem_bo_destroy);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    vigs_gem->type = type;
+    vigs_gem->pin_count = 0;
+    vigs_gem->destroy = destroy;
+
+    DRM_DEBUG_DRIVER("GEM created (type = %u, off = 0x%llX, sz = %lu)\n",
+                     type,
+                     vigs_gem_mmap_offset(vigs_gem),
+                     vigs_gem_size(vigs_gem));
+
+    return 0;
+}
+
+void vigs_gem_cleanup(struct vigs_gem_object *vigs_gem)
+{
+    struct ttm_buffer_object *bo = &vigs_gem->bo;
+
+    ttm_bo_unref(&bo);
+}
+
+int vigs_gem_pin(struct vigs_gem_object *vigs_gem)
+{
+    u32 placements[1];
+    struct ttm_placement placement;
+    int ret;
+
+    if (vigs_gem->pin_count) {
+        ++vigs_gem->pin_count;
+        return 0;
+    }
+
+    if (vigs_gem->type == VIGS_GEM_TYPE_EXECBUFFER) {
+        vigs_gem->pin_count = 1;
+
+        return 0;
+    }
+
+    placements[0] =
+        TTM_PL_FLAG_CACHED | TTM_PL_FLAG_VRAM | TTM_PL_FLAG_NO_EVICT;
+
+    memset(&placement, 0, sizeof(placement));
+
+    placement.placement = placements;
+    placement.busy_placement = placements;
+    placement.num_placement = 1;
+    placement.num_busy_placement = 1;
+
+    ret = ttm_bo_validate(&vigs_gem->bo, &placement, false, false);
+
+    if (ret != 0) {
+        DRM_ERROR("GEM pin failed (type = %u, off = 0x%llX, sz = %lu)\n",
+                  vigs_gem->type,
+                  vigs_gem_mmap_offset(vigs_gem),
+                  vigs_gem_size(vigs_gem));
+        return ret;
+    }
+
+    vigs_gem->pin_count = 1;
+
+    DRM_DEBUG_DRIVER("GEM pinned (type = %u, off = 0x%llX, sz = %lu)\n",
+                     vigs_gem->type,
+                     vigs_gem_mmap_offset(vigs_gem),
+                     vigs_gem_size(vigs_gem));
+
+    return 0;
+}
+
+void vigs_gem_unpin(struct vigs_gem_object *vigs_gem)
+{
+    u32 placements[2];
+    struct ttm_placement placement;
+    int ret;
+
+    BUG_ON(vigs_gem->pin_count == 0);
+
+    if (--vigs_gem->pin_count > 0) {
+        return;
+    }
+
+    if (vigs_gem->type == VIGS_GEM_TYPE_EXECBUFFER) {
+        return;
+    }
+
+    vigs_gem_kunmap(vigs_gem);
+
+    placements[0] =
+        TTM_PL_FLAG_CACHED | TTM_PL_FLAG_VRAM;
+    placements[1] =
+        TTM_PL_FLAG_CACHED | TTM_PL_FLAG_TT | TTM_PL_FLAG_NO_EVICT;
+
+    memset(&placement, 0, sizeof(placement));
+
+    placement.placement = placements;
+    placement.busy_placement = placements;
+    placement.num_placement = 2;
+    placement.num_busy_placement = 2;
+
+    ret = ttm_bo_validate(&vigs_gem->bo, &placement, false, false);
+
+    if (ret != 0) {
+        DRM_ERROR("GEM unpin failed (type = %u, off = 0x%llX, sz = %lu)\n",
+                  vigs_gem->type,
+                  vigs_gem_mmap_offset(vigs_gem),
+                  vigs_gem_size(vigs_gem));
+    } else {
+        DRM_DEBUG_DRIVER("GEM unpinned (type = %u, off = 0x%llX, sz = %lu)\n",
+                         vigs_gem->type,
+                         vigs_gem_mmap_offset(vigs_gem),
+                         vigs_gem_size(vigs_gem));
+    }
+}
+
+int vigs_gem_kmap(struct vigs_gem_object *vigs_gem)
+{
+    bool is_iomem;
+    int ret;
+
+    BUG_ON((vigs_gem->type == VIGS_GEM_TYPE_SURFACE) &&
+           (vigs_gem->pin_count == 0));
+
+    if (vigs_gem->kptr) {
+        return 0;
+    }
+
+    ret = ttm_bo_kmap(&vigs_gem->bo,
+                      0,
+                      vigs_gem->bo.num_pages,
+                      &vigs_gem->kmap);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    vigs_gem->kptr = ttm_kmap_obj_virtual(&vigs_gem->kmap, &is_iomem);
+
+    DRM_DEBUG_DRIVER("GEM (type = %u, off = 0x%llX, sz = %lu) mapped to 0x%p\n",
+                     vigs_gem->type,
+                     vigs_gem_mmap_offset(vigs_gem),
+                     vigs_gem_size(vigs_gem),
+                     vigs_gem->kptr);
+
+    return 0;
+}
+
+void vigs_gem_kunmap(struct vigs_gem_object *vigs_gem)
+{
+    if (vigs_gem->kptr == NULL) {
+        return;
+    }
+
+    vigs_gem->kptr = NULL;
+
+    ttm_bo_kunmap(&vigs_gem->kmap);
+
+    DRM_DEBUG_DRIVER("GEM (type = %u, off = 0x%llX, sz = %lu) unmapped\n",
+                     vigs_gem->type,
+                     vigs_gem_mmap_offset(vigs_gem),
+                     vigs_gem_size(vigs_gem));
+}
+
+int vigs_gem_in_vram(struct vigs_gem_object *vigs_gem)
+{
+    return vigs_gem->bo.mem.mem_type == TTM_PL_VRAM;
+}
+
+int vigs_gem_wait(struct vigs_gem_object *vigs_gem)
+{
+    int ret;
+
+    spin_lock(&vigs_gem->bo.bdev->fence_lock);
+
+    ret = ttm_bo_wait(&vigs_gem->bo, true, false, false);
+
+    spin_unlock(&vigs_gem->bo.bdev->fence_lock);
+
+    return ret;
+}
+
+void vigs_gem_free_object(struct drm_gem_object *gem)
+{
+    struct vigs_gem_object *vigs_gem = gem_to_vigs_gem(gem);
+
+    if (vigs_gem->type == VIGS_GEM_TYPE_SURFACE) {
+        struct vigs_device *vigs_dev = gem->dev->dev_private;
+
+        vigs_dp_remove_surface(vigs_dev->dp,
+                               vigs_gem_to_vigs_surface(vigs_gem));
+    }
+
+    vigs_gem_reserve(vigs_gem);
+
+    vigs_gem_kunmap(vigs_gem);
+
+    vigs_gem_unreserve(vigs_gem);
+
+    vigs_gem->freed = true;
+
+    DRM_DEBUG_DRIVER("GEM free (type = %u, off = 0x%llX, sz = %lu)\n",
+                     vigs_gem->type,
+                     vigs_gem_mmap_offset(vigs_gem),
+                     vigs_gem_size(vigs_gem));
+
+    /* vigs_gem_cleanup(vigs_gem); */
+}
+
+int vigs_gem_init_object(struct drm_gem_object *gem)
+{
+    return 0;
+}
+
+int vigs_gem_open_object(struct drm_gem_object *gem,
+                         struct drm_file *file_priv)
+{
+    return 0;
+}
+
+void vigs_gem_close_object(struct drm_gem_object *gem,
+                           struct drm_file *file_priv)
+{
+}
+
+int vigs_gem_map_ioctl(struct drm_device *drm_dev,
+                       void *data,
+                       struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_gem_map *args = data;
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+    struct mm_struct *mm = current->mm;
+    unsigned long address, unused;
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, args->handle);
+
+    if (gem == NULL) {
+        return -ENOENT;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    down_write(&mm->mmap_sem);
+
+    /*
+     * We can't use 'do_mmap' here (like in i915, exynos and others) because
+     * 'do_mmap' takes an offset in bytes and our
+     * offset is 64-bit (since it's TTM offset) and it can't fit into 32-bit
+     * variable.
+     */
+    vigs_dev->track_gem_access = args->track_access;
+    address = do_mmap_pgoff(file_priv->filp, 0, vigs_gem_size(vigs_gem),
+                            PROT_READ | PROT_WRITE,
+                            MAP_SHARED,
+                            vigs_gem_mmap_offset(vigs_gem) >> PAGE_SHIFT,
+                            &unused);
+    vigs_dev->track_gem_access = false;
+
+    up_write(&mm->mmap_sem);
+
+    drm_gem_object_unreference_unlocked(gem);
+
+    if (IS_ERR((void*)address)) {
+        return PTR_ERR((void*)address);
+    }
+
+    args->address = address;
+
+    return 0;
+}
+
+int vigs_gem_wait_ioctl(struct drm_device *drm_dev,
+                        void *data,
+                        struct drm_file *file_priv)
+{
+    struct drm_vigs_gem_wait *args = data;
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+    int ret;
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, args->handle);
+
+    if (gem == NULL) {
+        return -ENOENT;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    vigs_gem_reserve(vigs_gem);
+
+    ret = vigs_gem_wait(vigs_gem);
+
+    vigs_gem_unreserve(vigs_gem);
+
+    drm_gem_object_unreference_unlocked(gem);
+
+    return ret;
+}
+
+int vigs_gem_dumb_create(struct drm_file *file_priv,
+                         struct drm_device *drm_dev,
+                         struct drm_mode_create_dumb *args)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct vigs_surface *sfc = NULL;
+    uint32_t handle;
+    int ret;
+
+    if (args->bpp != 32) {
+        DRM_ERROR("Only 32 bpp surfaces are supported for now\n");
+        return -EINVAL;
+    }
+
+    args->pitch = args->width * ((args->bpp + 7) / 8);
+
+    ret = vigs_surface_create(vigs_dev,
+                              args->width,
+                              args->height,
+                              args->pitch,
+                              vigsp_surface_bgrx8888,
+                              true,
+                              &sfc);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    args->size = vigs_gem_size(&sfc->gem);
+
+    ret = drm_gem_handle_create(file_priv,
+                                &sfc->gem.base,
+                                &handle);
+
+    drm_gem_object_unreference_unlocked(&sfc->gem.base);
+
+    if (ret == 0) {
+        args->handle = handle;
+    }
+
+    return 0;
+}
+
+int vigs_gem_dumb_map_offset(struct drm_file *file_priv,
+                             struct drm_device *drm_dev,
+                             uint32_t handle, uint64_t *offset_p)
+{
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+
+    BUG_ON(!offset_p);
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, handle);
+
+    if (gem == NULL) {
+        return -ENOENT;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    *offset_p = vigs_gem_mmap_offset(vigs_gem);
+
+    drm_gem_object_unreference_unlocked(gem);
+
+    return 0;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_gem.h b/drivers/gpu/drm/vigs/vigs_gem.h
new file mode 100644
index 0000000..dd8d8d1
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_gem.h
@@ -0,0 +1,220 @@
+#ifndef _VIGS_GEM_H_
+#define _VIGS_GEM_H_
+
+#include "drmP.h"
+#include <ttm/ttm_bo_driver.h>
+#include <ttm/ttm_object.h>
+
+#define VIGS_GEM_TYPE_SURFACE ttm_driver_type0
+#define VIGS_GEM_TYPE_EXECBUFFER ttm_driver_type1
+
+struct vigs_device;
+struct vigs_gem_object;
+
+typedef void (*vigs_gem_destroy_func)(struct vigs_gem_object *vigs_gem);
+
+struct vigs_gem_object
+{
+    struct drm_gem_object base;
+
+    struct ttm_buffer_object bo;
+
+    /*
+     * Indicates that drm_driver::gem_free_object was called.
+     */
+    bool freed;
+
+    enum ttm_object_type type;
+
+    /*
+     * Valid only after successful call to 'vigs_gem_kmap'.
+     * @{
+     */
+
+    struct ttm_bo_kmap_obj kmap;
+    void *kptr; /* Kernel pointer to buffer data. */
+
+    /*
+     * @}
+     */
+
+    volatile unsigned pin_count;
+
+    vigs_gem_destroy_func destroy;
+};
+
+static inline struct vigs_gem_object *gem_to_vigs_gem(struct drm_gem_object *gem)
+{
+    return container_of(gem, struct vigs_gem_object, base);
+}
+
+static inline struct vigs_gem_object *bo_to_vigs_gem(struct ttm_buffer_object *bo)
+{
+    return container_of(bo, struct vigs_gem_object, bo);
+}
+
+/*
+ * Must be called with drm_device::struct_mutex held.
+ * @{
+ */
+
+static inline bool vigs_gem_freed(struct vigs_gem_object *vigs_gem)
+{
+    return vigs_gem->freed;
+}
+
+/*
+ * @}
+ */
+
+/*
+ * Initializes a gem object. 'size' is automatically rounded up to page size.
+ * 'vigs_gem' is kfree'd on failure.
+ */
+int vigs_gem_init(struct vigs_gem_object *vigs_gem,
+                  struct vigs_device *vigs_dev,
+                  enum ttm_object_type type,
+                  unsigned long size,
+                  bool kernel,
+                  vigs_gem_destroy_func destroy);
+
+void vigs_gem_cleanup(struct vigs_gem_object *vigs_gem);
+
+/*
+ * Buffer size.
+ */
+static inline unsigned long vigs_gem_size(struct vigs_gem_object *vigs_gem)
+{
+    return vigs_gem->bo.num_pages << PAGE_SHIFT;
+}
+
+/*
+ * GEM offset in a placement. In case of execbuffer always the same.
+ * In case of surface only valid when GEM is in VRAM.
+ */
+static inline unsigned long vigs_gem_offset(struct vigs_gem_object *vigs_gem)
+{
+    return vigs_gem->bo.offset;
+}
+
+/*
+ * GEM offset relative to DRM_FILE_OFFSET. For kernel buffers it's always 0.
+ */
+static inline u64 vigs_gem_mmap_offset(struct vigs_gem_object *vigs_gem)
+{
+    return drm_vma_node_offset_addr(&vigs_gem->bo.vma_node);
+}
+
+static inline void vigs_gem_reserve(struct vigs_gem_object *vigs_gem)
+{
+    int ret;
+
+    ret = ttm_bo_reserve(&vigs_gem->bo, false, false, false, 0);
+
+    BUG_ON(ret != 0);
+}
+
+static inline void vigs_gem_unreserve(struct vigs_gem_object *vigs_gem)
+{
+    ttm_bo_unreserve(&vigs_gem->bo);
+}
+
+/*
+ * Functions below MUST be called between
+ * vigs_gem_reserve/vigs_gem_unreserve.
+ * @{
+ */
+
+/*
+ * Pin/unpin GEM. For execbuffers this is a no-op, since they're always
+ * in RAM placement. For surfaces this pins the GEM into VRAM. The
+ * operation can fail if there's no room in VRAM and all GEMs currently
+ * in VRAM are pinned.
+ * @{
+ */
+int vigs_gem_pin(struct vigs_gem_object *vigs_gem);
+void vigs_gem_unpin(struct vigs_gem_object *vigs_gem);
+/*
+ * @}
+ */
+
+/*
+ * Surface GEMs must be pinned before calling these.
+ * @{
+ */
+int vigs_gem_kmap(struct vigs_gem_object *vigs_gem);
+void vigs_gem_kunmap(struct vigs_gem_object *vigs_gem);
+/*
+ * @}
+ */
+
+/*
+ * true if GEM is currently in VRAM. Note that this doesn't
+ * necessarily mean that it's pinned.
+ */
+int vigs_gem_in_vram(struct vigs_gem_object *vigs_gem);
+
+int vigs_gem_wait(struct vigs_gem_object *vigs_gem);
+
+/*
+ * @}
+ */
+
+/*
+ * Driver hooks.
+ * @{
+ */
+
+void vigs_gem_free_object(struct drm_gem_object *gem);
+
+int vigs_gem_init_object(struct drm_gem_object *gem);
+
+int vigs_gem_open_object(struct drm_gem_object *gem,
+                         struct drm_file *file_priv);
+
+void vigs_gem_close_object(struct drm_gem_object *gem,
+                           struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_gem_map_ioctl(struct drm_device *drm_dev,
+                       void *data,
+                       struct drm_file *file_priv);
+
+int vigs_gem_wait_ioctl(struct drm_device *drm_dev,
+                        void *data,
+                        struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+/*
+ * Dumb
+ * @{
+ */
+
+int vigs_gem_dumb_create(struct drm_file *file_priv,
+                         struct drm_device *drm_dev,
+                         struct drm_mode_create_dumb *args);
+
+int vigs_gem_dumb_destroy(struct drm_file *file_priv,
+                          struct drm_device *drm_dev,
+                          uint32_t handle);
+
+int vigs_gem_dumb_map_offset(struct drm_file *file_priv,
+                             struct drm_device *drm_dev,
+                             uint32_t handle, uint64_t *offset_p);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_irq.c b/drivers/gpu/drm/vigs/vigs_irq.c
new file mode 100644
index 0000000..42f1074
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_irq.c
@@ -0,0 +1,127 @@
+#include "vigs_irq.h"
+#include "vigs_device.h"
+#include "vigs_regs.h"
+#include "vigs_fenceman.h"
+
+static void vigs_finish_pageflips(struct vigs_device *vigs_dev)
+{
+    struct drm_pending_vblank_event *event, *tmp;
+    struct timeval now;
+    unsigned long flags;
+    bool is_checked = false;
+
+    spin_lock_irqsave(&vigs_dev->drm_dev->event_lock, flags);
+
+    list_for_each_entry_safe(event, tmp,
+                             &vigs_dev->pageflip_event_list,
+                             base.link) {
+        if (event->pipe != 0) {
+            continue;
+        }
+
+        is_checked = true;
+
+        do_gettimeofday(&now);
+        event->event.sequence = 0;
+        event->event.tv_sec = now.tv_sec;
+        event->event.tv_usec = now.tv_usec;
+
+        list_move_tail(&event->base.link, &event->base.file_priv->event_list);
+        wake_up_interruptible(&event->base.file_priv->event_wait);
+    }
+
+    if (is_checked) {
+        /*
+         * Call 'drm_vblank_put' only in case that 'drm_vblank_get' was
+         * called.
+         */
+        if (atomic_read(&vigs_dev->drm_dev->vblank->refcount) > 0) {
+            drm_vblank_put(vigs_dev->drm_dev, 0);
+        }
+    }
+
+    spin_unlock_irqrestore(&vigs_dev->drm_dev->event_lock, flags);
+}
+
+int vigs_enable_vblank(struct drm_device *drm_dev, int crtc)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    u32 value;
+
+    DRM_DEBUG_KMS("enter: crtc = %d\n", crtc);
+
+    if (crtc != 0) {
+        DRM_ERROR("bad crtc = %d", crtc);
+        return -EINVAL;
+    }
+
+    value = VIGS_REG_CON_VBLANK_ENABLE;
+
+    writel(value, vigs_dev->io_map->handle + VIGS_REG_CON);
+
+    return 0;
+}
+
+void vigs_disable_vblank(struct drm_device *drm_dev, int crtc)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    u32 value;
+
+    DRM_DEBUG_KMS("enter: crtc = %d\n", crtc);
+
+    if (crtc != 0) {
+        DRM_ERROR("bad crtc = %d", crtc);
+    }
+
+    value = 0;
+
+    writel(value, vigs_dev->io_map->handle + VIGS_REG_CON);
+}
+
+irqreturn_t vigs_irq_handler(int irq, void *arg)
+{
+    struct drm_device *drm_dev = (struct drm_device*)arg;
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    u32 int_value;
+    irqreturn_t ret = IRQ_NONE;
+
+    int_value = readl(vigs_dev->io_map->handle + VIGS_REG_INT);
+
+    if ((int_value & (VIGS_REG_INT_VBLANK_PENDING | VIGS_REG_INT_FENCE_ACK_PENDING)) != 0) {
+        /*
+         * Clear the interrupt first in order
+         * not to stall the hardware.
+         */
+
+        writel(int_value, vigs_dev->io_map->handle + VIGS_REG_INT);
+
+        ret = IRQ_HANDLED;
+    }
+
+    if ((int_value & VIGS_REG_INT_FENCE_ACK_PENDING) != 0) {
+        u32 lower, upper;
+
+        while (1) {
+            spin_lock(&vigs_dev->irq_lock);
+
+            lower = readl(vigs_dev->io_map->handle + VIGS_REG_FENCE_LOWER);
+            upper = readl(vigs_dev->io_map->handle + VIGS_REG_FENCE_UPPER);
+
+            spin_unlock(&vigs_dev->irq_lock);
+
+            if (lower) {
+                vigs_fenceman_ack(vigs_dev->fenceman, lower, upper);
+            } else {
+                break;
+            }
+        }
+    }
+
+    if ((int_value & VIGS_REG_INT_VBLANK_PENDING) != 0) {
+        drm_handle_vblank(drm_dev, 0);
+
+        vigs_finish_pageflips(vigs_dev);
+    }
+
+    return ret;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_irq.h b/drivers/gpu/drm/vigs/vigs_irq.h
new file mode 100644
index 0000000..d650087
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_irq.h
@@ -0,0 +1,12 @@
+#ifndef _VIGS_IRQ_H_
+#define _VIGS_IRQ_H_
+
+#include "drmP.h"
+
+int vigs_enable_vblank(struct drm_device *drm_dev, int crtc);
+
+void vigs_disable_vblank(struct drm_device *drm_dev, int crtc);
+
+irqreturn_t vigs_irq_handler(int irq, void *arg);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_mman.c b/drivers/gpu/drm/vigs/vigs_mman.c
new file mode 100644
index 0000000..5e6000e
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_mman.c
@@ -0,0 +1,724 @@
+#include "vigs_mman.h"
+#include "vigs_fence.h"
+#include <ttm/ttm_placement.h>
+
+/*
+ * This is TTM-based memory manager for VIGS, it supports 4 memory placements:
+ * CPU - This is for target-only memory, not shared with host, forced by TTM,
+ * not used.
+ * GPU - This is host-only memory, not shared with target.
+ * VRAM - This gets allocated on "VRAM" PCI BAR, shared with host, used
+ * for surface placement.
+ * RAM - This gets allocated on "RAM" PCI BAR, shared with host, used for
+ * execbuffer placement.
+ *
+ * Eviction is supported, so buffers can be moved between some placements.
+ * Allowed movements:
+ * VRAM -> GPU
+ * GPU -> VRAM
+ */
+
+/*
+ * Offsets for mmap will start at DRM_FILE_OFFSET
+ */
+#define DRM_FILE_OFFSET 0x100000000ULL
+#define DRM_FILE_PAGE_OFFSET (DRM_FILE_OFFSET >> PAGE_SHIFT)
+
+/*
+ * DRM_GLOBAL_TTM_MEM init/release thunks
+ * @{
+ */
+
+static int vigs_ttm_mem_global_init(struct drm_global_reference *ref)
+{
+    return ttm_mem_global_init(ref->object);
+}
+
+static void vigs_ttm_mem_global_release(struct drm_global_reference *ref)
+{
+    ttm_mem_global_release(ref->object);
+}
+
+/*
+ * @}
+ */
+
+/*
+ * Here we initialize mman::bo_global_ref and mman::mem_global_ref.
+ * This is required in order to bring up TTM bo subsystem and TTM memory
+ * subsystem if they aren't already up. The first one who
+ * calls 'drm_global_item_ref' automatically initializes the specified
+ * subsystem and the last one who calls 'drm_global_item_unref' automatically
+ * brings down the specified subsystem.
+ * @{
+ */
+
+static int vigs_mman_global_init(struct vigs_mman *mman)
+{
+    struct drm_global_reference *global_ref = NULL;
+    int ret = 0;
+
+    global_ref = &mman->mem_global_ref;
+    global_ref->global_type = DRM_GLOBAL_TTM_MEM;
+    global_ref->size = sizeof(struct ttm_mem_global);
+    global_ref->init = &vigs_ttm_mem_global_init;
+    global_ref->release = &vigs_ttm_mem_global_release;
+
+    ret = drm_global_item_ref(global_ref);
+
+    if (ret != 0) {
+        DRM_ERROR("failed setting up TTM memory subsystem: %d\n", ret);
+        return ret;
+    }
+
+    mman->bo_global_ref.mem_glob = mman->mem_global_ref.object;
+    global_ref = &mman->bo_global_ref.ref;
+    global_ref->global_type = DRM_GLOBAL_TTM_BO;
+    global_ref->size = sizeof(struct ttm_bo_global);
+    global_ref->init = &ttm_bo_global_init;
+    global_ref->release = &ttm_bo_global_release;
+
+    ret = drm_global_item_ref(global_ref);
+
+    if (ret != 0) {
+        DRM_ERROR("failed setting up TTM bo subsystem: %d\n", ret);
+        drm_global_item_unref(&mman->mem_global_ref);
+        return ret;
+    }
+
+    return 0;
+}
+
+static void vigs_mman_global_cleanup(struct vigs_mman *mman)
+{
+    drm_global_item_unref(&mman->bo_global_ref.ref);
+    drm_global_item_unref(&mman->mem_global_ref);
+}
+
+/*
+ * @}
+ */
+
+/*
+ * TTM backend functions.
+ * @{
+ */
+
+static int vigs_ttm_backend_bind(struct ttm_tt *tt,
+                                 struct ttm_mem_reg *bo_mem)
+{
+    return 0;
+}
+
+static int vigs_ttm_backend_unbind(struct ttm_tt *tt)
+{
+    return 0;
+}
+
+static void vigs_ttm_backend_destroy(struct ttm_tt *tt)
+{
+    struct ttm_dma_tt *dma_tt = (void*)tt;
+
+    ttm_dma_tt_fini(dma_tt);
+    kfree(dma_tt);
+}
+
+static struct ttm_backend_func vigs_ttm_backend_func = {
+    .bind = &vigs_ttm_backend_bind,
+    .unbind = &vigs_ttm_backend_unbind,
+    .destroy = &vigs_ttm_backend_destroy,
+};
+
+static struct ttm_tt *vigs_ttm_tt_create(struct ttm_bo_device *bo_dev,
+                                         unsigned long size,
+                                         uint32_t page_flags,
+                                         struct page *dummy_read_page)
+{
+    struct ttm_dma_tt *dma_tt;
+    int ret;
+
+    dma_tt = kzalloc(sizeof(struct ttm_dma_tt), GFP_KERNEL);
+
+    if (dma_tt == NULL) {
+        DRM_ERROR("cannot allocate ttm_dma_tt: OOM\n");
+        return NULL;
+    }
+
+    dma_tt->ttm.func = &vigs_ttm_backend_func;
+
+    ret = ttm_dma_tt_init(dma_tt, bo_dev, size, page_flags,
+                          dummy_read_page);
+
+    if (ret != 0) {
+        DRM_ERROR("ttm_dma_tt_init failed: %d\n", ret);
+        kfree(dma_tt);
+        return NULL;
+    }
+
+    return &dma_tt->ttm;
+}
+
+static int vigs_ttm_tt_populate(struct ttm_tt *tt)
+{
+    return 0;
+}
+
+static void vigs_ttm_tt_unpopulate(struct ttm_tt *tt)
+{
+}
+
+/*
+ * @}
+ */
+
+static int vigs_ttm_invalidate_caches(struct ttm_bo_device *bo_dev,
+                                      uint32_t flags)
+{
+    return 0;
+}
+
+static int vigs_ttm_init_mem_type(struct ttm_bo_device *bo_dev,
+                                  uint32_t type,
+                                  struct ttm_mem_type_manager *man)
+{
+    switch (type) {
+    case TTM_PL_SYSTEM:
+        man->flags = TTM_MEMTYPE_FLAG_MAPPABLE;
+        man->available_caching = TTM_PL_MASK_CACHING;
+        man->default_caching = TTM_PL_FLAG_CACHED;
+        break;
+    case TTM_PL_TT:
+        man->func = &ttm_bo_manager_func;
+        man->gpu_offset = 0;
+        man->flags = TTM_MEMTYPE_FLAG_MAPPABLE |
+                     TTM_MEMTYPE_FLAG_CMA;
+        man->available_caching = TTM_PL_MASK_CACHING;
+        man->default_caching = TTM_PL_FLAG_CACHED;
+        break;
+    case TTM_PL_VRAM:
+    case TTM_PL_PRIV0:
+        man->func = &ttm_bo_manager_func;
+        man->gpu_offset = 0;
+        man->flags = TTM_MEMTYPE_FLAG_FIXED |
+                     TTM_MEMTYPE_FLAG_MAPPABLE;
+        man->available_caching = TTM_PL_MASK_CACHING;
+        man->default_caching = TTM_PL_FLAG_CACHED;
+        break;
+    default:
+        DRM_ERROR("unsupported memory type: %u\n", (unsigned)type);
+        return -EINVAL;
+    }
+    return 0;
+}
+
+static const u32 evict_placements[1] =
+{
+    TTM_PL_FLAG_CACHED | TTM_PL_FLAG_TT | TTM_PL_FLAG_NO_EVICT
+};
+
+static const struct ttm_placement evict_placement =
+{
+    .fpfn = 0,
+    .lpfn = 0,
+    .num_placement = ARRAY_SIZE(evict_placements),
+    .placement = evict_placements,
+    .num_busy_placement = ARRAY_SIZE(evict_placements),
+    .busy_placement = evict_placements
+};
+
+static void vigs_ttm_evict_flags(struct ttm_buffer_object *bo,
+                                 struct ttm_placement *placement)
+{
+    BUG_ON(bo->mem.mem_type != TTM_PL_VRAM);
+
+    *placement = evict_placement;
+}
+
+static int vigs_ttm_move(struct ttm_buffer_object *bo,
+                         bool evict,
+                         bool interruptible,
+                         bool no_wait_gpu,
+                         struct ttm_mem_reg *new_mem)
+{
+    struct vigs_mman *mman = bo_dev_to_vigs_mman(bo->bdev);
+    struct ttm_mem_reg *old_mem = &bo->mem;
+
+    if ((old_mem->mem_type == TTM_PL_VRAM) &&
+        (new_mem->mem_type == TTM_PL_TT)) {
+        mman->ops->vram_to_gpu(mman->user_data, bo);
+
+        ttm_bo_mem_put(bo, old_mem);
+
+        *old_mem = *new_mem;
+        new_mem->mm_node = NULL;
+
+        return 0;
+    } else if ((old_mem->mem_type == TTM_PL_TT) &&
+               (new_mem->mem_type == TTM_PL_VRAM)) {
+        mman->ops->gpu_to_vram(mman->user_data, bo,
+                               (new_mem->start << PAGE_SHIFT) +
+                               bo->bdev->man[new_mem->mem_type].gpu_offset);
+
+        ttm_bo_mem_put(bo, old_mem);
+
+        *old_mem = *new_mem;
+        new_mem->mm_node = NULL;
+
+        return 0;
+    } else {
+        return ttm_bo_move_memcpy(bo, evict, no_wait_gpu, new_mem);
+    }
+}
+
+static int vigs_ttm_verify_access(struct ttm_buffer_object *bo,
+                                  struct file *filp)
+{
+    return 0;
+}
+
+static bool vigs_ttm_sync_obj_signaled(void *sync_obj)
+{
+    return vigs_fence_signaled((struct vigs_fence*)sync_obj);
+}
+
+static int vigs_ttm_sync_obj_wait(void *sync_obj,
+                                  bool lazy,
+                                  bool interruptible)
+{
+    return vigs_fence_wait((struct vigs_fence*)sync_obj, interruptible);
+}
+
+static int vigs_ttm_sync_obj_flush(void *sync_obj)
+{
+    return 0;
+}
+
+static void vigs_ttm_sync_obj_unref(void **sync_obj)
+{
+    struct vigs_fence* fence = *sync_obj;
+    vigs_fence_unref(fence);
+    *sync_obj = NULL;
+}
+
+static void *vigs_ttm_sync_obj_ref(void *sync_obj)
+{
+    vigs_fence_ref((struct vigs_fence*)sync_obj);
+    return sync_obj;
+}
+
+static int vigs_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)
+{
+    u32 placements[1];
+    struct ttm_placement placement;
+    int ret;
+
+    if (bo->mem.mem_type != TTM_PL_TT) {
+        /*
+         * We're only interested in GPU memory page faults.
+         */
+
+        return 0;
+    }
+
+    /*
+     * It's GPU memory page fault. Move this buffer into VRAM.
+     */
+
+    placements[0] = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_VRAM;
+
+    memset(&placement, 0, sizeof(placement));
+
+    placement.placement = placements;
+    placement.busy_placement = placements;
+    placement.num_placement = 1;
+    placement.num_busy_placement = 1;
+
+    ret = ttm_bo_validate(bo, &placement, false, false);
+
+    if (ret != 0) {
+        DRM_ERROR("movement failed for 0x%llX\n",
+                  drm_vma_node_offset_addr(&bo->vma_node));
+        return ret;
+    }
+
+    return 0;
+}
+
+static int vigs_ttm_io_mem_reserve(struct ttm_bo_device *bo_dev,
+                                   struct ttm_mem_reg *mem)
+{
+    struct ttm_mem_type_manager *man = &bo_dev->man[mem->mem_type];
+    struct vigs_mman *mman = bo_dev_to_vigs_mman(bo_dev);
+
+    mem->bus.addr = NULL;
+    mem->bus.offset = 0;
+    mem->bus.size = mem->num_pages << PAGE_SHIFT;
+    mem->bus.base = 0;
+    mem->bus.is_iomem = false;
+
+    if (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE)) {
+        return -EINVAL;
+    }
+
+    switch (mem->mem_type) {
+    case TTM_PL_SYSTEM:
+    case TTM_PL_TT:
+        break;
+    case TTM_PL_VRAM:
+        mem->bus.is_iomem = true;
+        mem->bus.base = mman->vram_base;
+        mem->bus.offset = mem->start << PAGE_SHIFT;
+        break;
+    case TTM_PL_PRIV0:
+        mem->bus.is_iomem = true;
+        mem->bus.base = mman->ram_base;
+        mem->bus.offset = mem->start << PAGE_SHIFT;
+        break;
+    default:
+        return -EINVAL;
+    }
+
+    return 0;
+}
+
+static void vigs_ttm_io_mem_free(struct ttm_bo_device *bo_dev,
+                                 struct ttm_mem_reg *mem)
+{
+}
+
+static struct ttm_bo_driver vigs_ttm_bo_driver =
+{
+    .ttm_tt_create = &vigs_ttm_tt_create, /* Needed for ttm_bo_type_kernel and TTM_PL_TT */
+    .ttm_tt_populate = &vigs_ttm_tt_populate, /* Needed for TTM_PL_TT */
+    .ttm_tt_unpopulate = &vigs_ttm_tt_unpopulate, /* Needed for TTM_PL_TT */
+    .invalidate_caches = &vigs_ttm_invalidate_caches,
+    .init_mem_type = &vigs_ttm_init_mem_type,
+    .evict_flags = &vigs_ttm_evict_flags,
+    .move = &vigs_ttm_move,
+    .verify_access = &vigs_ttm_verify_access,
+    .sync_obj_signaled = vigs_ttm_sync_obj_signaled,
+    .sync_obj_wait = vigs_ttm_sync_obj_wait,
+    .sync_obj_flush = vigs_ttm_sync_obj_flush,
+    .sync_obj_unref = vigs_ttm_sync_obj_unref,
+    .sync_obj_ref = vigs_ttm_sync_obj_ref,
+    .fault_reserve_notify = &vigs_ttm_fault_reserve_notify,
+    .io_mem_reserve = &vigs_ttm_io_mem_reserve,
+    .io_mem_free = &vigs_ttm_io_mem_free,
+};
+
+/*
+ * VMA related.
+ * @{
+ */
+
+static u32 vigs_vma_cache_index = 0;
+static struct vm_operations_struct vigs_ttm_vm_ops;
+static const struct vm_operations_struct *ttm_vm_ops = NULL;
+
+/*
+ * Represents per-VMA data.
+ *
+ * Since TTM already uses struct vm_area_struct::vm_private_data
+ * we're forced to use some other way to add our own data
+ * to VMA. Currently we use struct vm_area_struct::vm_ops for this.
+ * Generally, TTM should be refactored to not use
+ * struct vm_area_struct directly, but provide helper functions
+ * instead so that user could store whatever he wants into
+ * struct vm_area_struct::vm_private_data.
+ */
+struct vigs_mman_vma
+{
+    struct vm_operations_struct vm_ops;
+    struct ttm_buffer_object *bo;
+    struct kref kref;
+    u8 data[1];
+};
+
+static void vigs_mman_vma_release(struct kref *kref)
+{
+    struct vigs_mman_vma *vigs_vma =
+        container_of(kref, struct vigs_mman_vma, kref);
+    struct ttm_buffer_object *bo = vigs_vma->bo;
+    struct vigs_mman *mman = bo_dev_to_vigs_mman(bo->bdev);
+
+    mman->ops->cleanup_vma(mman->user_data, &vigs_vma->data[0]);
+
+    kmem_cache_free(mman->vma_cache, vigs_vma);
+}
+
+/*
+ * @}
+ */
+
+int vigs_mman_create(resource_size_t vram_base,
+                     resource_size_t vram_size,
+                     resource_size_t ram_base,
+                     resource_size_t ram_size,
+                     uint32_t vma_data_size,
+                     struct vigs_mman_ops *ops,
+                     void *user_data,
+                     struct vigs_mman **mman)
+{
+    int ret = 0;
+    char vma_cache_name[100];
+    unsigned long num_pages = 0;
+
+    DRM_DEBUG_DRIVER("enter\n");
+
+    BUG_ON(vma_data_size <= 0);
+
+    *mman = kzalloc(sizeof(**mman), GFP_KERNEL);
+
+    if (!*mman) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    sprintf(vma_cache_name, "vigs_vma_cache%u", vigs_vma_cache_index++);
+
+    (*mman)->vma_cache = kmem_cache_create(vma_cache_name,
+                                           sizeof(struct vigs_mman_vma) +
+                                           vma_data_size - 1,
+                                           0, 0, NULL);
+
+    if (!(*mman)->vma_cache) {
+        ret = -ENOMEM;
+        goto fail2;
+    }
+
+    ret = vigs_mman_global_init(*mman);
+
+    if (ret != 0) {
+        goto fail3;
+    }
+
+    (*mman)->vram_base = vram_base;
+    (*mman)->ram_base = ram_base;
+    (*mman)->ops = ops;
+    (*mman)->user_data = user_data;
+
+    ret = ttm_bo_device_init(&(*mman)->bo_dev,
+                             (*mman)->bo_global_ref.ref.object,
+                             &vigs_ttm_bo_driver,
+                             DRM_FILE_PAGE_OFFSET,
+                             0);
+    if (ret != 0) {
+        DRM_ERROR("failed initializing bo driver: %d\n", ret);
+        goto fail4;
+    }
+
+    /*
+     * Init GPU
+     * @{
+     */
+
+    /*
+     * For GPU we're only limited by host resources, let the target create
+     * as many buffers as it likes.
+     */
+    ret = ttm_bo_init_mm(&(*mman)->bo_dev,
+                         TTM_PL_TT,
+                         (0xFFFFFFFFUL / PAGE_SIZE));
+    if (ret != 0) {
+        DRM_ERROR("failed initializing GPU mm\n");
+        goto fail5;
+    }
+
+    /*
+     * @}
+     */
+
+    /*
+     * Init VRAM
+     * @{
+     */
+
+    num_pages = vram_size / PAGE_SIZE;
+
+    ret = ttm_bo_init_mm(&(*mman)->bo_dev,
+                         TTM_PL_VRAM,
+                         num_pages);
+    if (ret != 0) {
+        DRM_ERROR("failed initializing VRAM mm\n");
+        goto fail6;
+    }
+
+    /*
+     * @}
+     */
+
+    /*
+     * Init RAM
+     * @{
+     */
+
+    num_pages = ram_size / PAGE_SIZE;
+
+    ret = ttm_bo_init_mm(&(*mman)->bo_dev,
+                         TTM_PL_PRIV0,
+                         num_pages);
+    if (ret != 0) {
+        DRM_ERROR("failed initializing RAM mm\n");
+        goto fail7;
+    }
+
+    /*
+     * @}
+     */
+
+    return 0;
+
+fail7:
+    ttm_bo_clean_mm(&(*mman)->bo_dev, TTM_PL_VRAM);
+fail6:
+    ttm_bo_clean_mm(&(*mman)->bo_dev, TTM_PL_TT);
+fail5:
+    ttm_bo_device_release(&(*mman)->bo_dev);
+fail4:
+    vigs_mman_global_cleanup(*mman);
+fail3:
+    kmem_cache_destroy((*mman)->vma_cache);
+fail2:
+    kfree(*mman);
+fail1:
+    *mman = NULL;
+
+    return ret;
+}
+
+void vigs_mman_destroy(struct vigs_mman *mman)
+{
+    DRM_DEBUG_DRIVER("enter\n");
+
+    ttm_bo_clean_mm(&mman->bo_dev, TTM_PL_PRIV0);
+    ttm_bo_clean_mm(&mman->bo_dev, TTM_PL_VRAM);
+    ttm_bo_clean_mm(&mman->bo_dev, TTM_PL_TT);
+    ttm_bo_device_release(&mman->bo_dev);
+    vigs_mman_global_cleanup(mman);
+    kmem_cache_destroy(mman->vma_cache);
+
+    kfree(mman);
+}
+
+static void vigs_ttm_open(struct vm_area_struct *vma)
+{
+    struct vigs_mman_vma *vigs_vma = (struct vigs_mman_vma*)vma->vm_ops;
+
+    BUG_ON(vigs_vma->bo != (struct ttm_buffer_object*)vma->vm_private_data);
+
+    ttm_vm_ops->open(vma);
+    kref_get(&vigs_vma->kref);
+}
+
+static void vigs_ttm_close(struct vm_area_struct *vma)
+{
+    struct vigs_mman_vma *vigs_vma = (struct vigs_mman_vma*)vma->vm_ops;
+
+    BUG_ON(vigs_vma->bo != (struct ttm_buffer_object*)vma->vm_private_data);
+
+    vma->vm_ops = &vigs_ttm_vm_ops;
+
+    kref_put(&vigs_vma->kref, &vigs_mman_vma_release);
+    ttm_vm_ops->close(vma);
+}
+
+static int vigs_ttm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+    struct ttm_buffer_object *bo = vma->vm_private_data;
+
+    if (bo == NULL) {
+        return VM_FAULT_NOPAGE;
+    }
+
+    return ttm_vm_ops->fault(vma, vmf);
+}
+
+int vigs_mman_mmap(struct vigs_mman *mman,
+                   struct file *filp,
+                   struct vm_area_struct *vma,
+                   bool track_access)
+{
+    struct vigs_mman_vma *vigs_vma;
+    int ret;
+    struct ttm_buffer_object *bo;
+
+    if (unlikely(vma->vm_pgoff < DRM_FILE_PAGE_OFFSET)) {
+        return drm_mmap(filp, vma);
+    }
+
+    vigs_vma = kmem_cache_alloc(mman->vma_cache, GFP_KERNEL);
+
+    if (!vigs_vma) {
+        return -ENOMEM;
+    }
+
+    ret = ttm_bo_mmap(filp, vma, &mman->bo_dev);
+
+    if (unlikely(ret != 0)) {
+        kmem_cache_free(mman->vma_cache, vigs_vma);
+        return ret;
+    }
+
+    if (unlikely(ttm_vm_ops == NULL)) {
+        ttm_vm_ops = vma->vm_ops;
+        vigs_ttm_vm_ops = *ttm_vm_ops;
+        vigs_ttm_vm_ops.fault = &vigs_ttm_fault;
+    }
+
+    bo = vma->vm_private_data;
+
+    vigs_vma->vm_ops = vigs_ttm_vm_ops;
+    vigs_vma->bo = bo;
+    vigs_vma->vm_ops.open = &vigs_ttm_open;
+    vigs_vma->vm_ops.close = &vigs_ttm_close;
+    kref_init(&vigs_vma->kref);
+    mman->ops->init_vma(mman->user_data,
+                        &vigs_vma->data[0],
+                        bo,
+                        track_access);
+
+    vma->vm_ops = &vigs_vma->vm_ops;
+
+    return 0;
+}
+
+int vigs_mman_access_vma(struct vigs_mman *mman,
+                         unsigned long address,
+                         vigs_mman_access_vma_func func,
+                         void *user_data)
+{
+    struct mm_struct *mm = current->mm;
+    struct vm_area_struct *vma;
+    int ret;
+    struct ttm_buffer_object *bo;
+    struct vigs_mman_vma *vigs_vma;
+
+    down_read(&mm->mmap_sem);
+
+    vma = find_vma(mm, address);
+
+    if (!vma ||
+        !vma->vm_ops ||
+        (vma->vm_ops->fault != &vigs_ttm_fault)) {
+        ret = -ENOENT;
+        goto out;
+    }
+
+    bo = vma->vm_private_data;
+
+    BUG_ON(!bo);
+
+    if (bo->bdev != &mman->bo_dev) {
+        ret = -ENOENT;
+        goto out;
+    }
+
+    vigs_vma = (struct vigs_mman_vma*)vma->vm_ops;
+
+    ret = func(user_data, &vigs_vma->data[0]);
+
+out:
+    up_read(&mm->mmap_sem);
+
+    return ret;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_mman.h b/drivers/gpu/drm/vigs/vigs_mman.h
new file mode 100644
index 0000000..286e99a
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_mman.h
@@ -0,0 +1,92 @@
+#ifndef _VIGS_MMAN_H_
+#define _VIGS_MMAN_H_
+
+#include "drmP.h"
+#include <linux/slab.h>
+#include <ttm/ttm_bo_driver.h>
+
+struct vigs_mman_ops
+{
+    /*
+     * 'bo' is reserved while calling these.
+     * @{
+     */
+
+    void (*vram_to_gpu)(void *user_data, struct ttm_buffer_object *bo);
+    void (*gpu_to_vram)(void *user_data, struct ttm_buffer_object *bo,
+                        unsigned long new_offset);
+    /*
+     * @}
+     */
+
+    /*
+     * Per-VMA data init/cleanup. VMA may be opened/closed many times
+     * as the result of split/copy, but the init/cleanup handlers are called
+     * only once, i.e. vigs_mman is handling the reference counts.
+     *
+     * current's 'mmap_sem' is locked while calling this.
+     * @{
+     */
+
+    void (*init_vma)(void *user_data,
+                     void *vma_data,
+                     struct ttm_buffer_object *bo,
+                     bool track_access);
+
+    /*
+     * current's 'mmap_sem' is locked while calling this.
+     */
+    void (*cleanup_vma)(void *user_data, void *vma_data);
+
+    /*
+     * @}
+     */
+};
+
+typedef int (*vigs_mman_access_vma_func)(void *user_data, void *vma_data);
+
+struct vigs_mman
+{
+    struct kmem_cache *vma_cache;
+
+    struct drm_global_reference mem_global_ref;
+    struct ttm_bo_global_ref bo_global_ref;
+    struct ttm_bo_device bo_dev;
+
+    resource_size_t vram_base;
+    resource_size_t ram_base;
+
+    struct vigs_mman_ops *ops;
+    void *user_data;
+};
+
+static inline struct vigs_mman *bo_dev_to_vigs_mman(struct ttm_bo_device *bo_dev)
+{
+    return container_of(bo_dev, struct vigs_mman, bo_dev);
+}
+
+int vigs_mman_create(resource_size_t vram_base,
+                     resource_size_t vram_size,
+                     resource_size_t ram_base,
+                     resource_size_t ram_size,
+                     uint32_t vma_data_size,
+                     struct vigs_mman_ops *ops,
+                     void *user_data,
+                     struct vigs_mman **mman);
+
+void vigs_mman_destroy(struct vigs_mman *mman);
+
+int vigs_mman_mmap(struct vigs_mman *mman,
+                   struct file *filp,
+                   struct vm_area_struct *vma,
+                   bool track_access);
+
+/*
+ * current's 'mmap_sem' is locked while calling 'func'.
+ */
+int vigs_mman_access_vma(struct vigs_mman *mman,
+                         unsigned long address,
+                         vigs_mman_access_vma_func func,
+                         void *user_data);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_output.c b/drivers/gpu/drm/vigs/vigs_output.c
new file mode 100644
index 0000000..fffcb70
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_output.c
@@ -0,0 +1,290 @@
+#include "vigs_output.h"
+#include "vigs_device.h"
+#include "drm_crtc_helper.h"
+#include <linux/init.h>
+
+#define DPI_DEF_VALUE 3160
+#define DPI_MIN_VALUE 1000
+#define DPI_MAX_VALUE 4800
+
+#ifndef MODULE
+static int vigs_atoi(const char *str)
+{
+    int val = 0;
+
+    for (;; ++str) {
+        switch (*str) {
+        case '0' ... '9':
+            val = (10 * val) + (*str - '0');
+            break;
+        default:
+            return val;
+        }
+    }
+}
+#endif
+
+struct vigs_output
+{
+    /*
+     * 'connector' is the owner of the 'vigs_output', i.e.
+     * when 'connector' is destroyed whole structure is destroyed.
+     */
+    struct drm_connector connector;
+    struct drm_encoder encoder;
+};
+
+static inline struct vigs_output *connector_to_vigs_output(struct drm_connector *connector)
+{
+    return container_of(connector, struct vigs_output, connector);
+}
+
+static inline struct vigs_output *encoder_to_vigs_output(struct drm_encoder *encoder)
+{
+    return container_of(encoder, struct vigs_output, encoder);
+}
+
+static void vigs_connector_save(struct drm_connector *connector)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static void vigs_connector_restore(struct drm_connector *connector)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static enum drm_connector_status vigs_connector_detect(
+    struct drm_connector *connector,
+    bool force)
+{
+    DRM_DEBUG_KMS("enter: force = %d\n", force);
+
+    return connector_status_connected;
+}
+
+static int vigs_connector_set_property(struct drm_connector *connector,
+                                       struct drm_property *property,
+                                       uint64_t value)
+{
+    DRM_DEBUG_KMS("enter: %s = %llu\n", property->name, value);
+
+    return 0;
+}
+
+static void vigs_connector_destroy(struct drm_connector *connector)
+{
+    struct vigs_output *vigs_output = connector_to_vigs_output(connector);
+
+    DRM_DEBUG_KMS("enter\n");
+
+    drm_sysfs_connector_remove(connector);
+    drm_connector_cleanup(connector);
+
+    kfree(vigs_output);
+}
+
+static int vigs_connector_get_modes(struct drm_connector *connector)
+{
+    struct vigs_output *vigs_output = connector_to_vigs_output(connector);
+    struct drm_device *drm_dev = vigs_output->connector.dev;
+    char *option = NULL;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    if (fb_get_options(drm_get_connector_name(connector), &option) == 0) {
+        struct drm_cmdline_mode cmdline_mode;
+
+        if (drm_mode_parse_command_line_for_connector(option,
+                                                      connector,
+                                                      &cmdline_mode)) {
+            struct drm_display_mode *preferred_mode =
+                drm_mode_create_from_cmdline_mode(drm_dev,
+                                                  &cmdline_mode);
+            preferred_mode->type = DRM_MODE_TYPE_PREFERRED | DRM_MODE_TYPE_DRIVER;
+            drm_mode_probed_add(connector, preferred_mode);
+            return 1;
+        }
+    }
+
+    return 0;
+}
+
+static int vigs_connector_mode_valid(struct drm_connector *connector,
+                                     struct drm_display_mode *mode)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    return MODE_OK;
+}
+
+struct drm_encoder *vigs_connector_best_encoder(struct drm_connector *connector)
+{
+    struct vigs_output *vigs_output = connector_to_vigs_output(connector);
+
+    DRM_DEBUG_KMS("enter\n");
+
+    return &vigs_output->encoder;
+}
+
+static void vigs_encoder_destroy(struct drm_encoder *encoder)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    drm_encoder_cleanup(encoder);
+}
+
+static void vigs_encoder_dpms(struct drm_encoder *encoder, int mode)
+{
+    DRM_DEBUG_KMS("enter: mode = %d\n", mode);
+}
+
+static bool vigs_encoder_mode_fixup(struct drm_encoder *encoder,
+                                    const struct drm_display_mode *mode,
+                                    struct drm_display_mode *adjusted_mode)
+{
+    DRM_DEBUG_KMS("enter\n");
+
+    return true;
+}
+
+static void vigs_encoder_prepare(struct drm_encoder *encoder)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static void vigs_encoder_mode_set(struct drm_encoder *encoder,
+                                  struct drm_display_mode *mode,
+                                  struct drm_display_mode *adjusted_mode)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static void vigs_encoder_commit(struct drm_encoder *encoder)
+{
+    DRM_DEBUG_KMS("enter\n");
+}
+
+static const struct drm_connector_funcs vigs_connector_funcs =
+{
+    .dpms = drm_helper_connector_dpms,
+    .save = vigs_connector_save,
+    .restore = vigs_connector_restore,
+    .detect = vigs_connector_detect,
+    .fill_modes = drm_helper_probe_single_connector_modes,
+    .set_property = vigs_connector_set_property,
+    .destroy = vigs_connector_destroy,
+};
+
+static const struct drm_connector_helper_funcs vigs_connector_helper_funcs =
+{
+    .get_modes = vigs_connector_get_modes,
+    .mode_valid = vigs_connector_mode_valid,
+    .best_encoder = vigs_connector_best_encoder,
+};
+
+static const struct drm_encoder_funcs vigs_encoder_funcs =
+{
+    .destroy = vigs_encoder_destroy,
+};
+
+static const struct drm_encoder_helper_funcs vigs_encoder_helper_funcs =
+{
+    .dpms = vigs_encoder_dpms,
+    .mode_fixup = vigs_encoder_mode_fixup,
+    .prepare = vigs_encoder_prepare,
+    .mode_set = vigs_encoder_mode_set,
+    .commit = vigs_encoder_commit,
+};
+
+int vigs_output_init(struct vigs_device *vigs_dev)
+{
+    struct vigs_output *vigs_output;
+    int ret;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    vigs_output = kzalloc(sizeof(*vigs_output), GFP_KERNEL);
+
+    if (!vigs_output) {
+        return -ENOMEM;
+    }
+
+    ret = drm_connector_init(vigs_dev->drm_dev,
+                             &vigs_output->connector,
+                             &vigs_connector_funcs,
+                             DRM_MODE_CONNECTOR_LVDS);
+
+    if (ret != 0) {
+        kfree(vigs_output);
+        return ret;
+    }
+
+    ret = drm_encoder_init(vigs_dev->drm_dev,
+                           &vigs_output->encoder,
+                           &vigs_encoder_funcs,
+                           DRM_MODE_ENCODER_LVDS);
+
+    if (ret != 0) {
+        /*
+         * KMS subsystem will delete 'vigs_output'
+         */
+
+        return ret;
+    }
+
+    /*
+     * We only have a single CRTC.
+     */
+    vigs_output->encoder.possible_crtcs = (1 << 0);
+
+    ret = drm_mode_connector_attach_encoder(&vigs_output->connector,
+                                            &vigs_output->encoder);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    drm_encoder_helper_add(&vigs_output->encoder, &vigs_encoder_helper_funcs);
+
+    drm_connector_helper_add(&vigs_output->connector, &vigs_connector_helper_funcs);
+
+    ret = drm_sysfs_connector_add(&vigs_output->connector);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    return 0;
+}
+
+int vigs_output_get_dpi(void)
+{
+    int dpi = DPI_DEF_VALUE;
+#ifndef MODULE
+    char *str;
+    char dpi_info[16];
+
+    str = strstr(saved_command_line, "dpi=");
+
+    if (str != NULL) {
+        str += 4;
+        strncpy(dpi_info, str, 4);
+        dpi = vigs_atoi(dpi_info);
+        if ((dpi < DPI_MIN_VALUE) || (dpi > DPI_MAX_VALUE)) {
+            dpi = DPI_DEF_VALUE;
+        }
+    }
+#endif
+    return dpi;
+}
+
+int vigs_output_get_phys_width(int dpi, u32 width)
+{
+    return ((width * 2540 / dpi) + 5) / 10;
+}
+
+int vigs_output_get_phys_height(int dpi, u32 height)
+{
+    return ((height * 2540 / dpi) + 5) / 10;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_output.h b/drivers/gpu/drm/vigs/vigs_output.h
new file mode 100644
index 0000000..2af43f6
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_output.h
@@ -0,0 +1,16 @@
+#ifndef _VIGS_OUTPUT_H_
+#define _VIGS_OUTPUT_H_
+
+#include "drmP.h"
+
+struct vigs_device;
+
+int vigs_output_init(struct vigs_device *vigs_dev);
+
+int vigs_output_get_dpi(void);
+
+int vigs_output_get_phys_width(int dpi, u32 width);
+
+int vigs_output_get_phys_height(int dpi, u32 height);
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_plane.c b/drivers/gpu/drm/vigs/vigs_plane.c
new file mode 100644
index 0000000..74d1a8c
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_plane.c
@@ -0,0 +1,286 @@
+#include "vigs_plane.h"
+#include "vigs_device.h"
+#include "vigs_framebuffer.h"
+#include "vigs_surface.h"
+#include "vigs_comm.h"
+#include <drm/vigs_drm.h>
+
+static const uint32_t formats[] =
+{
+    DRM_FORMAT_XRGB8888,
+    DRM_FORMAT_ARGB8888,
+    DRM_FORMAT_NV21,
+    fourcc_code('N', 'V', '4', '2'),
+    DRM_FORMAT_NV61,
+    DRM_FORMAT_YUV420
+};
+
+static int vigs_plane_update(struct drm_plane *plane,
+                             struct drm_crtc *crtc,
+                             struct drm_framebuffer *fb,
+                             int crtc_x, int crtc_y,
+                             unsigned int crtc_w,
+                             unsigned int crtc_h,
+                             uint32_t src_x, uint32_t src_y,
+                             uint32_t src_w, uint32_t src_h)
+{
+    struct vigs_plane *vigs_plane = plane_to_vigs_plane(plane);
+    struct vigs_device *vigs_dev = plane->dev->dev_private;
+    struct vigs_framebuffer *vigs_fb = fb_to_vigs_fb(fb);
+    int ret, i;
+    uint32_t src_x_whole = src_x >> 16;
+    uint32_t src_y_whole = src_y >> 16;
+    uint32_t src_w_whole = src_w >> 16;
+    uint32_t src_h_whole = src_h >> 16;
+    vigsp_surface_id surface_ids[4] = { 0, 0, 0, 0 };
+    vigsp_plane_format format;
+
+    DRM_DEBUG_KMS("enter: crtc_x = %d, crtc_y = %d, crtc_w = %u, crtc_h = %u, src_x = %u, src_y = %u, src_w = %u, src_h = %u\n",
+                  crtc_x, crtc_y, crtc_w, crtc_h, src_x, src_y, src_w, src_h);
+
+    if (vigs_fb->surfaces[0]->scanout) {
+        vigs_gem_reserve(&vigs_fb->surfaces[0]->gem);
+
+        if (vigs_gem_in_vram(&vigs_fb->surfaces[0]->gem) &&
+            vigs_surface_need_gpu_update(vigs_fb->surfaces[0])) {
+            vigs_comm_update_gpu(vigs_dev->comm,
+                                 vigs_fb->surfaces[0]->id,
+                                 vigs_fb->surfaces[0]->width,
+                                 vigs_fb->surfaces[0]->height,
+                                 vigs_gem_offset(&vigs_fb->surfaces[0]->gem));
+        }
+
+        vigs_gem_unreserve(&vigs_fb->surfaces[0]->gem);
+    }
+
+    for (i = 0; i < 4; ++i) {
+        if (vigs_fb->surfaces[i]) {
+            surface_ids[i] = vigs_fb->surfaces[i]->id;
+        }
+    }
+
+    switch (fb->pixel_format) {
+    case DRM_FORMAT_XRGB8888:
+        format = vigsp_plane_bgrx8888;
+        break;
+    case DRM_FORMAT_ARGB8888:
+        format = vigsp_plane_bgra8888;
+        break;
+    case DRM_FORMAT_NV21:
+        format = vigsp_plane_nv21;
+        break;
+    case fourcc_code('N', 'V', '4', '2'):
+        format = vigsp_plane_nv42;
+        break;
+    case DRM_FORMAT_NV61:
+        format = vigsp_plane_nv61;
+        break;
+    case DRM_FORMAT_YUV420:
+        format = vigsp_plane_yuv420;
+        break;
+    default:
+        BUG();
+        format = vigsp_plane_bgrx8888;
+        break;
+    }
+
+    ret = vigs_comm_set_plane(vigs_dev->comm,
+                              vigs_plane->index,
+                              fb->width,
+                              fb->height,
+                              format,
+                              surface_ids,
+                              src_x_whole,
+                              src_y_whole,
+                              src_w_whole,
+                              src_h_whole,
+                              crtc_x,
+                              crtc_y,
+                              crtc_w,
+                              crtc_h,
+                              vigs_plane->z_pos,
+                              vigs_plane->hflip,
+                              vigs_plane->vflip,
+                              vigs_plane->rotation);
+
+    if (ret == 0) {
+        vigs_plane->src_x = src_x;
+        vigs_plane->src_y = src_y;
+        vigs_plane->src_w = src_w;
+        vigs_plane->src_h = src_h;
+
+        vigs_plane->crtc_x = crtc_x;
+        vigs_plane->crtc_y = crtc_y;
+        vigs_plane->crtc_w = crtc_w;
+        vigs_plane->crtc_h = crtc_h;
+
+        vigs_plane->enabled = true;
+    }
+
+    return ret;
+}
+
+static int vigs_plane_disable(struct drm_plane *plane)
+{
+    struct vigs_plane *vigs_plane = plane_to_vigs_plane(plane);
+    struct vigs_device *vigs_dev = plane->dev->dev_private;
+    int ret;
+    vigsp_surface_id surface_ids[4] = { 0, 0, 0, 0 };
+
+    DRM_DEBUG_KMS("enter\n");
+
+    if (!vigs_plane->enabled) {
+        return 0;
+    }
+
+    ret = vigs_comm_set_plane(vigs_dev->comm,
+                              vigs_plane->index,
+                              0,
+                              0,
+                              0,
+                              surface_ids,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0,
+                              0);
+
+    if (ret == 0) {
+        vigs_plane->src_x = 0;
+        vigs_plane->src_y = 0;
+        vigs_plane->src_w = 0;
+        vigs_plane->src_h = 0;
+
+        vigs_plane->crtc_x = 0;
+        vigs_plane->crtc_y = 0;
+        vigs_plane->crtc_w = 0;
+        vigs_plane->crtc_h = 0;
+
+        vigs_plane->enabled = false;
+    }
+
+    return ret;
+}
+
+static void vigs_plane_destroy(struct drm_plane *plane)
+{
+    struct vigs_plane *vigs_plane = plane_to_vigs_plane(plane);
+
+    DRM_DEBUG_KMS("enter\n");
+
+    vigs_plane_disable(plane);
+    drm_plane_cleanup(plane);
+    kfree(vigs_plane);
+}
+
+static const struct drm_plane_funcs vigs_plane_funcs =
+{
+    .update_plane = vigs_plane_update,
+    .disable_plane = vigs_plane_disable,
+    .destroy = vigs_plane_destroy,
+};
+
+int vigs_plane_init(struct vigs_device *vigs_dev, u32 index)
+{
+    struct vigs_plane *vigs_plane;
+    int ret;
+
+    DRM_DEBUG_KMS("enter\n");
+
+    vigs_plane = kzalloc(sizeof(*vigs_plane), GFP_KERNEL);
+
+    if (!vigs_plane) {
+        return -ENOMEM;
+    }
+
+    vigs_plane->index = index;
+
+    ret = drm_plane_init(vigs_dev->drm_dev,
+                         &vigs_plane->base,
+                         (1 << 0),
+                         &vigs_plane_funcs,
+                         formats,
+                         ARRAY_SIZE(formats),
+                         false);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    return 0;
+}
+
+int vigs_plane_set_zpos_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv)
+{
+    struct drm_vigs_plane_set_zpos *args = data;
+    struct drm_mode_object *obj;
+    struct drm_plane *plane;
+    struct vigs_plane *vigs_plane;
+    int ret;
+
+    drm_modeset_lock_all(drm_dev);
+
+    obj = drm_mode_object_find(drm_dev,
+                               args->plane_id,
+                               DRM_MODE_OBJECT_PLANE);
+    if (!obj) {
+        ret = -EINVAL;
+        goto out;
+    }
+
+    plane = obj_to_plane(obj);
+    vigs_plane = plane_to_vigs_plane(plane);
+
+    vigs_plane->z_pos = args->zpos;
+
+    ret = 0;
+
+out:
+    drm_modeset_unlock_all(drm_dev);
+
+    return ret;
+}
+
+int vigs_plane_set_transform_ioctl(struct drm_device *drm_dev,
+                                   void *data,
+                                   struct drm_file *file_priv)
+{
+    struct drm_vigs_plane_set_transform *args = data;
+    struct drm_mode_object *obj;
+    struct drm_plane *plane;
+    struct vigs_plane *vigs_plane;
+    int ret;
+
+    drm_modeset_lock_all(drm_dev);
+
+    obj = drm_mode_object_find(drm_dev,
+                               args->plane_id,
+                               DRM_MODE_OBJECT_PLANE);
+    if (!obj) {
+        ret = -EINVAL;
+        goto out;
+    }
+
+    plane = obj_to_plane(obj);
+    vigs_plane = plane_to_vigs_plane(plane);
+
+    vigs_plane->hflip = args->hflip;
+    vigs_plane->vflip = args->vflip;
+    vigs_plane->rotation = args->rotation;
+
+    ret = 0;
+
+out:
+    drm_modeset_unlock_all(drm_dev);
+
+    return ret;
+}
diff --git a/drivers/gpu/drm/vigs/vigs_plane.h b/drivers/gpu/drm/vigs/vigs_plane.h
new file mode 100644
index 0000000..327f55c
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_plane.h
@@ -0,0 +1,56 @@
+#ifndef _VIGS_PLANE_H_
+#define _VIGS_PLANE_H_
+
+#include "drmP.h"
+
+struct vigs_device;
+
+struct vigs_plane
+{
+    struct drm_plane base;
+
+    u32 index;
+
+    unsigned int src_x;
+    unsigned int src_y;
+    unsigned int src_w;
+    unsigned int src_h;
+
+    int crtc_x;
+    int crtc_y;
+    unsigned int crtc_w;
+    unsigned int crtc_h;
+
+    int z_pos;
+    int hflip;
+    int vflip;
+    int rotation;
+
+    bool enabled;
+};
+
+static inline struct vigs_plane *plane_to_vigs_plane(struct drm_plane *plane)
+{
+    return container_of(plane, struct vigs_plane, base);
+}
+
+int vigs_plane_init(struct vigs_device *vigs_dev, u32 index);
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_plane_set_zpos_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv);
+
+int vigs_plane_set_transform_ioctl(struct drm_device *drm_dev,
+                                   void *data,
+                                   struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_prime.c b/drivers/gpu/drm/vigs/vigs_prime.c
new file mode 100644
index 0000000..06459aa
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_prime.c
@@ -0,0 +1,104 @@
+#include <linux/dma-buf.h>
+#include <drm/ttm/ttm_object.h>
+
+static int vigs_drm_prime_map_attach(struct dma_buf *dma_buf,
+				     struct device *target_dev,
+				     struct dma_buf_attachment *attach)
+{
+    return -ENOSYS;
+}
+
+static void vigs_drm_prime_map_detach(struct dma_buf *dma_buf,
+				      struct dma_buf_attachment *attach)
+{
+}
+
+static struct sg_table *vigs_drm_prime_map_dma_buf(
+                                             struct dma_buf_attachment *attach,
+					     enum dma_data_direction dir)
+{
+    return ERR_PTR(-ENOSYS);
+}
+
+static void vigs_drm_prime_unmap_dma_buf(struct dma_buf_attachment *attach,
+				         struct sg_table *sgb,
+				         enum dma_data_direction dir)
+{
+}
+
+static void *vigs_drm_prime_dmabuf_vmap(struct dma_buf *dma_buf)
+{
+    return NULL;
+}
+
+static void vigs_drm_prime_dmabuf_vunmap(struct dma_buf *dma_buf, void *vaddr)
+{
+}
+
+static void *vigs_drm_prime_dmabuf_kmap_atomic(struct dma_buf *dma_buf,
+                                               unsigned long page_num)
+{
+    return NULL;
+}
+
+static void vigs_drm_prime_dmabuf_kunmap_atomic(struct dma_buf *dma_buf,
+                                                unsigned long page_num,
+                                                void *addr)
+{
+
+}
+static void *vigs_drm_prime_dmabuf_kmap(struct dma_buf *dma_buf,
+                                        unsigned long page_num)
+{
+    return NULL;
+}
+
+static void vigs_drm_prime_dmabuf_kunmap(struct dma_buf *dma_buf,
+                                         unsigned long page_num, void *addr)
+{
+
+}
+
+static int vigs_drm_prime_dmabuf_mmap(struct dma_buf *dma_buf,
+				      struct vm_area_struct *vma)
+{
+    WARN_ONCE(true, "Attempted use of dmabuf mmap. Bad.\n");
+    return -ENOSYS;
+}
+
+const struct dma_buf_ops vigs_drm_prime_dmabuf_ops =
+{
+    .attach = vigs_drm_prime_map_attach,
+    .detach = vigs_drm_prime_map_detach,
+    .map_dma_buf = vigs_drm_prime_map_dma_buf,
+    .unmap_dma_buf = vigs_drm_prime_unmap_dma_buf,
+    .release = NULL,
+    .kmap = vigs_drm_prime_dmabuf_kmap,
+    .kmap_atomic = vigs_drm_prime_dmabuf_kmap_atomic,
+    .kunmap = vigs_drm_prime_dmabuf_kunmap,
+    .kunmap_atomic = vigs_drm_prime_dmabuf_kunmap_atomic,
+    .mmap = vigs_drm_prime_dmabuf_mmap,
+    .vmap = vigs_drm_prime_dmabuf_vmap,
+    .vunmap = vigs_drm_prime_dmabuf_vunmap,
+};
+
+/* int vigs_drm_prime_fd_to_handle(struct drm_device *dev, */
+/*                                 struct drm_file *file_priv, */
+/*                                 int fd, u32 *handle) */
+/* { */
+/*     struct vigs_file *vigs_file = file_priv; */
+/*     struct ttm_object_file *tfile = vigs_file->obj_file; */
+
+/*     return ttm_prime_fd_to_handle(tfile, fd, handle); */
+/* } */
+
+/* int vigs_drm_prime_handle_to_fd(struct drm_device *dev, */
+/*                                 struct drm_file *file_priv, */
+/*                                 uint32_t handle, uint32_t flags, */
+/*                                 int *prime_fd) */
+/* { */
+/*     struct vigs_file *vigs_file = file_priv; */
+/*     struct ttm_object_file *tfile = vigs_file->obj_file; */
+
+/*     return ttm_prime_handle_to_fd(tfile, handle, flags, prime_fd); */
+/* } */
diff --git a/drivers/gpu/drm/vigs/vigs_protocol.h b/drivers/gpu/drm/vigs/vigs_protocol.h
new file mode 100644
index 0000000..1f18cd6
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_protocol.h
@@ -0,0 +1,380 @@
+#ifndef _VIGS_PROTOCOL_H_
+#define _VIGS_PROTOCOL_H_
+
+/*
+ * VIGS protocol is a multiple request-no response protocol.
+ */
+
+/*
+ * Bump this whenever protocol changes.
+ */
+#define VIGS_PROTOCOL_VERSION 20
+
+#define VIGS_MAX_PLANES 2
+
+typedef signed char vigsp_s8;
+typedef signed short vigsp_s16;
+typedef signed int vigsp_s32;
+typedef signed long long vigsp_s64;
+typedef unsigned char vigsp_u8;
+typedef unsigned short vigsp_u16;
+typedef unsigned int vigsp_u32;
+typedef unsigned long long vigsp_u64;
+
+typedef vigsp_u32 vigsp_bool;
+typedef vigsp_u32 vigsp_surface_id;
+typedef vigsp_u32 vigsp_offset;
+typedef vigsp_u32 vigsp_color;
+typedef vigsp_u32 vigsp_fence_seq;
+
+typedef enum
+{
+    /*
+     * These command are guaranteed to sync on host, i.e.
+     * no fence is required.
+     * @{
+     */
+    vigsp_cmd_init = 0x0,
+    vigsp_cmd_reset = 0x1,
+    vigsp_cmd_exit = 0x2,
+    vigsp_cmd_set_root_surface = 0x3,
+    /*
+     * @}
+     */
+    /*
+     * These commands are executed asynchronously.
+     * @{
+     */
+    vigsp_cmd_create_surface = 0x4,
+    vigsp_cmd_destroy_surface = 0x5,
+    vigsp_cmd_update_vram = 0x6,
+    vigsp_cmd_update_gpu = 0x7,
+    vigsp_cmd_copy = 0x8,
+    vigsp_cmd_solid_fill = 0x9,
+    vigsp_cmd_set_plane = 0xA,
+    vigsp_cmd_ga_copy = 0xB
+    /*
+     * @}
+     */
+} vigsp_cmd;
+
+typedef enum
+{
+    vigsp_surface_bgrx8888 = 0x0,
+    vigsp_surface_bgra8888 = 0x1,
+} vigsp_surface_format;
+
+typedef enum
+{
+    vigsp_plane_bgrx8888 = 0x0,
+    vigsp_plane_bgra8888 = 0x1,
+    vigsp_plane_nv21 = 0x2,
+    vigsp_plane_nv42 = 0x3,
+    vigsp_plane_nv61 = 0x4,
+    vigsp_plane_yuv420 = 0x5
+} vigsp_plane_format;
+
+typedef enum
+{
+    vigsp_rotation0   = 0x0,
+    vigsp_rotation90  = 0x1,
+    vigsp_rotation180 = 0x2,
+    vigsp_rotation270 = 0x3
+} vigsp_rotation;
+
+#pragma pack(1)
+
+struct vigsp_point
+{
+    vigsp_u32 x;
+    vigsp_u32 y;
+};
+
+struct vigsp_size
+{
+    vigsp_u32 w;
+    vigsp_u32 h;
+};
+
+struct vigsp_rect
+{
+    struct vigsp_point pos;
+    struct vigsp_size size;
+};
+
+struct vigsp_copy
+{
+    struct vigsp_point from;
+    struct vigsp_point to;
+    struct vigsp_size size;
+};
+
+struct vigsp_cmd_batch_header
+{
+    /*
+     * Fence sequence requested by this batch.
+     * 0 for none.
+     */
+    vigsp_fence_seq fence_seq;
+
+    /*
+     * Batch size starting from batch header.
+     * Can be 0.
+     */
+    vigsp_u32 size;
+};
+
+struct vigsp_cmd_request_header
+{
+    vigsp_cmd cmd;
+
+    /*
+     * Request size starting from request header.
+     */
+    vigsp_u32 size;
+};
+
+/*
+ * cmd_init
+ *
+ * First command to be sent, client passes its protocol version
+ * and receives server's in response. If 'client_version' doesn't match
+ * 'server_version' then initialization is considered failed. This
+ * is typically called on target's DRM driver load.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_init_request
+{
+    vigsp_u32 client_version;
+    vigsp_u32 server_version;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_reset
+ *
+ * Destroys all surfaces but root surface, this typically happens
+ * or DRM's lastclose.
+ *
+ * @{
+ * @}
+ */
+
+/*
+ * cmd_exit
+ *
+ * Destroys all surfaces and transitions into uninitialized state, this
+ * typically happens when target's DRM driver gets unloaded.
+ *
+ * @{
+ * @}
+ */
+
+/*
+ * cmd_create_surface
+ *
+ * Called for each surface created. Client passes 'id' of the surface,
+ * all further operations must be carried out using this is. 'id' is
+ * unique across whole target system.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_create_surface_request
+{
+    vigsp_u32 width;
+    vigsp_u32 height;
+    vigsp_u32 stride;
+    vigsp_surface_format format;
+    vigsp_surface_id id;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_destroy_surface
+ *
+ * Destroys the surface identified by 'id'. Surface 'id' may not be used
+ * after this call and its id can be assigned to some other surface right
+ * after this call.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_destroy_surface_request
+{
+    vigsp_surface_id id;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_set_root_surface
+ *
+ * Sets surface identified by 'id' as new root surface. Root surface is the
+ * one that's displayed on screen. Root surface resides in VRAM
+ * all the time if 'scanout' is true.
+ *
+ * Pass 0 as id in order to reset the root surface.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_set_root_surface_request
+{
+    vigsp_surface_id id;
+    vigsp_bool scanout;
+    vigsp_offset offset;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_update_vram
+ *
+ * Updates 'sfc_id' in vram.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_update_vram_request
+{
+    vigsp_surface_id sfc_id;
+    vigsp_offset offset;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_update_gpu
+ *
+ * Updates 'sfc_id' in GPU.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_update_gpu_request
+{
+    vigsp_surface_id sfc_id;
+    vigsp_offset offset;
+    vigsp_u32 num_entries;
+    struct vigsp_rect entries[0];
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_copy
+ *
+ * Copies parts of surface 'src_id' to
+ * surface 'dst_id'.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_copy_request
+{
+    vigsp_surface_id src_id;
+    vigsp_surface_id dst_id;
+    vigsp_u32 num_entries;
+    struct vigsp_copy entries[0];
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_solid_fill
+ *
+ * Fills surface 'sfc_id' with color 'color' at 'entries'.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_solid_fill_request
+{
+    vigsp_surface_id sfc_id;
+    vigsp_color color;
+    vigsp_u32 num_entries;
+    struct vigsp_rect entries[0];
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_set_plane
+ *
+ * Assigns surfaces 'surfaces' to plane identified by 'plane'.
+ *
+ * Pass 0 as surfaces[0] in order to disable the plane.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_set_plane_request
+{
+    vigsp_u32 plane;
+    vigsp_u32 width;
+    vigsp_u32 height;
+    vigsp_plane_format format;
+    vigsp_surface_id surfaces[4];
+    struct vigsp_rect src_rect;
+    vigsp_s32 dst_x;
+    vigsp_s32 dst_y;
+    struct vigsp_size dst_size;
+    vigsp_s32 z_pos;
+    vigsp_bool hflip;
+    vigsp_bool vflip;
+    vigsp_rotation rotation;
+};
+
+/*
+ * @}
+ */
+
+/*
+ * cmd_ga_copy
+ *
+ * Copies part of surface 'src_id' to
+ * surface 'dst_id' given surface
+ * sizes.
+ *
+ * @{
+ */
+
+struct vigsp_cmd_ga_copy_request
+{
+    vigsp_surface_id src_id;
+    vigsp_bool src_scanout;
+    vigsp_offset src_offset;
+    vigsp_u32 src_stride;
+    vigsp_surface_id dst_id;
+    vigsp_u32 dst_stride;
+    struct vigsp_copy entry;
+};
+
+/*
+ * @}
+ */
+
+#pragma pack()
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_regs.h b/drivers/gpu/drm/vigs/vigs_regs.h
new file mode 100644
index 0000000..003479c
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_regs.h
@@ -0,0 +1,15 @@
+#ifndef _VIGS_REGS_H_
+#define _VIGS_REGS_H_
+
+#define VIGS_REG_EXEC 0
+#define VIGS_REG_CON 8
+#define VIGS_REG_INT 16
+#define VIGS_REG_FENCE_LOWER 24
+#define VIGS_REG_FENCE_UPPER 32
+
+#define VIGS_REG_CON_VBLANK_ENABLE 1
+
+#define VIGS_REG_INT_VBLANK_PENDING 1
+#define VIGS_REG_INT_FENCE_ACK_PENDING 2
+
+#endif
diff --git a/drivers/gpu/drm/vigs/vigs_surface.c b/drivers/gpu/drm/vigs/vigs_surface.c
new file mode 100644
index 0000000..b5ec2f4
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_surface.c
@@ -0,0 +1,472 @@
+#include "vigs_surface.h"
+#include "vigs_device.h"
+#include "vigs_comm.h"
+#include "vigs_mman.h"
+#include <drm/vigs_drm.h>
+
+/*
+ * Functions below MUST be accessed between
+ * vigs_gem_reserve/vigs_gem_unreserve.
+ * @{
+ */
+
+static u32 vigs_surface_saf(struct vigs_surface *sfc)
+{
+    u32 saf = 0;
+
+    if (sfc->num_readers > 0) {
+        saf |= DRM_VIGS_SAF_READ;
+    }
+
+    if (sfc->num_writers > 0) {
+        saf |= DRM_VIGS_SAF_WRITE;
+    }
+
+    return saf;
+}
+
+static void vigs_surface_saf_changed(struct vigs_surface *sfc,
+                                     u32 old_saf)
+{
+    u32 new_saf = vigs_surface_saf(sfc);
+
+    if (old_saf == new_saf) {
+        return;
+    }
+
+    /*
+     * If we're in GPU and access is write-only then we can
+     * obviously skip first VRAM update, since there's nothing
+     * to read back yet. After first VRAM update, however, we must
+     * read back every time since the clients must see their
+     * changes.
+     */
+
+    sfc->skip_vram_update = !vigs_gem_in_vram(&sfc->gem) &&
+                            (new_saf == DRM_VIGS_SAF_WRITE) &&
+                            !(old_saf & DRM_VIGS_SAF_WRITE);
+}
+
+static void vigs_vma_data_end_access(struct vigs_vma_data *vma_data, bool sync)
+{
+    struct vigs_surface *sfc = vma_data->sfc;
+    struct vigs_device *vigs_dev = sfc->gem.base.dev->dev_private;
+    u32 old_saf = vigs_surface_saf(sfc);
+
+    if (vma_data->saf & DRM_VIGS_SAF_READ) {
+        --sfc->num_readers;
+    }
+
+    if ((vma_data->saf & DRM_VIGS_SAF_WRITE) == 0) {
+        goto out;
+    }
+
+    if (sync) {
+        /*
+         * We have a sync, drop all pending
+         * writers.
+         */
+        sfc->num_writers -= sfc->num_pending_writers;
+        sfc->num_pending_writers = 0;
+    }
+
+    if (!vigs_gem_in_vram(&sfc->gem)) {
+        --sfc->num_writers;
+        goto out;
+    }
+
+    if (sync) {
+        --sfc->num_writers;
+        vigs_comm_update_gpu(vigs_dev->comm,
+                             sfc->id,
+                             sfc->width,
+                             sfc->height,
+                             vigs_gem_offset(&sfc->gem));
+        sfc->is_gpu_dirty = false;
+    } else {
+        ++sfc->num_pending_writers;
+    }
+
+out:
+    vma_data->saf = 0;
+
+    vigs_surface_saf_changed(sfc, old_saf);
+}
+
+/*
+ * @}
+ */
+
+void vigs_vma_data_init(struct vigs_vma_data *vma_data,
+                        struct vigs_surface *sfc,
+                        bool track_access)
+{
+    struct vigs_device *vigs_dev = sfc->gem.base.dev->dev_private;
+    u32 old_saf;
+
+    vma_data->sfc = sfc;
+    vma_data->saf = 0;
+    vma_data->track_access = track_access;
+
+    if (track_access) {
+        return;
+    }
+
+    /*
+     * If we don't want to track access for this VMA
+     * then register as both reader and writer.
+     */
+
+    vigs_gem_reserve(&sfc->gem);
+
+    old_saf = vigs_surface_saf(sfc);
+
+    ++sfc->num_writers;
+    ++sfc->num_readers;
+
+    if (vigs_gem_in_vram(&sfc->gem) && sfc->is_gpu_dirty) {
+        vigs_comm_update_vram(vigs_dev->comm,
+                              sfc->id,
+                              vigs_gem_offset(&sfc->gem));
+        sfc->is_gpu_dirty = false;
+    }
+
+    vma_data->saf = DRM_VIGS_SAF_READ | DRM_VIGS_SAF_WRITE;
+
+    vigs_surface_saf_changed(sfc, old_saf);
+
+    vigs_gem_unreserve(&sfc->gem);
+}
+
+void vigs_vma_data_cleanup(struct vigs_vma_data *vma_data)
+{
+    vigs_gem_reserve(&vma_data->sfc->gem);
+
+    /*
+     * On unmap we sync only when access tracking is enabled.
+     * Otherwise, we pretend we're going to sync
+     * some time later, but we never will.
+     */
+    vigs_vma_data_end_access(vma_data,
+                             vma_data->track_access);
+
+    vigs_gem_unreserve(&vma_data->sfc->gem);
+}
+
+static void vigs_surface_destroy(struct vigs_gem_object *gem)
+{
+    struct vigs_surface *sfc = vigs_gem_to_vigs_surface(gem);
+    struct vigs_device *vigs_dev = gem->base.dev->dev_private;
+
+    if (sfc->id) {
+        vigs_comm_destroy_surface(vigs_dev->comm, sfc->id);
+
+        vigs_device_remove_surface(vigs_dev, sfc->id);
+
+        DRM_DEBUG_DRIVER("Surface destroyed (id = %u)\n", sfc->id);
+    }
+}
+
+int vigs_surface_create(struct vigs_device *vigs_dev,
+                        u32 width,
+                        u32 height,
+                        u32 stride,
+                        vigsp_surface_format format,
+                        bool scanout,
+                        struct vigs_surface **sfc)
+{
+    int ret = 0;
+
+    *sfc = kzalloc(sizeof(**sfc), GFP_KERNEL);
+
+    if (!*sfc) {
+        ret = -ENOMEM;
+        goto fail1;
+    }
+
+    (*sfc)->width = width;
+    (*sfc)->height = height;
+    (*sfc)->stride = stride;
+    (*sfc)->format = format;
+    (*sfc)->scanout = scanout;
+
+    ret = vigs_gem_init(&(*sfc)->gem,
+                        vigs_dev,
+                        VIGS_GEM_TYPE_SURFACE,
+                        stride * height,
+                        false,
+                        &vigs_surface_destroy);
+
+    if (ret != 0) {
+        goto fail1;
+    }
+
+    ret = vigs_device_add_surface_unlocked(vigs_dev, *sfc, &(*sfc)->id);
+
+    if (ret != 0) {
+        goto fail2;
+    }
+
+    ret = vigs_comm_create_surface(vigs_dev->comm,
+                                   width,
+                                   height,
+                                   stride,
+                                   format,
+                                   (*sfc)->id);
+
+    if (ret != 0) {
+        goto fail3;
+    }
+
+    return 0;
+
+fail3:
+    vigs_device_remove_surface_unlocked(vigs_dev, (*sfc)->id);
+fail2:
+    (*sfc)->id = 0;
+    vigs_gem_cleanup(&(*sfc)->gem);
+fail1:
+    *sfc = NULL;
+
+    return ret;
+}
+
+bool vigs_surface_need_vram_update(struct vigs_surface *sfc)
+{
+    u32 saf = vigs_surface_saf(sfc);
+    bool skip_vram_update = sfc->skip_vram_update;
+
+    sfc->skip_vram_update = false;
+
+    return (saf != 0) && !skip_vram_update;
+}
+
+bool vigs_surface_need_gpu_update(struct vigs_surface *sfc)
+{
+    u32 old_saf = vigs_surface_saf(sfc);
+
+    sfc->num_writers -= sfc->num_pending_writers;
+    sfc->num_pending_writers = 0;
+
+    vigs_surface_saf_changed(sfc, old_saf);
+
+    return old_saf & DRM_VIGS_SAF_WRITE;
+}
+
+int vigs_surface_create_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_create_surface *args = data;
+    struct vigs_surface *sfc = NULL;
+    uint32_t handle;
+    int ret;
+
+    ret = vigs_surface_create(vigs_dev,
+                              args->width,
+                              args->height,
+                              args->stride,
+                              args->format,
+                              args->scanout,
+                              &sfc);
+
+    if (ret != 0) {
+        return ret;
+    }
+
+    ret = drm_gem_handle_create(file_priv,
+                                &sfc->gem.base,
+                                &handle);
+
+    drm_gem_object_unreference_unlocked(&sfc->gem.base);
+
+    if (ret == 0) {
+        args->handle = handle;
+        args->size = vigs_gem_size(&sfc->gem);
+        args->id = sfc->id;
+    }
+
+    return ret;
+}
+
+int vigs_surface_info_ioctl(struct drm_device *drm_dev,
+                            void *data,
+                            struct drm_file *file_priv)
+{
+    struct drm_vigs_surface_info *args = data;
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+    struct vigs_surface *sfc;
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, args->handle);
+
+    if (gem == NULL) {
+        return -ENOENT;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    if (vigs_gem->type != VIGS_GEM_TYPE_SURFACE) {
+        drm_gem_object_unreference_unlocked(gem);
+        return -ENOENT;
+    }
+
+    sfc = vigs_gem_to_vigs_surface(vigs_gem);
+
+    args->width = sfc->width;
+    args->height = sfc->height;
+    args->stride = sfc->stride;
+    args->format = sfc->format;
+    args->scanout = sfc->scanout;
+    args->size = vigs_gem_size(vigs_gem);
+    args->id = sfc->id;
+
+    drm_gem_object_unreference_unlocked(gem);
+
+    return 0;
+}
+
+int vigs_surface_set_gpu_dirty_ioctl(struct drm_device *drm_dev,
+                                     void *data,
+                                     struct drm_file *file_priv)
+{
+    struct drm_vigs_surface_set_gpu_dirty *args = data;
+    struct drm_gem_object *gem;
+    struct vigs_gem_object *vigs_gem;
+    struct vigs_surface *sfc;
+
+    gem = drm_gem_object_lookup(drm_dev, file_priv, args->handle);
+
+    if (gem == NULL) {
+        return -ENOENT;
+    }
+
+    vigs_gem = gem_to_vigs_gem(gem);
+
+    if (vigs_gem->type != VIGS_GEM_TYPE_SURFACE) {
+        drm_gem_object_unreference_unlocked(gem);
+        return -ENOENT;
+    }
+
+    sfc = vigs_gem_to_vigs_surface(vigs_gem);
+
+    vigs_gem_reserve(&sfc->gem);
+
+    if (vigs_gem_in_vram(&sfc->gem)) {
+        sfc->is_gpu_dirty = true;
+    }
+
+    vigs_gem_unreserve(&sfc->gem);
+
+    drm_gem_object_unreference_unlocked(gem);
+
+    return 0;
+}
+
+static int vigs_surface_start_access(void *user_data, void *vma_data_opaque)
+{
+    struct drm_vigs_surface_start_access *args = user_data;
+    struct vigs_vma_data *vma_data = vma_data_opaque;
+    struct vigs_surface *sfc = vma_data->sfc;
+    struct vigs_device *vigs_dev;
+    u32 old_saf;
+
+    if (!sfc) {
+        return -ENOENT;
+    }
+
+    if (!vma_data->track_access) {
+        return 0;
+    }
+
+    vigs_dev = sfc->gem.base.dev->dev_private;
+
+    if ((args->saf & ~DRM_VIGS_SAF_MASK) != 0) {
+        return -EINVAL;
+    }
+
+    vigs_gem_reserve(&sfc->gem);
+
+    old_saf = vigs_surface_saf(sfc);
+
+    if (vma_data->saf & DRM_VIGS_SAF_READ) {
+        --sfc->num_readers;
+    }
+
+    if (vma_data->saf & DRM_VIGS_SAF_WRITE) {
+        --sfc->num_writers;
+    }
+
+    if (args->saf & DRM_VIGS_SAF_WRITE) {
+        ++sfc->num_writers;
+    }
+
+    if (args->saf & DRM_VIGS_SAF_READ) {
+        ++sfc->num_readers;
+
+        if (vigs_gem_in_vram(&sfc->gem) && sfc->is_gpu_dirty) {
+            vigs_comm_update_vram(vigs_dev->comm,
+                                  sfc->id,
+                                  vigs_gem_offset(&sfc->gem));
+            sfc->is_gpu_dirty = false;
+        }
+    }
+
+    vma_data->saf = args->saf;
+
+    vigs_surface_saf_changed(sfc, old_saf);
+
+    vigs_gem_unreserve(&sfc->gem);
+
+    return 0;
+}
+
+int vigs_surface_start_access_ioctl(struct drm_device *drm_dev,
+                                    void *data,
+                                    struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_surface_start_access *args = data;
+
+    return vigs_mman_access_vma(vigs_dev->mman,
+                                args->address,
+                                &vigs_surface_start_access,
+                                args);
+}
+
+static int vigs_surface_end_access(void *user_data, void *vma_data_opaque)
+{
+    struct drm_vigs_surface_end_access *args = user_data;
+    struct vigs_vma_data *vma_data = vma_data_opaque;
+    struct vigs_surface *sfc = vma_data->sfc;
+
+    if (!sfc) {
+        return -ENOENT;
+    }
+
+    if (!vma_data->track_access) {
+        return 0;
+    }
+
+    vigs_gem_reserve(&sfc->gem);
+
+    vigs_vma_data_end_access(vma_data, args->sync);
+
+    vigs_gem_unreserve(&sfc->gem);
+
+    return 0;
+}
+
+int vigs_surface_end_access_ioctl(struct drm_device *drm_dev,
+                                  void *data,
+                                  struct drm_file *file_priv)
+{
+    struct vigs_device *vigs_dev = drm_dev->dev_private;
+    struct drm_vigs_surface_end_access *args = data;
+
+    return vigs_mman_access_vma(vigs_dev->mman,
+                                args->address,
+                                &vigs_surface_end_access,
+                                args);
+}
diff --git a/drivers/gpu/drm/vigs/vigs_surface.h b/drivers/gpu/drm/vigs/vigs_surface.h
new file mode 100644
index 0000000..1d89e3b
--- /dev/null
+++ b/drivers/gpu/drm/vigs/vigs_surface.h
@@ -0,0 +1,128 @@
+#ifndef _VIGS_SURFACE_H_
+#define _VIGS_SURFACE_H_
+
+#include "drmP.h"
+#include "vigs_protocol.h"
+#include "vigs_gem.h"
+
+struct vigs_surface
+{
+    /*
+     * Must be first member!
+     */
+    struct vigs_gem_object gem;
+
+    u32 width;
+    u32 height;
+    u32 stride;
+    vigsp_surface_format format;
+    bool scanout;
+    vigsp_surface_id id;
+
+    /*
+     * Members below MUST be accessed between
+     * vigs_gem_reserve/vigs_gem_unreserve.
+     * @{
+     */
+
+    bool is_gpu_dirty;
+
+    /*
+     * Number of mmap areas (vmas) that accessed this surface for
+     * read/write.
+     * @{
+     */
+    u32 num_readers;
+    u32 num_writers;
+    /*
+     * @}
+     */
+
+    /*
+     * Number of mmap area writers that ended access asynchronously, i.e.
+     * they still account for in 'num_writers', but as soon as first GPU
+     * update operation takes place they'll be gone.
+     */
+    u32 num_pending_writers;
+
+    /*
+     * Specifies that we should not update VRAM on next 'update_vram'
+     * request. Lasts for one request.
+     */
+    bool skip_vram_update;
+
+    /*
+     * @}
+     */
+};
+
+struct vigs_vma_data
+{
+    struct vigs_surface *sfc;
+    u32 saf;
+    bool track_access;
+};
+
+void vigs_vma_data_init(struct vigs_vma_data *vma_data,
+                        struct vigs_surface *sfc,
+                        bool track_access);
+
+void vigs_vma_data_cleanup(struct vigs_vma_data *vma_data);
+
+static inline struct vigs_surface *vigs_gem_to_vigs_surface(struct vigs_gem_object *vigs_gem)
+{
+    return container_of(vigs_gem, struct vigs_surface, gem);
+}
+
+int vigs_surface_create(struct vigs_device *vigs_dev,
+                        u32 width,
+                        u32 height,
+                        u32 stride,
+                        vigsp_surface_format format,
+                        bool scanout,
+                        struct vigs_surface **sfc);
+
+/*
+ * Functions below MUST be accessed between
+ * vigs_gem_reserve/vigs_gem_unreserve.
+ * @{
+ */
+
+bool vigs_surface_need_vram_update(struct vigs_surface *sfc);
+
+bool vigs_surface_need_gpu_update(struct vigs_surface *sfc);
+
+/*
+ * @}
+ */
+
+/*
+ * IOCTLs
+ * @{
+ */
+
+int vigs_surface_create_ioctl(struct drm_device *drm_dev,
+                              void *data,
+                              struct drm_file *file_priv);
+
+int vigs_surface_info_ioctl(struct drm_device *drm_dev,
+                            void *data,
+                            struct drm_file *file_priv);
+
+int vigs_surface_set_gpu_dirty_ioctl(struct drm_device *drm_dev,
+                                     void *data,
+                                     struct drm_file *file_priv);
+
+int vigs_surface_start_access_ioctl(struct drm_device *drm_dev,
+                                    void *data,
+                                    struct drm_file *file_priv);
+
+int vigs_surface_end_access_ioctl(struct drm_device *drm_dev,
+                                  void *data,
+                                  struct drm_file *file_priv);
+
+/*
+ * @}
+ */
+
+#endif
diff --git a/include/drm/vigs_drm.h b/include/drm/vigs_drm.h
new file mode 100644
index 0000000..9ae8245
--- /dev/null
+++ b/include/drm/vigs_drm.h
@@ -0,0 +1,217 @@
+/*
+ * vigs_drm.h
+ */
+
+#ifndef _VIGS_DRM_H_
+#define _VIGS_DRM_H_
+
+/*
+ * Bump this whenever driver interface changes.
+ */
+#define DRM_VIGS_DRIVER_VERSION 14
+
+/*
+ * Surface access flags.
+ */
+#define DRM_VIGS_SAF_READ 1
+#define DRM_VIGS_SAF_WRITE 2
+#define DRM_VIGS_SAF_MASK 3
+
+/*
+ * Number of DP framebuffers.
+ */
+#define DRM_VIGS_NUM_DP_FB_BUF 4
+
+/*
+ * DP memory types.
+ */
+#define DRM_VIGS_DP_FB_Y 2
+#define DRM_VIGS_DP_FB_C 3
+
+struct drm_vigs_get_protocol_version
+{
+    uint32_t version;
+};
+
+struct drm_vigs_create_surface
+{
+    uint32_t width;
+    uint32_t height;
+    uint32_t stride;
+    uint32_t format;
+    int scanout;
+    uint32_t handle;
+    uint32_t size;
+    uint32_t id;
+};
+
+struct drm_vigs_create_execbuffer
+{
+    uint32_t size;
+    uint32_t handle;
+};
+
+struct drm_vigs_gem_map
+{
+    uint32_t handle;
+    int track_access;
+    unsigned long address;
+};
+
+struct drm_vigs_gem_wait
+{
+    uint32_t handle;
+};
+
+struct drm_vigs_surface_info
+{
+    uint32_t handle;
+    uint32_t width;
+    uint32_t height;
+    uint32_t stride;
+    uint32_t format;
+    int scanout;
+    uint32_t size;
+    uint32_t id;
+};
+
+struct drm_vigs_exec
+{
+    uint32_t handle;
+};
+
+struct drm_vigs_surface_set_gpu_dirty
+{
+    uint32_t handle;
+};
+
+struct drm_vigs_surface_start_access
+{
+    unsigned long address;
+    uint32_t saf;
+};
+
+struct drm_vigs_surface_end_access
+{
+    unsigned long address;
+    int sync;
+};
+
+struct drm_vigs_create_fence
+{
+    int send;
+    uint32_t handle;
+    uint32_t seq;
+};
+
+struct drm_vigs_fence_wait
+{
+    uint32_t handle;
+};
+
+struct drm_vigs_fence_signaled
+{
+    uint32_t handle;
+    int signaled;
+};
+
+struct drm_vigs_fence_unref
+{
+    uint32_t handle;
+};
+
+struct drm_vigs_plane_set_zpos
+{
+    uint32_t plane_id;
+    int zpos;
+};
+
+struct drm_vigs_plane_set_transform
+{
+    uint32_t plane_id;
+    int hflip;
+    int vflip;
+    int rotation;
+};
+
+struct drm_vigs_dp_create_surface
+{
+    uint32_t dp_plane;
+    uint32_t dp_fb_buf;
+    uint32_t dp_mem_flag;
+    uint32_t width;
+    uint32_t height;
+    uint32_t stride;
+    uint32_t format;
+    uint32_t handle;
+    uint32_t size;
+    uint32_t id;
+};
+
+struct drm_vigs_dp_open_surface
+{
+    uint32_t dp_plane;
+    uint32_t dp_fb_buf;
+    uint32_t dp_mem_flag;
+    uint32_t handle;
+};
+
+#define DRM_VIGS_GET_PROTOCOL_VERSION 0x00
+#define DRM_VIGS_CREATE_SURFACE 0x01
+#define DRM_VIGS_CREATE_EXECBUFFER 0x02
+#define DRM_VIGS_GEM_MAP 0x03
+#define DRM_VIGS_GEM_WAIT 0x04
+#define DRM_VIGS_SURFACE_INFO 0x05
+#define DRM_VIGS_EXEC 0x06
+#define DRM_VIGS_SURFACE_SET_GPU_DIRTY 0x07
+#define DRM_VIGS_SURFACE_START_ACCESS 0x08
+#define DRM_VIGS_SURFACE_END_ACCESS 0x09
+#define DRM_VIGS_CREATE_FENCE 0x0A
+#define DRM_VIGS_FENCE_WAIT 0x0B
+#define DRM_VIGS_FENCE_SIGNALED 0x0C
+#define DRM_VIGS_FENCE_UNREF 0x0D
+#define DRM_VIGS_PLANE_SET_ZPOS 0x0E
+#define DRM_VIGS_PLANE_SET_TRANSFORM 0x0F
+
+#define DRM_VIGS_DP_CREATE_SURFACE 0x20
+#define DRM_VIGS_DP_OPEN_SURFACE 0x21
+
+#define DRM_IOCTL_VIGS_GET_PROTOCOL_VERSION DRM_IOR(DRM_COMMAND_BASE + \
+            DRM_VIGS_GET_PROTOCOL_VERSION, struct drm_vigs_get_protocol_version)
+#define DRM_IOCTL_VIGS_CREATE_SURFACE DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_CREATE_SURFACE, struct drm_vigs_create_surface)
+#define DRM_IOCTL_VIGS_CREATE_EXECBUFFER DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_CREATE_EXECBUFFER, struct drm_vigs_create_execbuffer)
+#define DRM_IOCTL_VIGS_GEM_MAP DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_GEM_MAP, struct drm_vigs_gem_map)
+#define DRM_IOCTL_VIGS_GEM_WAIT DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_GEM_WAIT, struct drm_vigs_gem_wait)
+#define DRM_IOCTL_VIGS_SURFACE_INFO DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_SURFACE_INFO, struct drm_vigs_surface_info)
+#define DRM_IOCTL_VIGS_EXEC DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_EXEC, struct drm_vigs_exec)
+#define DRM_IOCTL_VIGS_SURFACE_SET_GPU_DIRTY DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_SURFACE_SET_GPU_DIRTY, struct drm_vigs_surface_set_gpu_dirty)
+#define DRM_IOCTL_VIGS_SURFACE_START_ACCESS DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_SURFACE_START_ACCESS, struct drm_vigs_surface_start_access)
+#define DRM_IOCTL_VIGS_SURFACE_END_ACCESS DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_SURFACE_END_ACCESS, struct drm_vigs_surface_end_access)
+#define DRM_IOCTL_VIGS_CREATE_FENCE DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_CREATE_FENCE, struct drm_vigs_create_fence)
+#define DRM_IOCTL_VIGS_FENCE_WAIT DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_FENCE_WAIT, struct drm_vigs_fence_wait)
+#define DRM_IOCTL_VIGS_FENCE_SIGNALED DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_FENCE_SIGNALED, struct drm_vigs_fence_signaled)
+#define DRM_IOCTL_VIGS_FENCE_UNREF DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_FENCE_UNREF, struct drm_vigs_fence_unref)
+#define DRM_IOCTL_VIGS_PLANE_SET_ZPOS DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_PLANE_SET_ZPOS, struct drm_vigs_plane_set_zpos)
+#define DRM_IOCTL_VIGS_PLANE_SET_TRANSFORM DRM_IOW(DRM_COMMAND_BASE + \
+            DRM_VIGS_PLANE_SET_TRANSFORM, struct drm_vigs_plane_set_transform)
+
+#define DRM_IOCTL_VIGS_DP_CREATE_SURFACE DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_DP_CREATE_SURFACE, struct drm_vigs_dp_create_surface)
+#define DRM_IOCTL_VIGS_DP_OPEN_SURFACE DRM_IOWR(DRM_COMMAND_BASE + \
+            DRM_VIGS_DP_OPEN_SURFACE, struct drm_vigs_dp_open_surface)
+
+#endif
